{"created": 1750047308.0704613, "duration": 8.523842573165894, "exitcode": 1, "root": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified", "environment": {}, "summary": {"passed": 205, "failed": 29, "error": 23, "total": 257, "collected": 257}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests", "type": "Package"}]}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_citation_with_minimal_data", "type": "Function", "lineno": 14}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_citation_with_missing_optional_fields", "type": "Function", "lineno": 30}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_citation_with_all_fields", "type": "Function", "lineno": 48}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_author_list_with_single_author", "type": "Function", "lineno": 76}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_author_list_with_two_authors", "type": "Function", "lineno": 86}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_author_list_with_many_authors", "type": "Function", "lineno": 97}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_direct_format_citation", "type": "Function", "lineno": 107}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_direct_format_author_list", "type": "Function", "lineno": 130}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_citation_book", "type": "Function", "lineno": 145}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_citation_conference", "type": "Function", "lineno": 172}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_citation_with_special_characters", "type": "Function", "lineno": 194}]}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_author_list", "type": "Function", "lineno": 45}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_citation_apa", "type": "Function", "lineno": 77}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_citation_mla", "type": "Function", "lineno": 89}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_citation_chicago", "type": "Function", "lineno": 102}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_citation_harvard", "type": "Function", "lineno": 115}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_citation_ieee", "type": "Function", "lineno": 127}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_citation_vancouver", "type": "Function", "lineno": 140}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_citation_bibtex", "type": "Function", "lineno": 152}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_citation_ris", "type": "Function", "lineno": 170}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_book_citation", "type": "Function", "lineno": 191}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_missing_fields", "type": "Function", "lineno": 214}]}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases::test_bibtex_file_parsing_basics_simple", "type": "Function", "lineno": 20}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases::test_bibtex_file_with_multiple_entries", "type": "Function", "lineno": 28}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases::test_parse_bibtex_file_nonexistent", "type": "Function", "lineno": 76}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases::test_ris_file_parsing_simple", "type": "Function", "lineno": 82}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases::test_parse_ris_file_nonexistent", "type": "Function", "lineno": 90}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases::test_extract_pdf_metadata_direct", "type": "Function", "lineno": 95}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases::test_extract_doi_functionality", "type": "Function", "lineno": 111}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases::test_extract_doi_not_found", "type": "Function", "lineno": 124}]}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/citations/test_parsers.py::TestCitationParsers", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/citations/test_parsers.py::TestCitationParsers::test_extract_doi_from_pdf", "type": "Function", "lineno": 16}, {"nodeid": "tests/academic_researcher/citations/test_parsers.py::TestCitationParsers::test_extract_pdf_metadata", "type": "Function", "lineno": 52}, {"nodeid": "tests/academic_researcher/citations/test_parsers.py::TestCitationParsers::test_parse_bibtex_file", "type": "Function", "lineno": 100}, {"nodeid": "tests/academic_researcher/citations/test_parsers.py::TestCitationParsers::test_parse_ris_file", "type": "Function", "lineno": 162}, {"nodeid": "tests/academic_researcher/citations/test_parsers.py::TestCitationParsers::test_parse_invalid_bibtex", "type": "Function", "lineno": 229}, {"nodeid": "tests/academic_researcher/citations/test_parsers.py::TestCitationParsers::test_parse_invalid_ris", "type": "Function", "lineno": 250}]}, {"nodeid": "tests/academic_researcher/citations/test_parsers.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/citations/test_parsers.py::TestCitationParsers", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/citations", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py", "type": "Module"}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py", "type": "Module"}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py", "type": "Module"}, {"nodeid": "tests/academic_researcher/citations/test_parsers.py", "type": "Module"}]}, {"nodeid": "tests/academic_researcher/collaboration", "outcome": "passed", "result": []}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_create_and_get_note", "type": "Function", "lineno": 34}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_update_note", "type": "Function", "lineno": 52}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_delete_note", "type": "Function", "lineno": 78}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_create_citation_and_link_note", "type": "Function", "lineno": 95}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_create_research_question_and_add_evidence", "type": "Function", "lineno": 129}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_create_experiment", "type": "Function", "lineno": 181}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_create_grant_proposal_and_add_items", "type": "Function", "lineno": 215}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_search", "type": "Function", "lineno": 267}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_get_related_nodes", "type": "Function", "lineno": 303}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_create_collaborator_and_annotation", "type": "Function", "lineno": 348}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_generate_citation", "type": "Function", "lineno": 382}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_backup_and_restore", "type": "Function", "lineno": 412}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_knowledge_graph_structure", "type": "Function", "lineno": 450}]}, {"nodeid": "tests/academic_researcher/core/test_brain.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestKnowledgeNode", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/core/test_models.py::TestKnowledgeNode::test_init_default_values", "type": "Function", "lineno": 18}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestKnowledgeNode::test_init_custom_values", "type": "Function", "lineno": 27}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestKnowledgeNode::test_update_method", "type": "Function", "lineno": 45}]}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestNote", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/core/test_models.py::TestNote::test_init_minimal", "type": "Function", "lineno": 59}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestNote::test_init_full", "type": "Function", "lineno": 70}]}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestCitation", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/core/test_models.py::TestCitation::test_init_minimal", "type": "Function", "lineno": 97}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestCitation::test_init_full", "type": "Function", "lineno": 113}]}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestResearchQuestion", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/core/test_models.py::TestResearchQuestion::test_init_minimal", "type": "Function", "lineno": 158}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestResearchQuestion::test_init_with_evidence", "type": "Function", "lineno": 170}]}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestExperiment", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/core/test_models.py::TestExperiment::test_init_minimal", "type": "Function", "lineno": 202}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestExperiment::test_init_full", "type": "Function", "lineno": 221}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestExperiment::test_end_date_validation", "type": "Function", "lineno": 255}]}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestGrantProposal", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/core/test_models.py::TestGrantProposal::test_init_minimal", "type": "Function", "lineno": 273}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestGrantProposal::test_init_full", "type": "Function", "lineno": 291}]}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestCollaborator", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/core/test_models.py::TestCollaborator::test_init_minimal", "type": "Function", "lineno": 327}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestCollaborator::test_init_full", "type": "Function", "lineno": 337}]}, {"nodeid": "tests/academic_researcher/core/test_models.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/core/test_models.py::TestKnowledgeNode", "type": "Class"}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestNote", "type": "Class"}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestCitation", "type": "Class"}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestResearchQuestion", "type": "Class"}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestExperiment", "type": "Class"}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestGrantProposal", "type": "Class"}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestCollaborator", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_init_creates_directories", "type": "Function", "lineno": 31}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_save_and_get_note", "type": "Function", "lineno": 51}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_save_and_get_citation", "type": "Function", "lineno": 68}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_update_existing_item", "type": "Function", "lineno": 88}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_get_nonexistent_item", "type": "Function", "lineno": 102}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_get_invalid_file", "type": "Function", "lineno": 109}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_delete_item", "type": "Function", "lineno": 121}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_delete_nonexistent_item", "type": "Function", "lineno": 136}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_list_all", "type": "Function", "lineno": 143}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_query", "type": "Function", "lineno": 173}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_save_and_get_attachment", "type": "Function", "lineno": 193}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_get_nonexistent_attachment", "type": "Function", "lineno": 218}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_export_to_dataframe", "type": "Function", "lineno": 224}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_backup_and_restore", "type": "Function", "lineno": 245}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_search_text", "type": "Function", "lineno": 289}]}, {"nodeid": "tests/academic_researcher/core/test_storage.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/core", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/core/test_brain.py", "type": "Module"}, {"nodeid": "tests/academic_researcher/core/test_models.py", "type": "Module"}, {"nodeid": "tests/academic_researcher/core/test_storage.py", "type": "Module"}]}, {"nodeid": "tests/academic_researcher/experiments/test_template_functions.py::TestTemplatesFunctions", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/experiments/test_template_functions.py::TestTemplatesFunctions::test_create_default_templates", "type": "Function", "lineno": 35}, {"nodeid": "tests/academic_researcher/experiments/test_template_functions.py::TestTemplatesFunctions::test_list_templates", "type": "Function", "lineno": 47}, {"nodeid": "tests/academic_researcher/experiments/test_template_functions.py::TestTemplatesFunctions::test_get_template", "type": "Function", "lineno": 70}, {"nodeid": "tests/academic_researcher/experiments/test_template_functions.py::TestTemplatesFunctions::test_create_template", "type": "Function", "lineno": 88}, {"nodeid": "tests/academic_researcher/experiments/test_template_functions.py::TestTemplatesFunctions::test_apply_template", "type": "Function", "lineno": 119}]}, {"nodeid": "tests/academic_researcher/experiments/test_template_functions.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/experiments/test_template_functions.py::TestTemplatesFunctions", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/experiments/test_templates.py::TestExperimentTemplates", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/experiments/test_templates.py::TestExperimentTemplates::test_create_default_templates", "type": "Function", "lineno": 30}, {"nodeid": "tests/academic_researcher/experiments/test_templates.py::TestExperimentTemplates::test_get_template", "type": "Function", "lineno": 51}, {"nodeid": "tests/academic_researcher/experiments/test_templates.py::TestExperimentTemplates::test_list_templates", "type": "Function", "lineno": 74}, {"nodeid": "tests/academic_researcher/experiments/test_templates.py::TestExperimentTemplates::test_create_template", "type": "Function", "lineno": 88}, {"nodeid": "tests/academic_researcher/experiments/test_templates.py::TestExperimentTemplates::test_apply_template", "type": "Function", "lineno": 127}, {"nodeid": "tests/academic_researcher/experiments/test_templates.py::TestExperimentTemplates::test_template_validation", "type": "Function", "lineno": 203}]}, {"nodeid": "tests/academic_researcher/experiments/test_templates.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/experiments/test_templates.py::TestExperimentTemplates", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/experiments", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/experiments/test_template_functions.py", "type": "Module"}, {"nodeid": "tests/academic_researcher/experiments/test_templates.py", "type": "Module"}]}, {"nodeid": "tests/academic_researcher/grants/test_export.py::TestGrantExport", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/grants/test_export.py::TestGrantExport::test_export_proposal_markdown", "type": "Function", "lineno": 85}, {"nodeid": "tests/academic_researcher/grants/test_export.py::TestGrantExport::test_export_proposal_yaml", "type": "Function", "lineno": 124}, {"nodeid": "tests/academic_researcher/grants/test_export.py::TestGrantExport::test_export_unsupported_format", "type": "Function", "lineno": 175}, {"nodeid": "tests/academic_researcher/grants/test_export.py::TestGrantExport::test_convert_uuids_to_strings", "type": "Function", "lineno": 202}]}, {"nodeid": "tests/academic_researcher/grants/test_export.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/grants/test_export.py::TestGrantExport", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/grants/test_export_functions.py::TestGrantExportFunctions", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/grants/test_export_functions.py::TestGrantExportFunctions::test_export_proposal_markdown", "type": "Function", "lineno": 119}, {"nodeid": "tests/academic_researcher/grants/test_export_functions.py::TestGrantExportFunctions::test_export_proposal_yaml", "type": "Function", "lineno": 153}, {"nodeid": "tests/academic_researcher/grants/test_export_functions.py::TestGrantExportFunctions::test_export_proposal_unknown_extension", "type": "Function", "lineno": 186}, {"nodeid": "tests/academic_researcher/grants/test_export_functions.py::TestGrantExportFunctions::test_export_markdown_error_handling", "type": "Function", "lineno": 211}, {"nodeid": "tests/academic_researcher/grants/test_export_functions.py::TestGrantExportFunctions::test_export_yaml_error_handling", "type": "Function", "lineno": 236}]}, {"nodeid": "tests/academic_researcher/grants/test_export_functions.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/grants/test_export_functions.py::TestGrantExportFunctions", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/grants", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/grants/test_export.py", "type": "Module"}, {"nodeid": "tests/academic_researcher/grants/test_export_functions.py", "type": "Module"}]}, {"nodeid": "tests/academic_researcher/notes", "outcome": "passed", "result": []}, {"nodeid": "tests/academic_researcher/research", "outcome": "passed", "result": []}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking::test_note_citation_linking", "type": "Function", "lineno": 29}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking::test_automatic_citation_extraction", "type": "Function", "lineno": 68}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking::test_multiple_citation_links", "type": "Function", "lineno": 95}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking::test_removing_citation_links", "type": "Function", "lineno": 134}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking::test_cascading_deletion", "type": "Function", "lineno": 187}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking::test_navigation_between_linked_items", "type": "Function", "lineno": 229}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking::test_circular_navigation", "type": "Function", "lineno": 277}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking::test_section_references", "type": "Function", "lineno": 343}]}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/test_citation_accuracy.py::TestCitationAccuracy", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_citation_accuracy.py::TestCitationAccuracy::test_apa_citation_accuracy", "type": "Function", "lineno": 30}, {"nodeid": "tests/academic_researcher/test_citation_accuracy.py::TestCitationAccuracy::test_mla_citation_accuracy", "type": "Function", "lineno": 58}, {"nodeid": "tests/academic_researcher/test_citation_accuracy.py::TestCitationAccuracy::test_chicago_citation_accuracy", "type": "Function", "lineno": 85}, {"nodeid": "tests/academic_researcher/test_citation_accuracy.py::TestCitationAccuracy::test_bibtex_citation_accuracy", "type": "Function", "lineno": 112}, {"nodeid": "tests/academic_researcher/test_citation_accuracy.py::TestCitationAccuracy::test_bibtex_parser_accuracy", "type": "Function", "lineno": 146}, {"nodeid": "tests/academic_researcher/test_citation_accuracy.py::TestCitationAccuracy::test_ris_parser_accuracy", "type": "Function", "lineno": 223}, {"nodeid": "tests/academic_researcher/test_citation_accuracy.py::TestCitationAccuracy::test_malformed_citation_handling", "type": "Function", "lineno": 306}]}, {"nodeid": "tests/academic_researcher/test_citation_accuracy.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_citation_accuracy.py::TestCitationAccuracy", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/test_cli.py::TestCLI", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_cli.py::TestCLI::test_init_command", "type": "Function", "lineno": 38}, {"nodeid": "tests/academic_researcher/test_cli.py::TestCLI::test_main_function", "type": "Function", "lineno": 55}, {"nodeid": "tests/academic_researcher/test_cli.py::TestCLI::test_note_command_integration_simplified", "type": "Function", "lineno": 84}, {"nodeid": "tests/academic_researcher/test_cli.py::TestCLI::test_search_command_existence", "type": "Function", "lineno": 103}, {"nodeid": "tests/academic_researcher/test_cli.py::TestCLI::test_main_with_init_integration", "type": "Function", "lineno": 139}, {"nodeid": "tests/academic_researcher/test_cli.py::TestCLI::test_multiple_cli_commands", "type": "Function", "lineno": 156}]}, {"nodeid": "tests/academic_researcher/test_cli.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_cli.py::TestCLI", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_create_collaborator", "type": "Function", "lineno": 108}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_add_annotations", "type": "Function", "lineno": 130}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_multiple_annotations_from_same_collaborator", "type": "Function", "lineno": 207}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_annotations_on_multiple_documents", "type": "Function", "lineno": 248}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_import_collaborator_annotations", "type": "Function", "lineno": 311}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_import_collaborator_annotations_with_replies", "type": "Function", "lineno": 386}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_import_collaborator_annotations_with_invalid_data", "type": "Function", "lineno": 450}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_import_invalid_annotations", "type": "Function", "lineno": 555}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_annotations_on_citations", "type": "Function", "lineno": 598}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_maintaining_annotation_integrity", "type": "Function", "lineno": 643}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_collaborative_feedback_integration", "type": "Function", "lineno": 680}]}, {"nodeid": "tests/academic_researcher/test_collaboration.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_default_templates_availability", "type": "Function", "lineno": 32}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_behavioral_experiment_template", "type": "Function", "lineno": 61}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_neuroimaging_experiment_template", "type": "Function", "lineno": 106}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_molecular_biology_experiment_template", "type": "Function", "lineno": 156}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_computational_modeling_experiment_template", "type": "Function", "lineno": 203}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_template_with_missing_required_fields", "type": "Function", "lineno": 252}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_template_with_optional_fields", "type": "Function", "lineno": 264}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_custom_template_creation", "type": "Function", "lineno": 286}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_sleep_study_template", "type": "Function", "lineno": 403}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_experiment_linking_to_research_question", "type": "Function", "lineno": 463}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_experiment_documentation_with_notes", "type": "Function", "lineno": 516}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_experiment_status_updates", "type": "Function", "lineno": 577}]}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_create_grant_proposal", "type": "Function", "lineno": 116}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_add_items_to_grant_workspace", "type": "Function", "lineno": 153}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_adding_items_incrementally", "type": "Function", "lineno": 208}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_grant_proposal_status_progression", "type": "Function", "lineno": 251}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_export_grant_proposal_to_markdown", "type": "Function", "lineno": 289}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_export_grant_proposal_to_yaml", "type": "Function", "lineno": 345}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_grant_proposal_version_history", "type": "Function", "lineno": 413}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_budget_and_timeline_management", "type": "Function", "lineno": 516}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_related_nodes_navigation", "type": "Function", "lineno": 631}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_multi_grant_proposal_organization", "type": "Function", "lineno": 689}]}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/test_main.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_main.py::test_main_module_imports", "type": "Function", "lineno": 7}, {"nodeid": "tests/academic_researcher/test_main.py::test_main_execution", "type": "Function", "lineno": 15}]}, {"nodeid": "tests/academic_researcher/test_performance_optimized.py::TestOptimizedPerformance", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_performance_optimized.py::TestOptimizedPerformance::test_basic_performance", "type": "Function", "lineno": 108}, {"nodeid": "tests/academic_researcher/test_performance_optimized.py::TestOptimizedPerformance::test_note_linking_performance", "type": "Function", "lineno": 163}, {"nodeid": "tests/academic_researcher/test_performance_optimized.py::TestOptimizedPerformance::test_citation_processing", "type": "Function", "lineno": 188}]}, {"nodeid": "tests/academic_researcher/test_performance_optimized.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_performance_optimized.py::TestOptimizedPerformance", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions::test_create_research_question", "type": "Function", "lineno": 30}, {"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions::test_adding_supporting_evidence", "type": "Function", "lineno": 60}, {"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions::test_adding_contradicting_evidence", "type": "Function", "lineno": 116}, {"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions::test_balanced_evidence_evaluation", "type": "Function", "lineno": 158}, {"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions::test_evidence_strength_levels", "type": "Function", "lineno": 253}, {"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions::test_evidence_with_multiple_citations", "type": "Function", "lineno": 337}, {"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions::test_related_questions", "type": "Function", "lineno": 385}, {"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions::test_identifying_knowledge_gaps", "type": "Function", "lineno": 504}]}, {"nodeid": "tests/academic_researcher/test_research_questions.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions", "type": "Class"}]}, {"nodeid": "tests/academic_researcher/test_workflows.py::TestUserWorkflows", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_workflows.py::TestUserWorkflows::test_workflow_paper_import_to_note_creation", "type": "Function", "lineno": 31}, {"nodeid": "tests/academic_researcher/test_workflows.py::TestUserWorkflows::test_workflow_research_question_analysis", "type": "Function", "lineno": 83}, {"nodeid": "tests/academic_researcher/test_workflows.py::TestUserWorkflows::test_workflow_grant_proposal_assembly", "type": "Function", "lineno": 158}, {"nodeid": "tests/academic_researcher/test_workflows.py::TestUserWorkflows::test_workflow_collaborative_annotation", "type": "Function", "lineno": 263}, {"nodeid": "tests/academic_researcher/test_workflows.py::TestUserWorkflows::test_workflow_experiment_documentation", "type": "Function", "lineno": 333}]}, {"nodeid": "tests/academic_researcher/test_workflows.py", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/test_workflows.py::TestUserWorkflows", "type": "Class"}]}, {"nodeid": "tests/academic_researcher", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher/citations", "type": "Package"}, {"nodeid": "tests/academic_researcher/collaboration", "type": "Package"}, {"nodeid": "tests/academic_researcher/core", "type": "Package"}, {"nodeid": "tests/academic_researcher/experiments", "type": "Package"}, {"nodeid": "tests/academic_researcher/grants", "type": "Package"}, {"nodeid": "tests/academic_researcher/notes", "type": "Package"}, {"nodeid": "tests/academic_researcher/research", "type": "Package"}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py", "type": "Module"}, {"nodeid": "tests/academic_researcher/test_citation_accuracy.py", "type": "Module"}, {"nodeid": "tests/academic_researcher/test_cli.py", "type": "Module"}, {"nodeid": "tests/academic_researcher/test_collaboration.py", "type": "Module"}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py", "type": "Module"}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py", "type": "Module"}, {"nodeid": "tests/academic_researcher/test_main.py", "type": "Module"}, {"nodeid": "tests/academic_researcher/test_performance_optimized.py", "type": "Module"}, {"nodeid": "tests/academic_researcher/test_research_questions.py", "type": "Module"}, {"nodeid": "tests/academic_researcher/test_workflows.py", "type": "Module"}]}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_initialization", "type": "Function", "lineno": 15}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_add_competitor", "type": "Function", "lineno": 28}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_add_competitive_feature", "type": "Function", "lineno": 53}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_add_market_gap", "type": "Function", "lineno": 78}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_get_competitor", "type": "Function", "lineno": 103}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_get_competitive_feature", "type": "Function", "lineno": 120}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_get_market_gap", "type": "Function", "lineno": 137}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_get_all_competitors", "type": "Function", "lineno": 154}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_get_all_competitive_features", "type": "Function", "lineno": 169}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_get_all_market_gaps", "type": "Function", "lineno": 184}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_update_competitor_feature", "type": "Function", "lineno": 199}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_update_feature_implementation", "type": "Function", "lineno": 227}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_compare_features", "type": "Function", "lineno": 266}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_identify_gaps", "type": "Function", "lineno": 313}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_create_market_gap_from_analysis", "type": "Function", "lineno": 348}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_generate_competitive_matrix", "type": "Function", "lineno": 382}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_track_competitive_timeline", "type": "Function", "lineno": 433}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_generate_feature_parity_report", "type": "Function", "lineno": 472}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_performance_with_large_dataset", "type": "Function", "lineno": 530}]}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem", "type": "Class"}]}, {"nodeid": "tests/product_manager/competitive_analysis", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/competitive_analysis/test_system.py", "type": "Module"}]}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_initialization", "type": "Function", "lineno": 15}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_add_decision", "type": "Function", "lineno": 24}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_get_decision", "type": "Function", "lineno": 48}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_get_all_decisions", "type": "Function", "lineno": 65}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_add_alternative_to_decision", "type": "Function", "lineno": 80}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_link_related_decisions", "type": "Function", "lineno": 142}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_record_outcome_assessment", "type": "Function", "lineno": 186}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_search_decisions", "type": "Function", "lineno": 210}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_get_decision_history", "type": "Function", "lineno": 247}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_build_decision_graph", "type": "Function", "lineno": 278}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_analyze_alternatives", "type": "Function", "lineno": 333}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_generate_decision_template", "type": "Function", "lineno": 375}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_export_decision", "type": "Function", "lineno": 413}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_calculate_decision_stats", "type": "Function", "lineno": 452}]}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry", "type": "Class"}]}, {"nodeid": "tests/product_manager/decision_registry", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/decision_registry/test_registry.py", "type": "Module"}]}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_initialization", "type": "Function", "lineno": 14}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_add_feedback", "type": "Function", "lineno": 29}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_get_feedback", "type": "Function", "lineno": 54}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_get_all_feedback", "type": "Function", "lineno": 71}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_analyze_sentiment", "type": "Function", "lineno": 86}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_cluster_feedback_kmeans", "type": "Function", "lineno": 117}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_cluster_feedback_dbscan", "type": "Function", "lineno": 144}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_extract_themes", "type": "Function", "lineno": 164}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_detect_trends", "type": "Function", "lineno": 192}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_get_cluster", "type": "Function", "lineno": 229}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_get_theme", "type": "Function", "lineno": 248}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_get_all_clusters", "type": "Function", "lineno": 267}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_get_all_themes", "type": "Function", "lineno": 283}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_search_feedback", "type": "Function", "lineno": 299}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_get_feedback_by_theme", "type": "Function", "lineno": 315}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_get_feedback_by_cluster", "type": "Function", "lineno": 343}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_performance_with_large_dataset", "type": "Function", "lineno": 363}]}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine", "type": "Class"}]}, {"nodeid": "tests/product_manager/feedback_analysis", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/feedback_analysis/test_engine.py", "type": "Module"}]}, {"nodeid": "tests/product_manager/fixtures/test_data.py", "outcome": "passed", "result": []}, {"nodeid": "tests/product_manager/fixtures", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/fixtures/test_data.py", "type": "Module"}]}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_initialization", "type": "Function", "lineno": 15}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_add_feature", "type": "Function", "lineno": 27}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_add_strategic_goal", "type": "Function", "lineno": 52}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_get_feature", "type": "Function", "lineno": 77}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_get_strategic_goal", "type": "Function", "lineno": 94}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_get_all_features", "type": "Function", "lineno": 111}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_get_all_strategic_goals", "type": "Function", "lineno": 126}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_map_strategic_alignment", "type": "Function", "lineno": 141}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_calculate_strategic_score", "type": "Function", "lineno": 179}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_prioritize_features_weighted", "type": "Function", "lineno": 215}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_prioritize_features_other_models", "type": "Function", "lineno": 251}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_analyze_dependencies", "type": "Function", "lineno": 289}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_generate_roadmap", "type": "Function", "lineno": 355}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_estimate_roi", "type": "Function", "lineno": 389}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_performance_with_large_dataset", "type": "Function", "lineno": 439}]}, {"nodeid": "tests/product_manager/prioritization/test_framework.py", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework", "type": "Class"}]}, {"nodeid": "tests/product_manager/prioritization", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/prioritization/test_framework.py", "type": "Module"}]}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_initialization", "type": "Function", "lineno": 14}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_add_stakeholder", "type": "Function", "lineno": 27}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_add_perspective", "type": "Function", "lineno": 52}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_add_relationship", "type": "Function", "lineno": 85}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_stakeholder", "type": "Function", "lineno": 113}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_perspective", "type": "Function", "lineno": 130}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_relationship", "type": "Function", "lineno": 148}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_all_stakeholders", "type": "Function", "lineno": 166}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_all_perspectives", "type": "Function", "lineno": 181}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_all_relationships", "type": "Function", "lineno": 197}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_stakeholder_perspectives", "type": "Function", "lineno": 213}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_perspectives_by_topic", "type": "Function", "lineno": 251}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_stakeholder_relationships", "type": "Function", "lineno": 282}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_detect_conflicts", "type": "Function", "lineno": 326}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_identify_consensus", "type": "Function", "lineno": 388}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_integrate_perspectives", "type": "Function", "lineno": 438}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_generate_stakeholder_map", "type": "Function", "lineno": 491}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_analyze_stakeholder_influence", "type": "Function", "lineno": 536}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_generate_stakeholder_matrix", "type": "Function", "lineno": 565}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_analyze_perspective_alignment", "type": "Function", "lineno": 616}]}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager", "type": "Class"}]}, {"nodeid": "tests/product_manager/stakeholder_insights", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py", "type": "Module"}]}, {"nodeid": "tests/product_manager", "outcome": "passed", "result": [{"nodeid": "tests/product_manager/competitive_analysis", "type": "Package"}, {"nodeid": "tests/product_manager/decision_registry", "type": "Package"}, {"nodeid": "tests/product_manager/feedback_analysis", "type": "Package"}, {"nodeid": "tests/product_manager/fixtures", "type": "Package"}, {"nodeid": "tests/product_manager/prioritization", "type": "Package"}, {"nodeid": "tests/product_manager/stakeholder_insights", "type": "Package"}]}, {"nodeid": "tests", "outcome": "passed", "result": [{"nodeid": "tests/academic_researcher", "type": "Package"}, {"nodeid": "tests/product_manager", "type": "Package"}]}], "tests": [{"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_citation_with_minimal_data", "lineno": 14, "outcome": "passed", "keywords": ["test_format_citation_with_minimal_data", "TestCitationFormatterEdgeCases", "test_formatter_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.001009116880595684, "outcome": "passed"}, "call": {"duration": 0.0005591972731053829, "outcome": "passed"}, "teardown": {"duration": 0.000266458373516798, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_citation_with_missing_optional_fields", "lineno": 30, "outcome": "passed", "keywords": ["test_format_citation_with_missing_optional_fields", "TestCitationFormatterEdgeCases", "test_formatter_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0001924019306898117, "outcome": "passed"}, "call": {"duration": 0.0003703916445374489, "outcome": "passed"}, "teardown": {"duration": 0.00017060525715351105, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_citation_with_all_fields", "lineno": 48, "outcome": "passed", "keywords": ["test_format_citation_with_all_fields", "TestCitationFormatterEdgeCases", "test_formatter_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00017695873975753784, "outcome": "passed"}, "call": {"duration": 0.0003551291301846504, "outcome": "passed"}, "teardown": {"duration": 0.00016312813386321068, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_author_list_with_single_author", "lineno": 76, "outcome": "passed", "keywords": ["test_format_author_list_with_single_author", "TestCitationFormatterEdgeCases", "test_formatter_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0001745498739182949, "outcome": "passed"}, "call": {"duration": 0.00023444462567567825, "outcome": "passed"}, "teardown": {"duration": 0.00017902301624417305, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_author_list_with_two_authors", "lineno": 86, "outcome": "passed", "keywords": ["test_format_author_list_with_two_authors", "TestCitationFormatterEdgeCases", "test_formatter_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00017007626593112946, "outcome": "passed"}, "call": {"duration": 0.0002621607854962349, "outcome": "passed"}, "teardown": {"duration": 0.00015722308307886124, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_author_list_with_many_authors", "lineno": 97, "outcome": "passed", "keywords": ["test_format_author_list_with_many_authors", "TestCitationFormatterEdgeCases", "test_formatter_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00016803434118628502, "outcome": "passed"}, "call": {"duration": 0.00024885497987270355, "outcome": "passed"}, "teardown": {"duration": 0.00015545962378382683, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_direct_format_citation", "lineno": 107, "outcome": "passed", "keywords": ["test_direct_format_citation", "TestCitationFormatterEdgeCases", "test_formatter_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00017667002975940704, "outcome": "passed"}, "call": {"duration": 0.00026080384850502014, "outcome": "passed"}, "teardown": {"duration": 0.00016520125791430473, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_direct_format_author_list", "lineno": 130, "outcome": "passed", "keywords": ["test_direct_format_author_list", "TestCitationFormatterEdgeCases", "test_formatter_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00017450936138629913, "outcome": "passed"}, "call": {"duration": 0.00022446690127253532, "outcome": "passed"}, "teardown": {"duration": 0.00016178283840417862, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_citation_book", "lineno": 145, "outcome": "passed", "keywords": ["test_format_citation_book", "TestCitationFormatterEdgeCases", "test_formatter_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0001704520545899868, "outcome": "passed"}, "call": {"duration": 0.0002778759226202965, "outcome": "passed"}, "teardown": {"duration": 0.00016099028289318085, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_citation_conference", "lineno": 172, "outcome": "passed", "keywords": ["test_format_citation_conference", "TestCitationFormatterEdgeCases", "test_formatter_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00017105881124734879, "outcome": "passed"}, "call": {"duration": 0.00026392610743641853, "outcome": "passed"}, "teardown": {"duration": 0.00016340380534529686, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatter_edge_cases.py::TestCitationFormatterEdgeCases::test_format_citation_with_special_characters", "lineno": 194, "outcome": "passed", "keywords": ["test_format_citation_with_special_characters", "TestCitationFormatterEdgeCases", "test_formatter_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00017478223890066147, "outcome": "passed"}, "call": {"duration": 0.00026033027097582817, "outcome": "passed"}, "teardown": {"duration": 0.00018290802836418152, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_author_list", "lineno": 45, "outcome": "passed", "keywords": ["test_format_author_list", "TestCitationFormatters", "test_formatters.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00019277585670351982, "outcome": "passed"}, "call": {"duration": 0.00027932366356253624, "outcome": "passed"}, "teardown": {"duration": 0.0001605781726539135, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_citation_apa", "lineno": 77, "outcome": "passed", "keywords": ["test_format_citation_apa", "TestCitationFormatters", "test_formatters.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00033772271126508713, "outcome": "passed"}, "call": {"duration": 0.0002295137383043766, "outcome": "passed"}, "teardown": {"duration": 0.0001934007741510868, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_citation_mla", "lineno": 89, "outcome": "passed", "keywords": ["test_format_citation_mla", "TestCitationFormatters", "test_formatters.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00030097318813204765, "outcome": "passed"}, "call": {"duration": 0.000254250131547451, "outcome": "passed"}, "teardown": {"duration": 0.00018825381994247437, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_citation_chicago", "lineno": 102, "outcome": "passed", "keywords": ["test_format_citation_chicago", "TestCitationFormatters", "test_formatters.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00028582895174622536, "outcome": "passed"}, "call": {"duration": 0.0002257530577480793, "outcome": "passed"}, "teardown": {"duration": 0.00018875021487474442, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_citation_harvard", "lineno": 115, "outcome": "passed", "keywords": ["test_format_citation_harvard", "TestCitationFormatters", "test_formatters.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0002936339005827904, "outcome": "passed"}, "call": {"duration": 0.0002233516424894333, "outcome": "passed"}, "teardown": {"duration": 0.0001932010054588318, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_citation_ieee", "lineno": 127, "outcome": "passed", "keywords": ["test_format_citation_ieee", "TestCitationFormatters", "test_formatters.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00028946204110980034, "outcome": "passed"}, "call": {"duration": 0.00024065887555480003, "outcome": "passed"}, "teardown": {"duration": 0.00018859375268220901, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_citation_vancouver", "lineno": 140, "outcome": "passed", "keywords": ["test_format_citation_vancouver", "TestCitationFormatters", "test_formatters.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0002906089648604393, "outcome": "passed"}, "call": {"duration": 0.00022301170974969864, "outcome": "passed"}, "teardown": {"duration": 0.00018455181270837784, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_citation_bibtex", "lineno": 152, "outcome": "passed", "keywords": ["test_format_citation_bibtex", "TestCitationFormatters", "test_formatters.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00028804782778024673, "outcome": "passed"}, "call": {"duration": 0.0002469872124493122, "outcome": "passed"}, "teardown": {"duration": 0.00019206805154681206, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_citation_ris", "lineno": 170, "outcome": "passed", "keywords": ["test_format_citation_ris", "TestCitationFormatters", "test_formatters.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0003952658735215664, "outcome": "passed"}, "call": {"duration": 0.00023507419973611832, "outcome": "passed"}, "teardown": {"duration": 0.00018909713253378868, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_book_citation", "lineno": 191, "outcome": "passed", "keywords": ["test_format_book_citation", "TestCitationFormatters", "test_formatters.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0002940129488706589, "outcome": "passed"}, "call": {"duration": 0.00023744581267237663, "outcome": "passed"}, "teardown": {"duration": 0.00018649175763130188, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_formatters.py::TestCitationFormatters::test_format_missing_fields", "lineno": 214, "outcome": "passed", "keywords": ["test_format_missing_fields", "TestCitationFormatters", "test_formatters.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00016914727166295052, "outcome": "passed"}, "call": {"duration": 0.0002667331136763096, "outcome": "passed"}, "teardown": {"duration": 0.00016397517174482346, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases::test_bibtex_file_parsing_basics_simple", "lineno": 20, "outcome": "passed", "keywords": ["test_bibtex_file_parsing_basics_simple", "TestCitationParserEdgeCases", "test_parser_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00019507715478539467, "outcome": "passed"}, "call": {"duration": 0.0002452721819281578, "outcome": "passed"}, "teardown": {"duration": 0.0001677451655268669, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases::test_bibtex_file_with_multiple_entries", "lineno": 28, "outcome": "passed", "keywords": ["test_bibtex_file_with_multiple_entries", "TestCitationParserEdgeCases", "test_parser_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00017142202705144882, "outcome": "passed"}, "call": {"duration": 0.006757375784218311, "outcome": "passed"}, "teardown": {"duration": 0.0001890622079372406, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases::test_parse_bibtex_file_nonexistent", "lineno": 76, "outcome": "passed", "keywords": ["test_parse_bibtex_file_nonexistent", "TestCitationParserEdgeCases", "test_parser_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00018452107906341553, "outcome": "passed"}, "call": {"duration": 0.0002499828115105629, "outcome": "passed"}, "teardown": {"duration": 0.00016553234308958054, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases::test_ris_file_parsing_simple", "lineno": 82, "outcome": "passed", "keywords": ["test_ris_file_parsing_simple", "TestCitationParserEdgeCases", "test_parser_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00017199991270899773, "outcome": "passed"}, "call": {"duration": 0.0002506040036678314, "outcome": "passed"}, "teardown": {"duration": 0.00015820888802409172, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases::test_parse_ris_file_nonexistent", "lineno": 90, "outcome": "passed", "keywords": ["test_parse_ris_file_nonexistent", "TestCitationParserEdgeCases", "test_parser_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00018585380166769028, "outcome": "passed"}, "call": {"duration": 0.0002409634180366993, "outcome": "passed"}, "teardown": {"duration": 0.00015726499259471893, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases::test_extract_pdf_metadata_direct", "lineno": 95, "outcome": "passed", "keywords": ["test_extract_pdf_metadata_direct", "TestCitationParserEdgeCases", "test_parser_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00017392495647072792, "outcome": "passed"}, "call": {"duration": 0.00025714514777064323, "outcome": "passed"}, "teardown": {"duration": 0.00016182661056518555, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases::test_extract_doi_functionality", "lineno": 111, "outcome": "passed", "keywords": ["test_extract_doi_functionality", "TestCitationParserEdgeCases", "test_parser_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00017054984346032143, "outcome": "passed"}, "call": {"duration": 0.0015911320224404335, "outcome": "passed"}, "teardown": {"duration": 0.0001779361627995968, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_parser_edge_cases.py::TestCitationParserEdgeCases::test_extract_doi_not_found", "lineno": 124, "outcome": "passed", "keywords": ["test_extract_doi_not_found", "TestCitationParserEdgeCases", "test_parser_edge_cases.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00018756557255983353, "outcome": "passed"}, "call": {"duration": 0.00249647069722414, "outcome": "passed"}, "teardown": {"duration": 0.00018763402476906776, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_parsers.py::TestCitationParsers::test_extract_doi_from_pdf", "lineno": 16, "outcome": "passed", "keywords": ["test_extract_doi_from_pdf", "TestCitationParsers", "test_parsers.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0003787442110478878, "outcome": "passed"}, "call": {"duration": 0.0003052651882171631, "outcome": "passed"}, "teardown": {"duration": 0.00021620932966470718, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_parsers.py::TestCitationParsers::test_extract_pdf_metadata", "lineno": 52, "outcome": "passed", "keywords": ["test_extract_pdf_metadata", "TestCitationParsers", "test_parsers.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0002722046338021755, "outcome": "passed"}, "call": {"duration": 0.0004071788862347603, "outcome": "passed"}, "teardown": {"duration": 0.00020134029909968376, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_parsers.py::TestCitationParsers::test_parse_bibtex_file", "lineno": 100, "outcome": "passed", "keywords": ["test_parse_bibtex_file", "TestCitationParsers", "test_parsers.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00018267100676894188, "outcome": "passed"}, "call": {"duration": 0.015056448988616467, "outcome": "passed"}, "teardown": {"duration": 0.0002007349394261837, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_parsers.py::TestCitationParsers::test_parse_ris_file", "lineno": 162, "outcome": "passed", "keywords": ["test_parse_ris_file", "TestCitationParsers", "test_parsers.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00018567172810435295, "outcome": "passed"}, "call": {"duration": 0.0006959899328649044, "outcome": "passed"}, "teardown": {"duration": 0.00017153704538941383, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_parsers.py::TestCitationParsers::test_parse_invalid_bibtex", "lineno": 229, "outcome": "passed", "keywords": ["test_parse_invalid_bibtex", "TestCitationParsers", "test_parsers.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0001881122589111328, "outcome": "passed"}, "call": {"duration": 0.008241736330091953, "outcome": "passed"}, "teardown": {"duration": 0.0002043093554675579, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/citations/test_parsers.py::TestCitationParsers::test_parse_invalid_ris", "lineno": 250, "outcome": "passed", "keywords": ["test_parse_invalid_ris", "TestCitationParsers", "test_parsers.py", "citations", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00018552597612142563, "outcome": "passed"}, "call": {"duration": 0.00043492065742611885, "outcome": "passed"}, "teardown": {"duration": 0.00017675897106528282, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_create_and_get_note", "lineno": 34, "outcome": "passed", "keywords": ["test_create_and_get_note", "TestResearchBrain", "test_brain.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0028153182938694954, "outcome": "passed"}, "call": {"duration": 0.003042568452656269, "outcome": "passed"}, "teardown": {"duration": 0.0006581060588359833, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_update_note", "lineno": 52, "outcome": "passed", "keywords": ["test_update_note", "TestResearchBrain", "test_brain.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.001278537791222334, "outcome": "passed"}, "call": {"duration": 0.004211143124848604, "outcome": "passed"}, "teardown": {"duration": 0.0010793940164148808, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_delete_note", "lineno": 78, "outcome": "passed", "keywords": ["test_delete_note", "TestResearchBrain", "test_brain.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012882761657238007, "outcome": "passed"}, "call": {"duration": 0.0024162600748240948, "outcome": "passed"}, "teardown": {"duration": 0.0005852961912751198, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_create_citation_and_link_note", "lineno": 95, "outcome": "passed", "keywords": ["test_create_citation_and_link_note", "TestResearchBrain", "test_brain.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00125159602612257, "outcome": "passed"}, "call": {"duration": 0.010568063240498304, "outcome": "passed"}, "teardown": {"duration": 0.0006182561628520489, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_create_research_question_and_add_evidence", "lineno": 129, "outcome": "failed", "keywords": ["test_create_research_question_and_add_evidence", "TestResearchBrain", "test_brain.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012439130805432796, "outcome": "passed"}, "call": {"duration": 0.005016392096877098, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.HIGH: 'high'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/core/test_brain.py", "lineno": 146, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.core.test_brain.TestResearchBrain object at 0x7fe554158130>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a85c9f60>\n\n    def test_create_research_question_and_add_evidence(self, brain):\n        \"\"\"Test creating a research question and adding evidence.\"\"\"\n        # Create a note with evidence\n        note_id = brain.create_note(\n            title=\"Evidence Note\",\n            content=\"This note contains evidence for a research question.\"\n        )\n    \n        # Create a citation\n        citation_id = brain.create_citation(\n            title=\"Supporting Paper\",\n            authors=[\"Researcher, A\"],\n            year=2023\n        )\n    \n        # Create a research question\n>       question_id = brain.create_research_question(\n            question=\"How does X affect Y?\",\n            description=\"Investigating the relationship between X and Y\",\n            tags={\"x\", \"y\", \"relationship\"},\n            priority=8\n        )\n\ntests/academic_researcher/core/test_brain.py:146: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion(), question = 'How does X affect Y?'\ntitle = 'How does X affect Y?'\ncontent = 'Investigating the relationship between X and Y'\ndescription = 'Investigating the relationship between X and Y'\npriority = <Priority.HIGH: 'high'>\nkwargs = {'description': 'Investigating the relationship between X and Y', 'knowledge_gaps': [], 'priority': <Priority.HIGH: 'high'>, 'question': 'How does X affect Y?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.HIGH: 'high'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0008209212683141232, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_create_experiment", "lineno": 181, "outcome": "failed", "keywords": ["test_create_experiment", "TestResearchBrain", "test_brain.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0014294730499386787, "outcome": "passed"}, "call": {"duration": 0.00035467324778437614, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/core/test_brain.py", "lineno": 185, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.core.test_brain.TestResearchBrain object at 0x7fe5541582e0>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a85c1630>\n\n    def test_create_experiment(self, brain):\n        \"\"\"Test creating an experiment.\"\"\"\n        # Create a research question\n>       question_id = brain.create_research_question(\n            question=\"What is the effect of drug X on condition Y?\",\n            priority=9\n        )\n\ntests/academic_researcher/core/test_brain.py:185: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'What is the effect of drug X on condition Y?'\ntitle = 'What is the effect of drug X on condition Y?', content = None\ndescription = None, priority = <Priority.CRITICAL: 'critical'>\nkwargs = {'description': None, 'knowledge_gaps': [], 'priority': <Priority.CRITICAL: 'critical'>, 'question': 'What is the effect of drug X on condition Y?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0006840359419584274, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_create_grant_proposal_and_add_items", "lineno": 215, "outcome": "failed", "keywords": ["test_create_grant_proposal_and_add_items", "TestResearchBrain", "test_brain.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0018874946981668472, "outcome": "passed"}, "call": {"duration": 0.0033474466763436794, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/core/test_brain.py", "lineno": 224, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.core.test_brain.TestResearchBrain object at 0x7fe554158490>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a8565d80>\n\n    def test_create_grant_proposal_and_add_items(self, brain):\n        \"\"\"Test creating a grant proposal and adding items to it.\"\"\"\n        # Create notes, experiments, and questions\n        note_id = brain.create_note(\n            title=\"Background Research\",\n            content=\"Literature review for grant proposal\"\n        )\n    \n>       question_id = brain.create_research_question(\n            question=\"How can we improve treatment X?\",\n            priority=9\n        )\n\ntests/academic_researcher/core/test_brain.py:224: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion(), question = 'How can we improve treatment X?'\ntitle = 'How can we improve treatment X?', content = None, description = None\npriority = <Priority.CRITICAL: 'critical'>\nkwargs = {'description': None, 'knowledge_gaps': [], 'priority': <Priority.CRITICAL: 'critical'>, 'question': 'How can we improve treatment X?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0008167983032763004, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_search", "lineno": 267, "outcome": "failed", "keywords": ["test_search", "TestResearchBrain", "test_brain.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0014400482177734375, "outcome": "passed"}, "call": {"duration": 0.005217236001044512, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.HIGH: 'high'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/core/test_brain.py", "lineno": 283, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.core.test_brain.TestResearchBrain object at 0x7fe554158670>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a83c7340>\n\n    def test_search(self, brain):\n        \"\"\"Test searching the knowledge base.\"\"\"\n        # Create some test data with a common theme\n        brain.create_note(\n            title=\"Neuroplasticity Research\",\n            content=\"Recent findings about brain plasticity\"\n        )\n    \n        brain.create_citation(\n            title=\"Neuroplasticity in Adults\",\n            authors=[\"Smith, J\"],\n            year=2023,\n            abstract=\"Research on adult neuroplasticity\"\n        )\n    \n>       brain.create_research_question(\n            question=\"How does exercise affect neuroplasticity?\",\n            priority=7\n        )\n\ntests/academic_researcher/core/test_brain.py:283: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'How does exercise affect neuroplasticity?'\ntitle = 'How does exercise affect neuroplasticity?', content = None\ndescription = None, priority = <Priority.HIGH: 'high'>\nkwargs = {'description': None, 'knowledge_gaps': [], 'priority': <Priority.HIGH: 'high'>, 'question': 'How does exercise affect neuroplasticity?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.HIGH: 'high'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0008030966855585575, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_get_related_nodes", "lineno": 303, "outcome": "failed", "keywords": ["test_get_related_nodes", "TestResearchBrain", "test_brain.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0014108391478657722, "outcome": "passed"}, "call": {"duration": 0.009953520260751247, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.MEDIUM: 'medium'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/core/test_brain.py", "lineno": 319, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.core.test_brain.TestResearchBrain object at 0x7fe554158880>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a8566ef0>\n\n    def test_get_related_nodes(self, brain):\n        \"\"\"Test retrieving related nodes.\"\"\"\n        # Create interconnected test data\n        note_id = brain.create_note(\n            title=\"Research Note\",\n            content=\"Research findings\"\n        )\n    \n        citation_id = brain.create_citation(\n            title=\"Related Paper\",\n            authors=[\"Author, A\"]\n        )\n    \n        brain.link_note_to_paper(note_id, citation_id)\n    \n>       question_id = brain.create_research_question(\n            question=\"Research Question\"\n        )\n\ntests/academic_researcher/core/test_brain.py:319: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion(), question = 'Research Question'\ntitle = 'Research Question', content = None, description = None\npriority = <Priority.MEDIUM: 'medium'>\nkwargs = {'description': None, 'knowledge_gaps': [], 'priority': <Priority.MEDIUM: 'medium'>, 'question': 'Research Question', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.MEDIUM: 'medium'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0006733890622854233, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_create_collaborator_and_annotation", "lineno": 348, "outcome": "failed", "keywords": ["test_create_collaborator_and_annotation", "TestResearchBrain", "test_brain.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0013057771138846874, "outcome": "passed"}, "call": {"duration": 0.0024108062498271465, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError: cannot pickle 'property' object"}, "traceback": [{"path": "tests/academic_researcher/core/test_brain.py", "lineno": 358, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 822, "message": "in create_collaborator"}, {"path": "researchbrain/core/storage.py", "lineno": 131, "message": "in save"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 253, "message": "in dump"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 241, "message": "in dump_all"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 27, "message": "in represent"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 48, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 207, "message": "in represent_dict"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 118, "message": "in represent_mapping"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 52, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError"}], "longrepr": "self = <tests.academic_researcher.core.test_brain.TestResearchBrain object at 0x7fe554158a90>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe600d66650>\n\n    def test_create_collaborator_and_annotation(self, brain):\n        \"\"\"Test creating a collaborator and adding annotations.\"\"\"\n        # Create a note to annotate\n        note_id = brain.create_note(\n            title=\"Note to Annotate\",\n            content=\"This note will receive annotations\"\n        )\n    \n        # Create a collaborator\n>       collaborator_id = brain.create_collaborator(\n            name=\"Jane Smith\",\n            email=\"jane@example.com\",\n            affiliation=\"University of Testing\"\n        )\n\ntests/academic_researcher/core/test_brain.py:358: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:822: in create_collaborator\n    self.storage.save(collaborator)\nresearchbrain/core/storage.py:131: in save\n    yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:253: in dump\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:241: in dump_all\n    dumper.represent(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:27: in represent\n    node = self.represent_data(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:48: in represent_data\n    node = self.yaml_representers[data_types[0]](self, data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:207: in represent_dict\n    return self.represent_mapping('tag:yaml.org,2002:map', data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:118: in represent_mapping\n    node_value = self.represent_data(item_value)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:52: in represent_data\n    node = self.yaml_multi_representers[data_type](self, data)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <yaml.dumper.Dumper object at 0x7fe4a8fe2260>\ndata = <property object at 0x7fe5ffc8e4d0>\n\n    def represent_object(self, data):\n        # We use __reduce__ API to save the data. data.__reduce__ returns\n        # a tuple of length 2-5:\n        #   (function, args, state, listitems, dictitems)\n    \n        # For reconstructing, we calls function(*args), then set its state,\n        # listitems, and dictitems if they are not None.\n    \n        # A special case is when function.__name__ == '__newobj__'. In this\n        # case we create the object with args[0].__new__(*args).\n    \n        # Another special case is when __reduce__ returns a string - we don't\n        # support it.\n    \n        # We produce a !!python/object, !!python/object/new or\n        # !!python/object/apply node.\n    \n        cls = type(data)\n        if cls in copyreg.dispatch_table:\n            reduce = copyreg.dispatch_table[cls](data)\n        elif hasattr(data, '__reduce_ex__'):\n>           reduce = data.__reduce_ex__(2)\nE           TypeError: cannot pickle 'property' object\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:317: TypeError"}, "teardown": {"duration": 0.0006600730121135712, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_generate_citation", "lineno": 382, "outcome": "passed", "keywords": ["test_generate_citation", "TestResearchBrain", "test_brain.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0013123750686645508, "outcome": "passed"}, "call": {"duration": 0.003638282883912325, "outcome": "passed"}, "teardown": {"duration": 0.0006249048747122288, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_backup_and_restore", "lineno": 412, "outcome": "passed", "keywords": ["test_backup_and_restore", "TestResearchBrain", "test_brain.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.001231457106769085, "outcome": "passed"}, "call": {"duration": 0.020120539236813784, "outcome": "passed", "stdout": "Creating backup at: /tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502\nBackup completed: /tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502\nSource path: /tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data, exists: True\nFiles in source path: [PosixPath('/tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/grants'), PosixPath('/tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/templates'), PosixPath('/tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/notes'), PosixPath('/tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/attachments'), PosixPath('/tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/annotations'), PosixPath('/tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/collaborators'), PosixPath('/tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/experiments'), PosixPath('/tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/citations'), PosixPath('/tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/indexes'), PosixPath('/tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/research_questions')]\nRestoring: /tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/grants\n  Created dir: /tmp/tmpu82ek3ae/grants\nRestoring: /tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/templates\n  Created dir: /tmp/tmpu82ek3ae/templates\nRestoring: /tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/notes\n  Created dir: /tmp/tmpu82ek3ae/notes\nRestoring: /tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/attachments\n  Created dir: /tmp/tmpu82ek3ae/attachments\nRestoring: /tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/annotations\n  Created dir: /tmp/tmpu82ek3ae/annotations\nRestoring: /tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/collaborators\n  Created dir: /tmp/tmpu82ek3ae/collaborators\nRestoring: /tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/experiments\n  Created dir: /tmp/tmpu82ek3ae/experiments\nRestoring: /tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/citations\n  Created dir: /tmp/tmpu82ek3ae/citations\nRestoring: /tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/indexes\n  Created dir: /tmp/tmpu82ek3ae/indexes\nRestoring: /tmp/tmpu82ek3ae/backups/researchbrain_backup_20250616_041502/data/research_questions\n  Created dir: /tmp/tmpu82ek3ae/research_questions\nRestore completed\n"}, "teardown": {"duration": 0.0009365510195493698, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_brain.py::TestResearchBrain::test_knowledge_graph_structure", "lineno": 450, "outcome": "failed", "keywords": ["test_knowledge_graph_structure", "TestResearchBrain", "test_brain.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012454502284526825, "outcome": "passed"}, "call": {"duration": 0.006396452896296978, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.MEDIUM: 'medium'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/core/test_brain.py", "lineno": 459, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.core.test_brain.TestResearchBrain object at 0x7fe554159060>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a853bbb0>\n\n    def test_knowledge_graph_structure(self, brain):\n        \"\"\"Test the knowledge graph structure with various interconnected nodes.\"\"\"\n        # Create interconnected test data\n        note1_id = brain.create_note(title=\"Note 1\", content=\"Content 1\")\n        note2_id = brain.create_note(title=\"Note 2\", content=\"Content 2\")\n    \n        citation_id = brain.create_citation(title=\"Citation\", authors=[\"Author\"])\n    \n>       question_id = brain.create_research_question(question=\"Question\")\n\ntests/academic_researcher/core/test_brain.py:459: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion(), question = 'Question', title = 'Question'\ncontent = None, description = None, priority = <Priority.MEDIUM: 'medium'>\nkwargs = {'description': None, 'knowledge_gaps': [], 'priority': <Priority.MEDIUM: 'medium'>, 'question': 'Question', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.MEDIUM: 'medium'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0006643328815698624, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestKnowledgeNode::test_init_default_values", "lineno": 18, "outcome": "passed", "keywords": ["test_init_default_values", "TestKnowledgeNode", "test_models.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0002061747945845127, "outcome": "passed"}, "call": {"duration": 0.0002412949688732624, "outcome": "passed"}, "teardown": {"duration": 0.00016768323257565498, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestKnowledgeNode::test_init_custom_values", "lineno": 27, "outcome": "passed", "keywords": ["test_init_custom_values", "TestKnowledgeNode", "test_models.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00018252013251185417, "outcome": "passed"}, "call": {"duration": 0.0002589491195976734, "outcome": "passed"}, "teardown": {"duration": 0.00016315188258886337, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestKnowledgeNode::test_update_method", "lineno": 45, "outcome": "failed", "keywords": ["test_update_method", "TestKnowledgeNode", "test_models.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00017050933092832565, "outcome": "passed"}, "call": {"duration": 0.00026074470952153206, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 989, "message": "AttributeError: 'BaseKnowledgeNode' object has no attribute 'update'"}, "traceback": [{"path": "tests/academic_researcher/core/test_models.py", "lineno": 53, "message": ""}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 989, "message": "AttributeError"}], "longrepr": "self = <tests.academic_researcher.core.test_models.TestKnowledgeNode object at 0x7fe55415a3b0>\n\n    def test_update_method(self):\n        \"\"\"Test the update method updates the timestamp.\"\"\"\n        test_time = datetime.datetime(2023, 1, 1, 12, 0, 0)\n        node = KnowledgeNode(updated_at=test_time)\n    \n        assert node.updated_at == test_time\n    \n>       node.update()\n\ntests/academic_researcher/core/test_models.py:53: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = BaseKnowledgeNode(id=UUID('a529eda0-eb1d-4945-8015-c9cbbb2d3a91'), created_at=datetime.datetime(2025, 6, 16, 4, 15, 2, 385328), updated_at=datetime.datetime(2023, 1, 1, 12, 0), tags=set(), metadata={})\nitem = 'update'\n\n    def __getattr__(self, item: str) -> Any:\n        private_attributes = object.__getattribute__(self, '__private_attributes__')\n        if item in private_attributes:\n            attribute = private_attributes[item]\n            if hasattr(attribute, '__get__'):\n                return attribute.__get__(self, type(self))  # type: ignore\n    \n            try:\n                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items\n                return self.__pydantic_private__[item]  # type: ignore\n            except KeyError as exc:\n                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\n        else:\n            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.\n            # See `BaseModel.__repr_args__` for more details\n            try:\n                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')\n            except AttributeError:\n                pydantic_extra = None\n    \n            if pydantic_extra:\n                try:\n                    return pydantic_extra[item]\n                except KeyError as exc:\n                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\n            else:\n                if hasattr(self.__class__, item):\n                    return super().__getattribute__(item)  # Raises AttributeError if appropriate\n                else:\n                    # this is the current error\n>                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\nE                   AttributeError: 'BaseKnowledgeNode' object has no attribute 'update'\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:989: AttributeError"}, "teardown": {"duration": 0.0002105063758790493, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestNote::test_init_minimal", "lineno": 59, "outcome": "passed", "keywords": ["test_init_minimal", "TestNote", "test_models.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00020056311041116714, "outcome": "passed"}, "call": {"duration": 0.00024569500237703323, "outcome": "passed"}, "teardown": {"duration": 0.00016396492719650269, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestNote::test_init_full", "lineno": 70, "outcome": "passed", "keywords": ["test_init_full", "TestNote", "test_models.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00018264399841427803, "outcome": "passed"}, "call": {"duration": 0.00026747630909085274, "outcome": "passed"}, "teardown": {"duration": 0.0001601330004632473, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestCitation::test_init_minimal", "lineno": 97, "outcome": "passed", "keywords": ["test_init_minimal", "TestCitation", "test_models.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0001724371686577797, "outcome": "passed"}, "call": {"duration": 0.00023106299340724945, "outcome": "passed"}, "teardown": {"duration": 0.00016630720347166061, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestCitation::test_init_full", "lineno": 113, "outcome": "passed", "keywords": ["test_init_full", "TestCitation", "test_models.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00017057126387953758, "outcome": "passed"}, "call": {"duration": 0.0002914019860327244, "outcome": "passed"}, "teardown": {"duration": 0.0001625330187380314, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestResearchQuestion::test_init_minimal", "lineno": 158, "outcome": "passed", "keywords": ["test_init_minimal", "TestResearchQuestion", "test_models.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00016992492601275444, "outcome": "passed"}, "call": {"duration": 0.00023426581174135208, "outcome": "passed"}, "teardown": {"duration": 0.00015447987243533134, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestResearchQuestion::test_init_with_evidence", "lineno": 170, "outcome": "passed", "keywords": ["test_init_with_evidence", "TestResearchQuestion", "test_models.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00017056800425052643, "outcome": "passed"}, "call": {"duration": 0.0002620830200612545, "outcome": "passed"}, "teardown": {"duration": 0.0001585632562637329, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestExperiment::test_init_minimal", "lineno": 202, "outcome": "passed", "keywords": ["test_init_minimal", "TestExperiment", "test_models.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0001705838367342949, "outcome": "passed"}, "call": {"duration": 0.0002517751418054104, "outcome": "passed"}, "teardown": {"duration": 0.00015780003741383553, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestExperiment::test_init_full", "lineno": 221, "outcome": "failed", "keywords": ["test_init_full", "TestExperiment", "test_models.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00016633328050374985, "outcome": "passed"}, "call": {"duration": 0.000589895062148571, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/tests/academic_researcher/core/test_models.py", "lineno": 247, "message": "AssertionError: assert None == datetime.datetime(2023, 1, 1, 0, 0)\n +  where None = Experiment(id=UUID('fe61a878-a633-4f09-8ed8-4b07f213d2be'), created_at=datetime.datetime(2025, 6, 16, 4, 15, 2, 433245...), notes=[UUID('df0e28da-6e47-4b01-9bab-9e1a27ae7290')], collaborators=[], template_name=None, reproducibility_info={}).start_date"}, "traceback": [{"path": "tests/academic_researcher/core/test_models.py", "lineno": 247, "message": "AssertionError"}], "longrepr": "self = <tests.academic_researcher.core.test_models.TestExperiment object at 0x7fe55415b220>\n\n    def test_init_full(self):\n        \"\"\"Test initialization with all fields.\"\"\"\n        question_id = uuid4()\n        note_id = uuid4()\n        start_date = datetime.datetime(2023, 1, 1)\n        end_date = datetime.datetime(2023, 2, 1)\n    \n        experiment = Experiment(\n            title=\"Test Experiment\",\n            hypothesis=\"X increases Y\",\n            methodology=\"Detailed methodology\",\n            status=ExperimentStatus.COMPLETED,\n            start_date=start_date,\n            end_date=end_date,\n            variables={\"x\": 10, \"y\": 20},\n            results=\"Experiment results\",\n            conclusion=\"Experiment conclusion\",\n            research_question_id=question_id,\n            notes=[note_id]\n        )\n    \n        assert experiment.title == \"Test Experiment\"\n        assert experiment.hypothesis == \"X increases Y\"\n        assert experiment.methodology == \"Detailed methodology\"\n        assert experiment.status == ExperimentStatus.COMPLETED\n>       assert experiment.start_date == start_date\nE       AssertionError: assert None == datetime.datetime(2023, 1, 1, 0, 0)\nE        +  where None = Experiment(id=UUID('fe61a878-a633-4f09-8ed8-4b07f213d2be'), created_at=datetime.datetime(2025, 6, 16, 4, 15, 2, 433245...), notes=[UUID('df0e28da-6e47-4b01-9bab-9e1a27ae7290')], collaborators=[], template_name=None, reproducibility_info={}).start_date\n\ntests/academic_researcher/core/test_models.py:247: AssertionError"}, "teardown": {"duration": 0.00026397593319416046, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestExperiment::test_end_date_validation", "lineno": 255, "outcome": "failed", "keywords": ["test_end_date_validation", "TestExperiment", "test_models.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00019233021885156631, "outcome": "passed"}, "call": {"duration": 0.00046118535101413727, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/tests/academic_researcher/core/test_models.py", "lineno": 261, "message": "Failed: DID NOT RAISE <class 'pydantic_core._pydantic_core.ValidationError'>"}, "traceback": [{"path": "tests/academic_researcher/core/test_models.py", "lineno": 261, "message": "Failed"}], "longrepr": "self = <tests.academic_researcher.core.test_models.TestExperiment object at 0x7fe55415b3a0>\n\n    def test_end_date_validation(self):\n        \"\"\"Test validation that end_date must be after start_date.\"\"\"\n        start_date = datetime.datetime(2023, 2, 1)\n        end_date = datetime.datetime(2023, 1, 1)  # Before start_date\n    \n>       with pytest.raises(ValidationError):\nE       Failed: DID NOT RAISE <class 'pydantic_core._pydantic_core.ValidationError'>\n\ntests/academic_researcher/core/test_models.py:261: Failed"}, "teardown": {"duration": 0.0002445690333843231, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestGrantProposal::test_init_minimal", "lineno": 273, "outcome": "passed", "keywords": ["test_init_minimal", "TestGrantProposal", "test_models.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00019051507115364075, "outcome": "passed"}, "call": {"duration": 0.00025418493896722794, "outcome": "passed"}, "teardown": {"duration": 0.00016562500968575478, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestGrantProposal::test_init_full", "lineno": 291, "outcome": "passed", "keywords": ["test_init_full", "TestGrantProposal", "test_models.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00017656339332461357, "outcome": "passed"}, "call": {"duration": 0.0002702069468796253, "outcome": "passed"}, "teardown": {"duration": 0.00016282498836517334, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestCollaborator::test_init_minimal", "lineno": 327, "outcome": "passed", "keywords": ["test_init_minimal", "TestCollaborator", "test_models.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00018052617087960243, "outcome": "passed"}, "call": {"duration": 0.00022842176258563995, "outcome": "passed"}, "teardown": {"duration": 0.00017161481082439423, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_models.py::TestCollaborator::test_init_full", "lineno": 337, "outcome": "passed", "keywords": ["test_init_full", "TestCollaborator", "test_models.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00017634499818086624, "outcome": "passed"}, "call": {"duration": 0.00023936526849865913, "outcome": "passed"}, "teardown": {"duration": 0.0001733154058456421, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_init_creates_directories", "lineno": 31, "outcome": "passed", "keywords": ["test_init_creates_directories", "TestLocalStorage", "test_storage.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00041925814002752304, "outcome": "passed"}, "call": {"duration": 0.0008544698357582092, "outcome": "passed"}, "teardown": {"duration": 0.0005590156652033329, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_save_and_get_note", "lineno": 51, "outcome": "passed", "keywords": ["test_save_and_get_note", "TestLocalStorage", "test_storage.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0007197940722107887, "outcome": "passed"}, "call": {"duration": 0.0021054563112556934, "outcome": "passed"}, "teardown": {"duration": 0.0006262417882680893, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_save_and_get_citation", "lineno": 68, "outcome": "passed", "keywords": ["test_save_and_get_citation", "TestLocalStorage", "test_storage.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.000722584780305624, "outcome": "passed"}, "call": {"duration": 0.0033673602156341076, "outcome": "passed"}, "teardown": {"duration": 0.0005963887088000774, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_update_existing_item", "lineno": 88, "outcome": "passed", "keywords": ["test_update_existing_item", "TestLocalStorage", "test_storage.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0007168040610849857, "outcome": "passed"}, "call": {"duration": 0.0036710379645228386, "outcome": "passed"}, "teardown": {"duration": 0.0006264201365411282, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_get_nonexistent_item", "lineno": 102, "outcome": "passed", "keywords": ["test_get_nonexistent_item", "TestLocalStorage", "test_storage.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0007476122118532658, "outcome": "passed"}, "call": {"duration": 0.00029360130429267883, "outcome": "passed"}, "teardown": {"duration": 0.0005523739382624626, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_get_invalid_file", "lineno": 109, "outcome": "passed", "keywords": ["test_get_invalid_file", "TestLocalStorage", "test_storage.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.000708491075783968, "outcome": "passed"}, "call": {"duration": 0.0008741742931306362, "outcome": "passed"}, "teardown": {"duration": 0.0005877143703401089, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_delete_item", "lineno": 121, "outcome": "passed", "keywords": ["test_delete_item", "TestLocalStorage", "test_storage.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0007153218612074852, "outcome": "passed"}, "call": {"duration": 0.0020348099060356617, "outcome": "passed"}, "teardown": {"duration": 0.0005734451115131378, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_delete_nonexistent_item", "lineno": 136, "outcome": "passed", "keywords": ["test_delete_nonexistent_item", "TestLocalStorage", "test_storage.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0007086517289280891, "outcome": "passed"}, "call": {"duration": 0.00029718875885009766, "outcome": "passed"}, "teardown": {"duration": 0.0005463859997689724, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_list_all", "lineno": 143, "outcome": "passed", "keywords": ["test_list_all", "TestLocalStorage", "test_storage.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0007301499135792255, "outcome": "passed"}, "call": {"duration": 0.009330128785222769, "outcome": "passed"}, "teardown": {"duration": 0.0006410074420273304, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_query", "lineno": 173, "outcome": "passed", "keywords": ["test_query", "TestLocalStorage", "test_storage.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0007284600287675858, "outcome": "passed"}, "call": {"duration": 0.007185756228864193, "outcome": "passed"}, "teardown": {"duration": 0.0006098700687289238, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_save_and_get_attachment", "lineno": 193, "outcome": "passed", "keywords": ["test_save_and_get_attachment", "TestLocalStorage", "test_storage.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0007083159871399403, "outcome": "passed"}, "call": {"duration": 0.000502259936183691, "outcome": "passed"}, "teardown": {"duration": 0.0005771201103925705, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_get_nonexistent_attachment", "lineno": 218, "outcome": "passed", "keywords": ["test_get_nonexistent_attachment", "TestLocalStorage", "test_storage.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0007059569470584393, "outcome": "passed"}, "call": {"duration": 0.0002530030906200409, "outcome": "passed"}, "teardown": {"duration": 0.0005525010637938976, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_export_to_dataframe", "lineno": 224, "outcome": "passed", "keywords": ["test_export_to_dataframe", "TestLocalStorage", "test_storage.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0007018218748271465, "outcome": "passed"}, "call": {"duration": 0.005567790940403938, "outcome": "passed"}, "teardown": {"duration": 0.0006604800000786781, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_backup_and_restore", "lineno": 245, "outcome": "passed", "keywords": ["test_backup_and_restore", "TestLocalStorage", "test_storage.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0007301620207726955, "outcome": "passed"}, "call": {"duration": 0.006556292995810509, "outcome": "passed"}, "teardown": {"duration": 0.0006621270440518856, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/core/test_storage.py::TestLocalStorage::test_search_text", "lineno": 289, "outcome": "passed", "keywords": ["test_search_text", "TestLocalStorage", "test_storage.py", "core", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0007264427840709686, "outcome": "passed"}, "call": {"duration": 0.00753155630081892, "outcome": "passed"}, "teardown": {"duration": 0.0006283493712544441, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/experiments/test_template_functions.py::TestTemplatesFunctions::test_create_default_templates", "lineno": 35, "outcome": "passed", "keywords": ["test_create_default_templates", "TestTemplatesFunctions", "test_template_functions.py", "experiments", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0006686551496386528, "outcome": "passed"}, "call": {"duration": 0.026336459908634424, "outcome": "passed"}, "teardown": {"duration": 0.0003321547992527485, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/experiments/test_template_functions.py::TestTemplatesFunctions::test_list_templates", "lineno": 47, "outcome": "passed", "keywords": ["test_list_templates", "TestTemplatesFunctions", "test_template_functions.py", "experiments", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00036805542185902596, "outcome": "passed"}, "call": {"duration": 0.02653793804347515, "outcome": "passed"}, "teardown": {"duration": 0.00033980095759034157, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/experiments/test_template_functions.py::TestTemplatesFunctions::test_get_template", "lineno": 70, "outcome": "passed", "keywords": ["test_get_template", "TestTemplatesFunctions", "test_template_functions.py", "experiments", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0003606448881328106, "outcome": "passed"}, "call": {"duration": 0.04616593010723591, "outcome": "passed"}, "teardown": {"duration": 0.00032515358179807663, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/experiments/test_template_functions.py::TestTemplatesFunctions::test_create_template", "lineno": 88, "outcome": "passed", "keywords": ["test_create_template", "TestTemplatesFunctions", "test_template_functions.py", "experiments", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0003687189891934395, "outcome": "passed"}, "call": {"duration": 0.0028541008941829205, "outcome": "passed"}, "teardown": {"duration": 0.00029102619737386703, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/experiments/test_template_functions.py::TestTemplatesFunctions::test_apply_template", "lineno": 119, "outcome": "passed", "keywords": ["test_apply_template", "TestTemplatesFunctions", "test_template_functions.py", "experiments", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0003666328266263008, "outcome": "passed"}, "call": {"duration": 0.00664003100246191, "outcome": "passed"}, "teardown": {"duration": 0.00030178017914295197, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/experiments/test_templates.py::TestExperimentTemplates::test_create_default_templates", "lineno": 30, "outcome": "passed", "keywords": ["test_create_default_templates", "TestExperimentTemplates", "test_templates.py", "experiments", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0005181888118386269, "outcome": "passed"}, "call": {"duration": 0.03589826403185725, "outcome": "passed"}, "teardown": {"duration": 0.00038604019209742546, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/experiments/test_templates.py::TestExperimentTemplates::test_get_template", "lineno": 51, "outcome": "passed", "keywords": ["test_get_template", "TestExperimentTemplates", "test_templates.py", "experiments", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0004750331863760948, "outcome": "passed"}, "call": {"duration": 0.04639045614749193, "outcome": "passed"}, "teardown": {"duration": 0.00038264086470007896, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/experiments/test_templates.py::TestExperimentTemplates::test_list_templates", "lineno": 74, "outcome": "passed", "keywords": ["test_list_templates", "TestExperimentTemplates", "test_templates.py", "experiments", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00046945689246058464, "outcome": "passed"}, "call": {"duration": 0.02583651104941964, "outcome": "passed"}, "teardown": {"duration": 0.0003753309138119221, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/experiments/test_templates.py::TestExperimentTemplates::test_create_template", "lineno": 88, "outcome": "passed", "keywords": ["test_create_template", "TestExperimentTemplates", "test_templates.py", "experiments", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0004707900807261467, "outcome": "passed"}, "call": {"duration": 0.00496049877256155, "outcome": "passed"}, "teardown": {"duration": 0.00033754901960492134, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/experiments/test_templates.py::TestExperimentTemplates::test_apply_template", "lineno": 127, "outcome": "passed", "keywords": ["test_apply_template", "TestExperimentTemplates", "test_templates.py", "experiments", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0004648449830710888, "outcome": "passed"}, "call": {"duration": 0.007475321181118488, "outcome": "passed"}, "teardown": {"duration": 0.00036631524562835693, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/experiments/test_templates.py::TestExperimentTemplates::test_template_validation", "lineno": 203, "outcome": "passed", "keywords": ["test_template_validation", "TestExperimentTemplates", "test_templates.py", "experiments", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00046894699335098267, "outcome": "passed"}, "call": {"duration": 0.00023268768563866615, "outcome": "passed"}, "teardown": {"duration": 0.0003042141906917095, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/grants/test_export.py::TestGrantExport::test_export_proposal_markdown", "lineno": 85, "outcome": "passed", "keywords": ["test_export_proposal_markdown", "TestGrantExport", "test_export.py", "grants", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0009037829004228115, "outcome": "passed"}, "call": {"duration": 0.024445924907922745, "outcome": "passed"}, "teardown": {"duration": 0.00029592402279376984, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/grants/test_export.py::TestGrantExport::test_export_proposal_yaml", "lineno": 124, "outcome": "passed", "keywords": ["test_export_proposal_yaml", "TestGrantExport", "test_export.py", "grants", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0006170817650854588, "outcome": "passed"}, "call": {"duration": 0.03724227286875248, "outcome": "passed"}, "teardown": {"duration": 0.0002827523276209831, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/grants/test_export.py::TestGrantExport::test_export_unsupported_format", "lineno": 175, "outcome": "passed", "keywords": ["test_export_unsupported_format", "TestGrantExport", "test_export.py", "grants", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0006162319332361221, "outcome": "passed"}, "call": {"duration": 0.02317698672413826, "outcome": "passed"}, "teardown": {"duration": 0.0003061327151954174, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/grants/test_export.py::TestGrantExport::test_convert_uuids_to_strings", "lineno": 202, "outcome": "passed", "keywords": ["test_convert_uuids_to_strings", "TestGrantExport", "test_export.py", "grants", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00018705008551478386, "outcome": "passed"}, "call": {"duration": 0.0002808147110044956, "outcome": "passed"}, "teardown": {"duration": 0.00017006415873765945, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/grants/test_export_functions.py::TestGrantExportFunctions::test_export_proposal_markdown", "lineno": 119, "outcome": "passed", "keywords": ["test_export_proposal_markdown", "TestGrantExportFunctions", "test_export_functions.py", "grants", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0006231316365301609, "outcome": "passed"}, "call": {"duration": 0.023338533006608486, "outcome": "passed"}, "teardown": {"duration": 0.00027961796149611473, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/grants/test_export_functions.py::TestGrantExportFunctions::test_export_proposal_yaml", "lineno": 153, "outcome": "passed", "keywords": ["test_export_proposal_yaml", "TestGrantExportFunctions", "test_export_functions.py", "grants", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00061429338529706, "outcome": "passed"}, "call": {"duration": 0.0428366819396615, "outcome": "passed"}, "teardown": {"duration": 0.0002854582853615284, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/grants/test_export_functions.py::TestGrantExportFunctions::test_export_proposal_unknown_extension", "lineno": 186, "outcome": "passed", "keywords": ["test_export_proposal_unknown_extension", "TestGrantExportFunctions", "test_export_functions.py", "grants", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0006196447648108006, "outcome": "passed"}, "call": {"duration": 0.02313890540972352, "outcome": "passed"}, "teardown": {"duration": 0.00029821600764989853, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/grants/test_export_functions.py::TestGrantExportFunctions::test_export_markdown_error_handling", "lineno": 211, "outcome": "passed", "keywords": ["test_export_markdown_error_handling", "TestGrantExportFunctions", "test_export_functions.py", "grants", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0006758901290595531, "outcome": "passed"}, "call": {"duration": 0.02243525581434369, "outcome": "passed"}, "teardown": {"duration": 0.00031539471819996834, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/grants/test_export_functions.py::TestGrantExportFunctions::test_export_yaml_error_handling", "lineno": 236, "outcome": "passed", "keywords": ["test_export_yaml_error_handling", "TestGrantExportFunctions", "test_export_functions.py", "grants", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0006808978505432606, "outcome": "passed"}, "call": {"duration": 0.0007604258134961128, "outcome": "passed", "stdout": "YAML export error: Test exception\n"}, "teardown": {"duration": 0.0002998742274940014, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking::test_note_citation_linking", "lineno": 29, "outcome": "passed", "keywords": ["test_note_citation_linking", "TestBidirectionalLinking", "test_bidirectional_linking.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.001371484249830246, "outcome": "passed"}, "call": {"duration": 0.009730903897434473, "outcome": "passed"}, "teardown": {"duration": 0.000641705933958292, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking::test_automatic_citation_extraction", "lineno": 68, "outcome": "passed", "keywords": ["test_automatic_citation_extraction", "TestBidirectionalLinking", "test_bidirectional_linking.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012693940661847591, "outcome": "passed"}, "call": {"duration": 0.00947639998048544, "outcome": "passed"}, "teardown": {"duration": 0.0006212838925421238, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking::test_multiple_citation_links", "lineno": 95, "outcome": "passed", "keywords": ["test_multiple_citation_links", "TestBidirectionalLinking", "test_bidirectional_linking.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.001251094974577427, "outcome": "passed"}, "call": {"duration": 0.025261724833399057, "outcome": "passed"}, "teardown": {"duration": 0.0006451550871133804, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking::test_removing_citation_links", "lineno": 134, "outcome": "failed", "keywords": ["test_removing_citation_links", "TestBidirectionalLinking", "test_bidirectional_linking.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012369919568300247, "outcome": "passed"}, "call": {"duration": 0.010574711952358484, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 989, "message": "AttributeError: 'Note' object has no attribute 'update'"}, "traceback": [{"path": "tests/academic_researcher/test_bidirectional_linking.py", "lineno": 163, "message": ""}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 989, "message": "AttributeError"}], "longrepr": "self = <tests.academic_researcher.test_bidirectional_linking.TestBidirectionalLinking object at 0x7fe54ff3ee60>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe54ff3eb00>\n\n    def test_removing_citation_links(self, brain):\n        \"\"\"Test removing links between notes and citations.\"\"\"\n        # Create a citation\n        citation_id = brain.create_citation(\n            title=\"Test Paper\",\n            authors=[\"Author, Test\"]\n        )\n    \n        # Create a note with citation reference\n        note_id = brain.create_note(\n            title=\"Test Note\",\n            content=\"This references [@test].\"\n        )\n    \n        # Link note to citation\n        brain.link_note_to_paper(note_id, citation_id)\n    \n        # Verify link exists\n        note = brain.get_note(note_id)\n        citation = brain.storage.get(Citation, citation_id)\n        assert citation_id in note.citations\n        assert note_id in citation.notes\n    \n        # Manually remove the link by updating the note and citation objects\n        # First, update the note to remove citation\n        note.citations.remove(citation_id)\n        note.source = None\n        note.page_reference = None\n>       note.update()\n\ntests/academic_researcher/test_bidirectional_linking.py:163: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Note(id=UUID('9ac2b6a6-2d28-497a-9a7e-6edfd0a4e00f'), created_at=datetime.datetime(2025, 6, 16, 4, 15, 3, 104480), upd...tent='This references [@test].', source=None, page_reference=None, attachments=[], citations=[], section_references={})\nitem = 'update'\n\n    def __getattr__(self, item: str) -> Any:\n        private_attributes = object.__getattribute__(self, '__private_attributes__')\n        if item in private_attributes:\n            attribute = private_attributes[item]\n            if hasattr(attribute, '__get__'):\n                return attribute.__get__(self, type(self))  # type: ignore\n    \n            try:\n                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items\n                return self.__pydantic_private__[item]  # type: ignore\n            except KeyError as exc:\n                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\n        else:\n            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.\n            # See `BaseModel.__repr_args__` for more details\n            try:\n                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')\n            except AttributeError:\n                pydantic_extra = None\n    \n            if pydantic_extra:\n                try:\n                    return pydantic_extra[item]\n                except KeyError as exc:\n                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\n            else:\n                if hasattr(self.__class__, item):\n                    return super().__getattribute__(item)  # Raises AttributeError if appropriate\n                else:\n                    # this is the current error\n>                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\nE                   AttributeError: 'Note' object has no attribute 'update'\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:989: AttributeError"}, "teardown": {"duration": 0.000662440899759531, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking::test_cascading_deletion", "lineno": 187, "outcome": "passed", "keywords": ["test_cascading_deletion", "TestBidirectionalLinking", "test_bidirectional_linking.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012488518841564655, "outcome": "passed"}, "call": {"duration": 0.019368068780750036, "outcome": "passed"}, "teardown": {"duration": 0.0006143189966678619, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking::test_navigation_between_linked_items", "lineno": 229, "outcome": "passed", "keywords": ["test_navigation_between_linked_items", "TestBidirectionalLinking", "test_bidirectional_linking.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012435629032552242, "outcome": "passed"}, "call": {"duration": 0.017680002842098475, "outcome": "passed"}, "teardown": {"duration": 0.0006197481416165829, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking::test_circular_navigation", "lineno": 277, "outcome": "failed", "keywords": ["test_circular_navigation", "TestBidirectionalLinking", "test_bidirectional_linking.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012390599586069584, "outcome": "passed"}, "call": {"duration": 0.01286028092727065, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 989, "message": "AttributeError: 'Note' object has no attribute 'update'"}, "traceback": [{"path": "tests/academic_researcher/test_bidirectional_linking.py", "lineno": 308, "message": ""}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 989, "message": "AttributeError"}], "longrepr": "self = <tests.academic_researcher.test_bidirectional_linking.TestBidirectionalLinking object at 0x7fe54ff3f490>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a85d1b10>\n\n    def test_circular_navigation(self, brain):\n        \"\"\"Test circular navigation through the knowledge graph.\"\"\"\n        # Create multiple nodes with circular references\n        citation_id = brain.create_citation(\n            title=\"Original Paper\",\n            authors=[\"Original, Author\"]\n        )\n    \n        note1_id = brain.create_note(\n            title=\"First Note\",\n            content=\"Analysis of the original paper.\"\n        )\n    \n        note2_id = brain.create_note(\n            title=\"Second Note\",\n            content=\"Further thoughts on the first note.\"\n        )\n    \n        note3_id = brain.create_note(\n            title=\"Third Note\",\n            content=\"Synthesis connecting back to the original paper.\"\n        )\n    \n        # Create connections forming a cycle\n        brain.link_note_to_paper(note1_id, citation_id)\n    \n        # Manually create links between notes\n        # Make note2 reference note1\n        note2 = brain.get_note(note2_id)\n        note2.content = f\"Further thoughts on the first note [{note1_id}].\"\n>       note2.update()\n\ntests/academic_researcher/test_bidirectional_linking.py:308: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Note(id=UUID('d57d3ca4-a262-45bf-a346-a87d869f52ee'), created_at=datetime.datetime(2025, 6, 16, 4, 15, 3, 201385), upd...-6e4a-44e2-8cc6-e0a648993a36].', source=None, page_reference=None, attachments=[], citations=[], section_references={})\nitem = 'update'\n\n    def __getattr__(self, item: str) -> Any:\n        private_attributes = object.__getattribute__(self, '__private_attributes__')\n        if item in private_attributes:\n            attribute = private_attributes[item]\n            if hasattr(attribute, '__get__'):\n                return attribute.__get__(self, type(self))  # type: ignore\n    \n            try:\n                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items\n                return self.__pydantic_private__[item]  # type: ignore\n            except KeyError as exc:\n                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\n        else:\n            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.\n            # See `BaseModel.__repr_args__` for more details\n            try:\n                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')\n            except AttributeError:\n                pydantic_extra = None\n    \n            if pydantic_extra:\n                try:\n                    return pydantic_extra[item]\n                except KeyError as exc:\n                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\n            else:\n                if hasattr(self.__class__, item):\n                    return super().__getattribute__(item)  # Raises AttributeError if appropriate\n                else:\n                    # this is the current error\n>                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\nE                   AttributeError: 'Note' object has no attribute 'update'\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:989: AttributeError"}, "teardown": {"duration": 0.0006726430729031563, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_bidirectional_linking.py::TestBidirectionalLinking::test_section_references", "lineno": 343, "outcome": "passed", "keywords": ["test_section_references", "TestBidirectionalLinking", "test_bidirectional_linking.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012473398819565773, "outcome": "passed"}, "call": {"duration": 0.016720097977668047, "outcome": "passed"}, "teardown": {"duration": 0.0006372588686645031, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_citation_accuracy.py::TestCitationAccuracy::test_apa_citation_accuracy", "lineno": 30, "outcome": "passed", "keywords": ["test_apa_citation_accuracy", "TestCitationAccuracy", "test_citation_accuracy.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012369947507977486, "outcome": "passed"}, "call": {"duration": 0.0036286842077970505, "outcome": "passed"}, "teardown": {"duration": 0.0005863308906555176, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_citation_accuracy.py::TestCitationAccuracy::test_mla_citation_accuracy", "lineno": 58, "outcome": "passed", "keywords": ["test_mla_citation_accuracy", "TestCitationAccuracy", "test_citation_accuracy.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012631169520318508, "outcome": "passed"}, "call": {"duration": 0.0035123839043080807, "outcome": "passed"}, "teardown": {"duration": 0.0005838661454617977, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_citation_accuracy.py::TestCitationAccuracy::test_chicago_citation_accuracy", "lineno": 85, "outcome": "passed", "keywords": ["test_chicago_citation_accuracy", "TestCitationAccuracy", "test_citation_accuracy.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.001230376772582531, "outcome": "passed"}, "call": {"duration": 0.00351478299126029, "outcome": "passed"}, "teardown": {"duration": 0.0005868342705070972, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_citation_accuracy.py::TestCitationAccuracy::test_bibtex_citation_accuracy", "lineno": 112, "outcome": "passed", "keywords": ["test_bibtex_citation_accuracy", "TestCitationAccuracy", "test_citation_accuracy.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012487168423831463, "outcome": "passed"}, "call": {"duration": 0.0038334112614393234, "outcome": "passed"}, "teardown": {"duration": 0.0006045028567314148, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_citation_accuracy.py::TestCitationAccuracy::test_bibtex_parser_accuracy", "lineno": 146, "outcome": "passed", "keywords": ["test_bibtex_parser_accuracy", "TestCitationAccuracy", "test_citation_accuracy.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00018710922449827194, "outcome": "passed"}, "call": {"duration": 0.016720464918762445, "outcome": "passed"}, "teardown": {"duration": 0.0001919870264828205, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_citation_accuracy.py::TestCitationAccuracy::test_ris_parser_accuracy", "lineno": 223, "outcome": "passed", "keywords": ["test_ris_parser_accuracy", "TestCitationAccuracy", "test_citation_accuracy.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0001869620755314827, "outcome": "passed"}, "call": {"duration": 0.0008058738894760609, "outcome": "passed"}, "teardown": {"duration": 0.0001907041296362877, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_citation_accuracy.py::TestCitationAccuracy::test_malformed_citation_handling", "lineno": 306, "outcome": "passed", "keywords": ["test_malformed_citation_handling", "TestCitationAccuracy", "test_citation_accuracy.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012618559412658215, "outcome": "passed"}, "call": {"duration": 0.0074259256944060326, "outcome": "passed"}, "teardown": {"duration": 0.0006290948949754238, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_cli.py::TestCLI::test_init_command", "lineno": 38, "outcome": "passed", "keywords": ["test_init_command", "TestCLI", "test_cli.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.001539616845548153, "outcome": "passed"}, "call": {"duration": 0.001842346042394638, "outcome": "passed"}, "teardown": {"duration": 0.0005817515775561333, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_cli.py::TestCLI::test_main_function", "lineno": 55, "outcome": "passed", "keywords": ["test_main_function", "TestCLI", "test_cli.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00018552597612142563, "outcome": "passed"}, "call": {"duration": 0.011708377860486507, "outcome": "passed"}, "teardown": {"duration": 0.0001839650794863701, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_cli.py::TestCLI::test_note_command_integration_simplified", "lineno": 84, "outcome": "passed", "keywords": ["test_note_command_integration_simplified", "TestCLI", "test_cli.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0001828647218644619, "outcome": "passed"}, "call": {"duration": 0.012475430965423584, "outcome": "passed"}, "teardown": {"duration": 0.00018304260447621346, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_cli.py::TestCLI::test_search_command_existence", "lineno": 103, "outcome": "passed", "keywords": ["test_search_command_existence", "TestCLI", "test_cli.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0001887320540845394, "outcome": "passed"}, "call": {"duration": 0.012656844221055508, "outcome": "passed"}, "teardown": {"duration": 0.00018361490219831467, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_cli.py::TestCLI::test_main_with_init_integration", "lineno": 139, "outcome": "passed", "keywords": ["test_main_with_init_integration", "TestCLI", "test_cli.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0003727083094418049, "outcome": "passed"}, "call": {"duration": 0.014515671879053116, "outcome": "passed"}, "teardown": {"duration": 0.0002903128042817116, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_cli.py::TestCLI::test_multiple_cli_commands", "lineno": 156, "outcome": "passed", "keywords": ["test_multiple_cli_commands", "TestCLI", "test_cli.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00018313387408852577, "outcome": "passed"}, "call": {"duration": 0.01381166698411107, "outcome": "passed"}, "teardown": {"duration": 0.00019264593720436096, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_create_collaborator", "lineno": 108, "outcome": "failed", "keywords": ["test_create_collaborator", "TestCollaboration", "test_collaboration.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0013025240041315556, "outcome": "passed"}, "call": {"duration": 0.0007069469429552555, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError: cannot pickle 'property' object"}, "traceback": [{"path": "tests/academic_researcher/test_collaboration.py", "lineno": 112, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 822, "message": "in create_collaborator"}, {"path": "researchbrain/core/storage.py", "lineno": 131, "message": "in save"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 253, "message": "in dump"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 241, "message": "in dump_all"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 27, "message": "in represent"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 48, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 207, "message": "in represent_dict"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 118, "message": "in represent_mapping"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 52, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError"}], "longrepr": "self = <tests.academic_researcher.test_collaboration.TestCollaboration object at 0x7fe54fd79a20>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a8211180>\n\n    def test_create_collaborator(self, brain):\n        \"\"\"Test creating a collaborator with all attributes.\"\"\"\n        # Create a collaborator with all fields\n>       collaborator_id = brain.create_collaborator(\n            name=\"Dr. Jane Smith\",\n            email=\"jsmith@university.edu\",\n            affiliation=\"University of Research\",\n            role=CollaboratorRole.PRINCIPAL_INVESTIGATOR\n        )\n\ntests/academic_researcher/test_collaboration.py:112: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:822: in create_collaborator\n    self.storage.save(collaborator)\nresearchbrain/core/storage.py:131: in save\n    yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:253: in dump\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:241: in dump_all\n    dumper.represent(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:27: in represent\n    node = self.represent_data(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:48: in represent_data\n    node = self.yaml_representers[data_types[0]](self, data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:207: in represent_dict\n    return self.represent_mapping('tag:yaml.org,2002:map', data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:118: in represent_mapping\n    node_value = self.represent_data(item_value)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:52: in represent_data\n    node = self.yaml_multi_representers[data_type](self, data)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <yaml.dumper.Dumper object at 0x7fe4a7086920>\ndata = <property object at 0x7fe5ffc8e4d0>\n\n    def represent_object(self, data):\n        # We use __reduce__ API to save the data. data.__reduce__ returns\n        # a tuple of length 2-5:\n        #   (function, args, state, listitems, dictitems)\n    \n        # For reconstructing, we calls function(*args), then set its state,\n        # listitems, and dictitems if they are not None.\n    \n        # A special case is when function.__name__ == '__newobj__'. In this\n        # case we create the object with args[0].__new__(*args).\n    \n        # Another special case is when __reduce__ returns a string - we don't\n        # support it.\n    \n        # We produce a !!python/object, !!python/object/new or\n        # !!python/object/apply node.\n    \n        cls = type(data)\n        if cls in copyreg.dispatch_table:\n            reduce = copyreg.dispatch_table[cls](data)\n        elif hasattr(data, '__reduce_ex__'):\n>           reduce = data.__reduce_ex__(2)\nE           TypeError: cannot pickle 'property' object\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:317: TypeError"}, "teardown": {"duration": 0.0007657390087842941, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_add_annotations", "lineno": 130, "outcome": "error", "keywords": ["test_add_annotations", "TestCollaboration", "test_collaboration.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.006179843097925186, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError: cannot pickle 'property' object"}, "traceback": [{"path": "tests/academic_researcher/test_collaboration.py", "lineno": 63, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 822, "message": "in create_collaborator"}, {"path": "researchbrain/core/storage.py", "lineno": 131, "message": "in save"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 253, "message": "in dump"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 241, "message": "in dump_all"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 27, "message": "in represent"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 48, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 207, "message": "in represent_dict"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 118, "message": "in represent_mapping"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 52, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError"}], "longrepr": "self = <tests.academic_researcher.test_collaboration.TestCollaboration object at 0x7fe54fd79750>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a853b580>\n\n    @pytest.fixture\n    def sample_collaborators(self, brain):\n        \"\"\"Fixture that creates sample collaborators with different roles.\"\"\"\n        collaborators = {}\n    \n        # Principal Investigator\n>       pi_id = brain.create_collaborator(\n            name=\"Dr. Sarah Johnson\",\n            email=\"sjohnson@university.edu\",\n            affiliation=\"University of Neuroscience\",\n            role=CollaboratorRole.PRINCIPAL_INVESTIGATOR\n        )\n\ntests/academic_researcher/test_collaboration.py:63: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:822: in create_collaborator\n    self.storage.save(collaborator)\nresearchbrain/core/storage.py:131: in save\n    yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:253: in dump\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:241: in dump_all\n    dumper.represent(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:27: in represent\n    node = self.represent_data(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:48: in represent_data\n    node = self.yaml_representers[data_types[0]](self, data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:207: in represent_dict\n    return self.represent_mapping('tag:yaml.org,2002:map', data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:118: in represent_mapping\n    node_value = self.represent_data(item_value)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:52: in represent_data\n    node = self.yaml_multi_representers[data_type](self, data)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <yaml.dumper.Dumper object at 0x7fe4a853b5e0>\ndata = <property object at 0x7fe5ffc8e4d0>\n\n    def represent_object(self, data):\n        # We use __reduce__ API to save the data. data.__reduce__ returns\n        # a tuple of length 2-5:\n        #   (function, args, state, listitems, dictitems)\n    \n        # For reconstructing, we calls function(*args), then set its state,\n        # listitems, and dictitems if they are not None.\n    \n        # A special case is when function.__name__ == '__newobj__'. In this\n        # case we create the object with args[0].__new__(*args).\n    \n        # Another special case is when __reduce__ returns a string - we don't\n        # support it.\n    \n        # We produce a !!python/object, !!python/object/new or\n        # !!python/object/apply node.\n    \n        cls = type(data)\n        if cls in copyreg.dispatch_table:\n            reduce = copyreg.dispatch_table[cls](data)\n        elif hasattr(data, '__reduce_ex__'):\n>           reduce = data.__reduce_ex__(2)\nE           TypeError: cannot pickle 'property' object\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:317: TypeError"}, "teardown": {"duration": 0.0007500969804823399, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_multiple_annotations_from_same_collaborator", "lineno": 207, "outcome": "error", "keywords": ["test_multiple_annotations_from_same_collaborator", "TestCollaboration", "test_collaboration.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.006015303079038858, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError: cannot pickle 'property' object"}, "traceback": [{"path": "tests/academic_researcher/test_collaboration.py", "lineno": 63, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 822, "message": "in create_collaborator"}, {"path": "researchbrain/core/storage.py", "lineno": 131, "message": "in save"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 253, "message": "in dump"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 241, "message": "in dump_all"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 27, "message": "in represent"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 48, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 207, "message": "in represent_dict"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 118, "message": "in represent_mapping"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 52, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError"}], "longrepr": "self = <tests.academic_researcher.test_collaboration.TestCollaboration object at 0x7fe54fd7b6a0>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a82e4670>\n\n    @pytest.fixture\n    def sample_collaborators(self, brain):\n        \"\"\"Fixture that creates sample collaborators with different roles.\"\"\"\n        collaborators = {}\n    \n        # Principal Investigator\n>       pi_id = brain.create_collaborator(\n            name=\"Dr. Sarah Johnson\",\n            email=\"sjohnson@university.edu\",\n            affiliation=\"University of Neuroscience\",\n            role=CollaboratorRole.PRINCIPAL_INVESTIGATOR\n        )\n\ntests/academic_researcher/test_collaboration.py:63: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:822: in create_collaborator\n    self.storage.save(collaborator)\nresearchbrain/core/storage.py:131: in save\n    yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:253: in dump\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:241: in dump_all\n    dumper.represent(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:27: in represent\n    node = self.represent_data(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:48: in represent_data\n    node = self.yaml_representers[data_types[0]](self, data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:207: in represent_dict\n    return self.represent_mapping('tag:yaml.org,2002:map', data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:118: in represent_mapping\n    node_value = self.represent_data(item_value)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:52: in represent_data\n    node = self.yaml_multi_representers[data_type](self, data)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <yaml.dumper.Dumper object at 0x7fe4a82e6140>\ndata = <property object at 0x7fe5ffc8e4d0>\n\n    def represent_object(self, data):\n        # We use __reduce__ API to save the data. data.__reduce__ returns\n        # a tuple of length 2-5:\n        #   (function, args, state, listitems, dictitems)\n    \n        # For reconstructing, we calls function(*args), then set its state,\n        # listitems, and dictitems if they are not None.\n    \n        # A special case is when function.__name__ == '__newobj__'. In this\n        # case we create the object with args[0].__new__(*args).\n    \n        # Another special case is when __reduce__ returns a string - we don't\n        # support it.\n    \n        # We produce a !!python/object, !!python/object/new or\n        # !!python/object/apply node.\n    \n        cls = type(data)\n        if cls in copyreg.dispatch_table:\n            reduce = copyreg.dispatch_table[cls](data)\n        elif hasattr(data, '__reduce_ex__'):\n>           reduce = data.__reduce_ex__(2)\nE           TypeError: cannot pickle 'property' object\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:317: TypeError"}, "teardown": {"duration": 0.0007963520474731922, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_annotations_on_multiple_documents", "lineno": 248, "outcome": "error", "keywords": ["test_annotations_on_multiple_documents", "TestCollaboration", "test_collaboration.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0018485356122255325, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError: cannot pickle 'property' object"}, "traceback": [{"path": "tests/academic_researcher/test_collaboration.py", "lineno": 63, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 822, "message": "in create_collaborator"}, {"path": "researchbrain/core/storage.py", "lineno": 131, "message": "in save"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 253, "message": "in dump"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 241, "message": "in dump_all"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 27, "message": "in represent"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 48, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 207, "message": "in represent_dict"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 118, "message": "in represent_mapping"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 52, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError"}], "longrepr": "self = <tests.academic_researcher.test_collaboration.TestCollaboration object at 0x7fe54fd7b880>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a81e8580>\n\n    @pytest.fixture\n    def sample_collaborators(self, brain):\n        \"\"\"Fixture that creates sample collaborators with different roles.\"\"\"\n        collaborators = {}\n    \n        # Principal Investigator\n>       pi_id = brain.create_collaborator(\n            name=\"Dr. Sarah Johnson\",\n            email=\"sjohnson@university.edu\",\n            affiliation=\"University of Neuroscience\",\n            role=CollaboratorRole.PRINCIPAL_INVESTIGATOR\n        )\n\ntests/academic_researcher/test_collaboration.py:63: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:822: in create_collaborator\n    self.storage.save(collaborator)\nresearchbrain/core/storage.py:131: in save\n    yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:253: in dump\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:241: in dump_all\n    dumper.represent(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:27: in represent\n    node = self.represent_data(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:48: in represent_data\n    node = self.yaml_representers[data_types[0]](self, data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:207: in represent_dict\n    return self.represent_mapping('tag:yaml.org,2002:map', data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:118: in represent_mapping\n    node_value = self.represent_data(item_value)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:52: in represent_data\n    node = self.yaml_multi_representers[data_type](self, data)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <yaml.dumper.Dumper object at 0x7fe4a81eb760>\ndata = <property object at 0x7fe5ffc8e4d0>\n\n    def represent_object(self, data):\n        # We use __reduce__ API to save the data. data.__reduce__ returns\n        # a tuple of length 2-5:\n        #   (function, args, state, listitems, dictitems)\n    \n        # For reconstructing, we calls function(*args), then set its state,\n        # listitems, and dictitems if they are not None.\n    \n        # A special case is when function.__name__ == '__newobj__'. In this\n        # case we create the object with args[0].__new__(*args).\n    \n        # Another special case is when __reduce__ returns a string - we don't\n        # support it.\n    \n        # We produce a !!python/object, !!python/object/new or\n        # !!python/object/apply node.\n    \n        cls = type(data)\n        if cls in copyreg.dispatch_table:\n            reduce = copyreg.dispatch_table[cls](data)\n        elif hasattr(data, '__reduce_ex__'):\n>           reduce = data.__reduce_ex__(2)\nE           TypeError: cannot pickle 'property' object\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:317: TypeError"}, "teardown": {"duration": 0.0008123330771923065, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_import_collaborator_annotations", "lineno": 311, "outcome": "error", "keywords": ["test_import_collaborator_annotations", "TestCollaboration", "test_collaboration.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.006328193005174398, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError: cannot pickle 'property' object"}, "traceback": [{"path": "tests/academic_researcher/test_collaboration.py", "lineno": 63, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 822, "message": "in create_collaborator"}, {"path": "researchbrain/core/storage.py", "lineno": 131, "message": "in save"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 253, "message": "in dump"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 241, "message": "in dump_all"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 27, "message": "in represent"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 48, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 207, "message": "in represent_dict"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 118, "message": "in represent_mapping"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 52, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError"}], "longrepr": "self = <tests.academic_researcher.test_collaboration.TestCollaboration object at 0x7fe54fd7bfd0>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a858fcd0>\n\n    @pytest.fixture\n    def sample_collaborators(self, brain):\n        \"\"\"Fixture that creates sample collaborators with different roles.\"\"\"\n        collaborators = {}\n    \n        # Principal Investigator\n>       pi_id = brain.create_collaborator(\n            name=\"Dr. Sarah Johnson\",\n            email=\"sjohnson@university.edu\",\n            affiliation=\"University of Neuroscience\",\n            role=CollaboratorRole.PRINCIPAL_INVESTIGATOR\n        )\n\ntests/academic_researcher/test_collaboration.py:63: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:822: in create_collaborator\n    self.storage.save(collaborator)\nresearchbrain/core/storage.py:131: in save\n    yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:253: in dump\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:241: in dump_all\n    dumper.represent(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:27: in represent\n    node = self.represent_data(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:48: in represent_data\n    node = self.yaml_representers[data_types[0]](self, data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:207: in represent_dict\n    return self.represent_mapping('tag:yaml.org,2002:map', data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:118: in represent_mapping\n    node_value = self.represent_data(item_value)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:52: in represent_data\n    node = self.yaml_multi_representers[data_type](self, data)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <yaml.dumper.Dumper object at 0x7fe4a858fca0>\ndata = <property object at 0x7fe5ffc8e4d0>\n\n    def represent_object(self, data):\n        # We use __reduce__ API to save the data. data.__reduce__ returns\n        # a tuple of length 2-5:\n        #   (function, args, state, listitems, dictitems)\n    \n        # For reconstructing, we calls function(*args), then set its state,\n        # listitems, and dictitems if they are not None.\n    \n        # A special case is when function.__name__ == '__newobj__'. In this\n        # case we create the object with args[0].__new__(*args).\n    \n        # Another special case is when __reduce__ returns a string - we don't\n        # support it.\n    \n        # We produce a !!python/object, !!python/object/new or\n        # !!python/object/apply node.\n    \n        cls = type(data)\n        if cls in copyreg.dispatch_table:\n            reduce = copyreg.dispatch_table[cls](data)\n        elif hasattr(data, '__reduce_ex__'):\n>           reduce = data.__reduce_ex__(2)\nE           TypeError: cannot pickle 'property' object\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:317: TypeError"}, "teardown": {"duration": 0.0007891412824392319, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_import_collaborator_annotations_with_replies", "lineno": 386, "outcome": "error", "keywords": ["test_import_collaborator_annotations_with_replies", "TestCollaboration", "test_collaboration.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.006114814896136522, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError: cannot pickle 'property' object"}, "traceback": [{"path": "tests/academic_researcher/test_collaboration.py", "lineno": 63, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 822, "message": "in create_collaborator"}, {"path": "researchbrain/core/storage.py", "lineno": 131, "message": "in save"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 253, "message": "in dump"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 241, "message": "in dump_all"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 27, "message": "in represent"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 48, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 207, "message": "in represent_dict"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 118, "message": "in represent_mapping"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 52, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError"}], "longrepr": "self = <tests.academic_researcher.test_collaboration.TestCollaboration object at 0x7fe54fd7b160>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a8293730>\n\n    @pytest.fixture\n    def sample_collaborators(self, brain):\n        \"\"\"Fixture that creates sample collaborators with different roles.\"\"\"\n        collaborators = {}\n    \n        # Principal Investigator\n>       pi_id = brain.create_collaborator(\n            name=\"Dr. Sarah Johnson\",\n            email=\"sjohnson@university.edu\",\n            affiliation=\"University of Neuroscience\",\n            role=CollaboratorRole.PRINCIPAL_INVESTIGATOR\n        )\n\ntests/academic_researcher/test_collaboration.py:63: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:822: in create_collaborator\n    self.storage.save(collaborator)\nresearchbrain/core/storage.py:131: in save\n    yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:253: in dump\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:241: in dump_all\n    dumper.represent(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:27: in represent\n    node = self.represent_data(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:48: in represent_data\n    node = self.yaml_representers[data_types[0]](self, data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:207: in represent_dict\n    return self.represent_mapping('tag:yaml.org,2002:map', data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:118: in represent_mapping\n    node_value = self.represent_data(item_value)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:52: in represent_data\n    node = self.yaml_multi_representers[data_type](self, data)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <yaml.dumper.Dumper object at 0x7fe4a82937f0>\ndata = <property object at 0x7fe5ffc8e4d0>\n\n    def represent_object(self, data):\n        # We use __reduce__ API to save the data. data.__reduce__ returns\n        # a tuple of length 2-5:\n        #   (function, args, state, listitems, dictitems)\n    \n        # For reconstructing, we calls function(*args), then set its state,\n        # listitems, and dictitems if they are not None.\n    \n        # A special case is when function.__name__ == '__newobj__'. In this\n        # case we create the object with args[0].__new__(*args).\n    \n        # Another special case is when __reduce__ returns a string - we don't\n        # support it.\n    \n        # We produce a !!python/object, !!python/object/new or\n        # !!python/object/apply node.\n    \n        cls = type(data)\n        if cls in copyreg.dispatch_table:\n            reduce = copyreg.dispatch_table[cls](data)\n        elif hasattr(data, '__reduce_ex__'):\n>           reduce = data.__reduce_ex__(2)\nE           TypeError: cannot pickle 'property' object\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:317: TypeError"}, "teardown": {"duration": 0.0007639857940375805, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_import_collaborator_annotations_with_invalid_data", "lineno": 450, "outcome": "error", "keywords": ["test_import_collaborator_annotations_with_invalid_data", "TestCollaboration", "test_collaboration.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.006070369854569435, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError: cannot pickle 'property' object"}, "traceback": [{"path": "tests/academic_researcher/test_collaboration.py", "lineno": 63, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 822, "message": "in create_collaborator"}, {"path": "researchbrain/core/storage.py", "lineno": 131, "message": "in save"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 253, "message": "in dump"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 241, "message": "in dump_all"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 27, "message": "in represent"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 48, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 207, "message": "in represent_dict"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 118, "message": "in represent_mapping"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 52, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError"}], "longrepr": "self = <tests.academic_researcher.test_collaboration.TestCollaboration object at 0x7fe54fd7b370>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a84ba1a0>\n\n    @pytest.fixture\n    def sample_collaborators(self, brain):\n        \"\"\"Fixture that creates sample collaborators with different roles.\"\"\"\n        collaborators = {}\n    \n        # Principal Investigator\n>       pi_id = brain.create_collaborator(\n            name=\"Dr. Sarah Johnson\",\n            email=\"sjohnson@university.edu\",\n            affiliation=\"University of Neuroscience\",\n            role=CollaboratorRole.PRINCIPAL_INVESTIGATOR\n        )\n\ntests/academic_researcher/test_collaboration.py:63: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:822: in create_collaborator\n    self.storage.save(collaborator)\nresearchbrain/core/storage.py:131: in save\n    yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:253: in dump\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:241: in dump_all\n    dumper.represent(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:27: in represent\n    node = self.represent_data(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:48: in represent_data\n    node = self.yaml_representers[data_types[0]](self, data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:207: in represent_dict\n    return self.represent_mapping('tag:yaml.org,2002:map', data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:118: in represent_mapping\n    node_value = self.represent_data(item_value)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:52: in represent_data\n    node = self.yaml_multi_representers[data_type](self, data)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <yaml.dumper.Dumper object at 0x7fe4a84bb1f0>\ndata = <property object at 0x7fe5ffc8e4d0>\n\n    def represent_object(self, data):\n        # We use __reduce__ API to save the data. data.__reduce__ returns\n        # a tuple of length 2-5:\n        #   (function, args, state, listitems, dictitems)\n    \n        # For reconstructing, we calls function(*args), then set its state,\n        # listitems, and dictitems if they are not None.\n    \n        # A special case is when function.__name__ == '__newobj__'. In this\n        # case we create the object with args[0].__new__(*args).\n    \n        # Another special case is when __reduce__ returns a string - we don't\n        # support it.\n    \n        # We produce a !!python/object, !!python/object/new or\n        # !!python/object/apply node.\n    \n        cls = type(data)\n        if cls in copyreg.dispatch_table:\n            reduce = copyreg.dispatch_table[cls](data)\n        elif hasattr(data, '__reduce_ex__'):\n>           reduce = data.__reduce_ex__(2)\nE           TypeError: cannot pickle 'property' object\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:317: TypeError"}, "teardown": {"duration": 0.0008619017899036407, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_import_invalid_annotations", "lineno": 555, "outcome": "error", "keywords": ["test_import_invalid_annotations", "TestCollaboration", "test_collaboration.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0019423640333116055, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError: cannot pickle 'property' object"}, "traceback": [{"path": "tests/academic_researcher/test_collaboration.py", "lineno": 63, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 822, "message": "in create_collaborator"}, {"path": "researchbrain/core/storage.py", "lineno": 131, "message": "in save"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 253, "message": "in dump"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 241, "message": "in dump_all"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 27, "message": "in represent"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 48, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 207, "message": "in represent_dict"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 118, "message": "in represent_mapping"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 52, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError"}], "longrepr": "self = <tests.academic_researcher.test_collaboration.TestCollaboration object at 0x7fe54fd7b550>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a83aabf0>\n\n    @pytest.fixture\n    def sample_collaborators(self, brain):\n        \"\"\"Fixture that creates sample collaborators with different roles.\"\"\"\n        collaborators = {}\n    \n        # Principal Investigator\n>       pi_id = brain.create_collaborator(\n            name=\"Dr. Sarah Johnson\",\n            email=\"sjohnson@university.edu\",\n            affiliation=\"University of Neuroscience\",\n            role=CollaboratorRole.PRINCIPAL_INVESTIGATOR\n        )\n\ntests/academic_researcher/test_collaboration.py:63: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:822: in create_collaborator\n    self.storage.save(collaborator)\nresearchbrain/core/storage.py:131: in save\n    yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:253: in dump\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:241: in dump_all\n    dumper.represent(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:27: in represent\n    node = self.represent_data(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:48: in represent_data\n    node = self.yaml_representers[data_types[0]](self, data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:207: in represent_dict\n    return self.represent_mapping('tag:yaml.org,2002:map', data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:118: in represent_mapping\n    node_value = self.represent_data(item_value)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:52: in represent_data\n    node = self.yaml_multi_representers[data_type](self, data)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <yaml.dumper.Dumper object at 0x7fe4a83aa110>\ndata = <property object at 0x7fe5ffc8e4d0>\n\n    def represent_object(self, data):\n        # We use __reduce__ API to save the data. data.__reduce__ returns\n        # a tuple of length 2-5:\n        #   (function, args, state, listitems, dictitems)\n    \n        # For reconstructing, we calls function(*args), then set its state,\n        # listitems, and dictitems if they are not None.\n    \n        # A special case is when function.__name__ == '__newobj__'. In this\n        # case we create the object with args[0].__new__(*args).\n    \n        # Another special case is when __reduce__ returns a string - we don't\n        # support it.\n    \n        # We produce a !!python/object, !!python/object/new or\n        # !!python/object/apply node.\n    \n        cls = type(data)\n        if cls in copyreg.dispatch_table:\n            reduce = copyreg.dispatch_table[cls](data)\n        elif hasattr(data, '__reduce_ex__'):\n>           reduce = data.__reduce_ex__(2)\nE           TypeError: cannot pickle 'property' object\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:317: TypeError"}, "teardown": {"duration": 0.0007112962193787098, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_annotations_on_citations", "lineno": 598, "outcome": "error", "keywords": ["test_annotations_on_citations", "TestCollaboration", "test_collaboration.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0018317829817533493, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError: cannot pickle 'property' object"}, "traceback": [{"path": "tests/academic_researcher/test_collaboration.py", "lineno": 63, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 822, "message": "in create_collaborator"}, {"path": "researchbrain/core/storage.py", "lineno": 131, "message": "in save"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 253, "message": "in dump"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 241, "message": "in dump_all"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 27, "message": "in represent"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 48, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 207, "message": "in represent_dict"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 118, "message": "in represent_mapping"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 52, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError"}], "longrepr": "self = <tests.academic_researcher.test_collaboration.TestCollaboration object at 0x7fe54fd7bc40>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a83a0fd0>\n\n    @pytest.fixture\n    def sample_collaborators(self, brain):\n        \"\"\"Fixture that creates sample collaborators with different roles.\"\"\"\n        collaborators = {}\n    \n        # Principal Investigator\n>       pi_id = brain.create_collaborator(\n            name=\"Dr. Sarah Johnson\",\n            email=\"sjohnson@university.edu\",\n            affiliation=\"University of Neuroscience\",\n            role=CollaboratorRole.PRINCIPAL_INVESTIGATOR\n        )\n\ntests/academic_researcher/test_collaboration.py:63: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:822: in create_collaborator\n    self.storage.save(collaborator)\nresearchbrain/core/storage.py:131: in save\n    yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:253: in dump\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:241: in dump_all\n    dumper.represent(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:27: in represent\n    node = self.represent_data(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:48: in represent_data\n    node = self.yaml_representers[data_types[0]](self, data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:207: in represent_dict\n    return self.represent_mapping('tag:yaml.org,2002:map', data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:118: in represent_mapping\n    node_value = self.represent_data(item_value)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:52: in represent_data\n    node = self.yaml_multi_representers[data_type](self, data)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <yaml.dumper.Dumper object at 0x7fe4a83a2d70>\ndata = <property object at 0x7fe5ffc8e4d0>\n\n    def represent_object(self, data):\n        # We use __reduce__ API to save the data. data.__reduce__ returns\n        # a tuple of length 2-5:\n        #   (function, args, state, listitems, dictitems)\n    \n        # For reconstructing, we calls function(*args), then set its state,\n        # listitems, and dictitems if they are not None.\n    \n        # A special case is when function.__name__ == '__newobj__'. In this\n        # case we create the object with args[0].__new__(*args).\n    \n        # Another special case is when __reduce__ returns a string - we don't\n        # support it.\n    \n        # We produce a !!python/object, !!python/object/new or\n        # !!python/object/apply node.\n    \n        cls = type(data)\n        if cls in copyreg.dispatch_table:\n            reduce = copyreg.dispatch_table[cls](data)\n        elif hasattr(data, '__reduce_ex__'):\n>           reduce = data.__reduce_ex__(2)\nE           TypeError: cannot pickle 'property' object\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:317: TypeError"}, "teardown": {"duration": 0.0006981841288506985, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_maintaining_annotation_integrity", "lineno": 643, "outcome": "error", "keywords": ["test_maintaining_annotation_integrity", "TestCollaboration", "test_collaboration.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.006185404025018215, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError: cannot pickle 'property' object"}, "traceback": [{"path": "tests/academic_researcher/test_collaboration.py", "lineno": 63, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 822, "message": "in create_collaborator"}, {"path": "researchbrain/core/storage.py", "lineno": 131, "message": "in save"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 253, "message": "in dump"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 241, "message": "in dump_all"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 27, "message": "in represent"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 48, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 207, "message": "in represent_dict"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 118, "message": "in represent_mapping"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 52, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError"}], "longrepr": "self = <tests.academic_researcher.test_collaboration.TestCollaboration object at 0x7fe54fd78e80>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a8424490>\n\n    @pytest.fixture\n    def sample_collaborators(self, brain):\n        \"\"\"Fixture that creates sample collaborators with different roles.\"\"\"\n        collaborators = {}\n    \n        # Principal Investigator\n>       pi_id = brain.create_collaborator(\n            name=\"Dr. Sarah Johnson\",\n            email=\"sjohnson@university.edu\",\n            affiliation=\"University of Neuroscience\",\n            role=CollaboratorRole.PRINCIPAL_INVESTIGATOR\n        )\n\ntests/academic_researcher/test_collaboration.py:63: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:822: in create_collaborator\n    self.storage.save(collaborator)\nresearchbrain/core/storage.py:131: in save\n    yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:253: in dump\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:241: in dump_all\n    dumper.represent(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:27: in represent\n    node = self.represent_data(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:48: in represent_data\n    node = self.yaml_representers[data_types[0]](self, data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:207: in represent_dict\n    return self.represent_mapping('tag:yaml.org,2002:map', data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:118: in represent_mapping\n    node_value = self.represent_data(item_value)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:52: in represent_data\n    node = self.yaml_multi_representers[data_type](self, data)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <yaml.dumper.Dumper object at 0x7fe4a8426dd0>\ndata = <property object at 0x7fe5ffc8e4d0>\n\n    def represent_object(self, data):\n        # We use __reduce__ API to save the data. data.__reduce__ returns\n        # a tuple of length 2-5:\n        #   (function, args, state, listitems, dictitems)\n    \n        # For reconstructing, we calls function(*args), then set its state,\n        # listitems, and dictitems if they are not None.\n    \n        # A special case is when function.__name__ == '__newobj__'. In this\n        # case we create the object with args[0].__new__(*args).\n    \n        # Another special case is when __reduce__ returns a string - we don't\n        # support it.\n    \n        # We produce a !!python/object, !!python/object/new or\n        # !!python/object/apply node.\n    \n        cls = type(data)\n        if cls in copyreg.dispatch_table:\n            reduce = copyreg.dispatch_table[cls](data)\n        elif hasattr(data, '__reduce_ex__'):\n>           reduce = data.__reduce_ex__(2)\nE           TypeError: cannot pickle 'property' object\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:317: TypeError"}, "teardown": {"duration": 0.0008947472088038921, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_collaboration.py::TestCollaboration::test_collaborative_feedback_integration", "lineno": 680, "outcome": "error", "keywords": ["test_collaborative_feedback_integration", "TestCollaboration", "test_collaboration.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.006416521035134792, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError: cannot pickle 'property' object"}, "traceback": [{"path": "tests/academic_researcher/test_collaboration.py", "lineno": 63, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 822, "message": "in create_collaborator"}, {"path": "researchbrain/core/storage.py", "lineno": 131, "message": "in save"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 253, "message": "in dump"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 241, "message": "in dump_all"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 27, "message": "in represent"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 48, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 207, "message": "in represent_dict"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 118, "message": "in represent_mapping"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 52, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError"}], "longrepr": "self = <tests.academic_researcher.test_collaboration.TestCollaboration object at 0x7fe54fd781f0>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a85c85e0>\n\n    @pytest.fixture\n    def sample_collaborators(self, brain):\n        \"\"\"Fixture that creates sample collaborators with different roles.\"\"\"\n        collaborators = {}\n    \n        # Principal Investigator\n>       pi_id = brain.create_collaborator(\n            name=\"Dr. Sarah Johnson\",\n            email=\"sjohnson@university.edu\",\n            affiliation=\"University of Neuroscience\",\n            role=CollaboratorRole.PRINCIPAL_INVESTIGATOR\n        )\n\ntests/academic_researcher/test_collaboration.py:63: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:822: in create_collaborator\n    self.storage.save(collaborator)\nresearchbrain/core/storage.py:131: in save\n    yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:253: in dump\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:241: in dump_all\n    dumper.represent(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:27: in represent\n    node = self.represent_data(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:48: in represent_data\n    node = self.yaml_representers[data_types[0]](self, data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:207: in represent_dict\n    return self.represent_mapping('tag:yaml.org,2002:map', data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:118: in represent_mapping\n    node_value = self.represent_data(item_value)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:52: in represent_data\n    node = self.yaml_multi_representers[data_type](self, data)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <yaml.dumper.Dumper object at 0x7fe4a85cbee0>\ndata = <property object at 0x7fe5ffc8e4d0>\n\n    def represent_object(self, data):\n        # We use __reduce__ API to save the data. data.__reduce__ returns\n        # a tuple of length 2-5:\n        #   (function, args, state, listitems, dictitems)\n    \n        # For reconstructing, we calls function(*args), then set its state,\n        # listitems, and dictitems if they are not None.\n    \n        # A special case is when function.__name__ == '__newobj__'. In this\n        # case we create the object with args[0].__new__(*args).\n    \n        # Another special case is when __reduce__ returns a string - we don't\n        # support it.\n    \n        # We produce a !!python/object, !!python/object/new or\n        # !!python/object/apply node.\n    \n        cls = type(data)\n        if cls in copyreg.dispatch_table:\n            reduce = copyreg.dispatch_table[cls](data)\n        elif hasattr(data, '__reduce_ex__'):\n>           reduce = data.__reduce_ex__(2)\nE           TypeError: cannot pickle 'property' object\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:317: TypeError"}, "teardown": {"duration": 0.0007677837274968624, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_default_templates_availability", "lineno": 32, "outcome": "passed", "keywords": ["test_default_templates_availability", "TestExperimentTemplates", "test_experiment_templates.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00021266471594572067, "outcome": "passed"}, "call": {"duration": 0.052472214214503765, "outcome": "passed"}, "teardown": {"duration": 0.00018360232934355736, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_behavioral_experiment_template", "lineno": 61, "outcome": "passed", "keywords": ["test_behavioral_experiment_template", "TestExperimentTemplates", "test_experiment_templates.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.001310430932790041, "outcome": "passed"}, "call": {"duration": 0.01890928205102682, "outcome": "passed"}, "teardown": {"duration": 0.000609355978667736, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_neuroimaging_experiment_template", "lineno": 106, "outcome": "passed", "keywords": ["test_neuroimaging_experiment_template", "TestExperimentTemplates", "test_experiment_templates.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012822751887142658, "outcome": "passed"}, "call": {"duration": 0.02002401603385806, "outcome": "passed"}, "teardown": {"duration": 0.0006042937748134136, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_molecular_biology_experiment_template", "lineno": 156, "outcome": "passed", "keywords": ["test_molecular_biology_experiment_template", "TestExperimentTemplates", "test_experiment_templates.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012425770983099937, "outcome": "passed"}, "call": {"duration": 0.0181057034060359, "outcome": "passed"}, "teardown": {"duration": 0.0006027398630976677, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_computational_modeling_experiment_template", "lineno": 203, "outcome": "passed", "keywords": ["test_computational_modeling_experiment_template", "TestExperimentTemplates", "test_experiment_templates.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012374869547784328, "outcome": "passed"}, "call": {"duration": 0.019005974754691124, "outcome": "passed"}, "teardown": {"duration": 0.0006126649677753448, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_template_with_missing_required_fields", "lineno": 252, "outcome": "passed", "keywords": ["test_template_with_missing_required_fields", "TestExperimentTemplates", "test_experiment_templates.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012486898340284824, "outcome": "passed"}, "call": {"duration": 0.009853004943579435, "outcome": "passed"}, "teardown": {"duration": 0.0005967780016362667, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_template_with_optional_fields", "lineno": 264, "outcome": "passed", "keywords": ["test_template_with_optional_fields", "TestExperimentTemplates", "test_experiment_templates.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012216772884130478, "outcome": "passed"}, "call": {"duration": 0.017879643011838198, "outcome": "passed"}, "teardown": {"duration": 0.0006149979308247566, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_custom_template_creation", "lineno": 286, "outcome": "passed", "keywords": ["test_custom_template_creation", "TestExperimentTemplates", "test_experiment_templates.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012477817945182323, "outcome": "passed"}, "call": {"duration": 0.02937529794871807, "outcome": "passed"}, "teardown": {"duration": 0.0006135348230600357, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_sleep_study_template", "lineno": 403, "outcome": "passed", "keywords": ["test_sleep_study_template", "TestExperimentTemplates", "test_experiment_templates.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00125511409714818, "outcome": "passed"}, "call": {"duration": 0.03327663382515311, "outcome": "passed"}, "teardown": {"duration": 0.0006229318678379059, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_experiment_linking_to_research_question", "lineno": 463, "outcome": "failed", "keywords": ["test_experiment_linking_to_research_question", "TestExperimentTemplates", "test_experiment_templates.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012471070513129234, "outcome": "passed"}, "call": {"duration": 0.00029492471367120743, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_experiment_templates.py", "lineno": 467, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_experiment_templates.TestExperimentTemplates object at 0x7fe54fc002e0>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a81922f0>\n\n    def test_experiment_linking_to_research_question(self, brain):\n        \"\"\"Test linking an experiment created from a template to a research question.\"\"\"\n        # Create a research question\n>       question_id = brain.create_research_question(\n            question=\"How does sleep deprivation affect working memory performance?\",\n            description=\"Investigating the effects of acute sleep deprivation on working memory tasks.\",\n            priority=9\n        )\n\ntests/academic_researcher/test_experiment_templates.py:467: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'How does sleep deprivation affect working memory performance?'\ntitle = 'How does sleep deprivation affect working memory performance?'\ncontent = 'Investigating the effects of acute sleep deprivation on working memory tasks.'\ndescription = 'Investigating the effects of acute sleep deprivation on working memory tasks.'\npriority = <Priority.CRITICAL: 'critical'>\nkwargs = {'description': 'Investigating the effects of acute sleep deprivation on working memory tasks.', 'knowledge_gaps': [],...ty': <Priority.CRITICAL: 'critical'>, 'question': 'How does sleep deprivation affect working memory performance?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.000619168858975172, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_experiment_documentation_with_notes", "lineno": 516, "outcome": "passed", "keywords": ["test_experiment_documentation_with_notes", "TestExperimentTemplates", "test_experiment_templates.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012344052083790302, "outcome": "passed"}, "call": {"duration": 0.029118067119270563, "outcome": "passed"}, "teardown": {"duration": 0.0006483830511569977, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_experiment_templates.py::TestExperimentTemplates::test_experiment_status_updates", "lineno": 577, "outcome": "passed", "keywords": ["test_experiment_status_updates", "TestExperimentTemplates", "test_experiment_templates.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012255189940333366, "outcome": "passed"}, "call": {"duration": 0.025944910012185574, "outcome": "passed"}, "teardown": {"duration": 0.0006064847111701965, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_create_grant_proposal", "lineno": 116, "outcome": "passed", "keywords": ["test_create_grant_proposal", "TestGrantProposals", "test_grant_proposals.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012439237907528877, "outcome": "passed"}, "call": {"duration": 0.003394210245460272, "outcome": "passed"}, "teardown": {"duration": 0.0006318259984254837, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_add_items_to_grant_workspace", "lineno": 153, "outcome": "error", "keywords": ["test_add_items_to_grant_workspace", "TestGrantProposals", "test_grant_proposals.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.001390489749610424, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_grant_proposals.py", "lineno": 38, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_grant_proposals.TestGrantProposals object at 0x7fe54fd79de0>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a70adf60>\n\n    @pytest.fixture\n    def sample_proposal_data(self, brain):\n        \"\"\"Fixture that creates sample data for a grant proposal.\"\"\"\n        # Create research questions\n>       question1_id = brain.create_research_question(\n            question=\"How does neuronal activity influence myelination in the developing brain?\",\n            description=\"Investigating the relationship between neural firing and oligodendrocyte myelination\",\n            priority=9\n        )\n\ntests/academic_researcher/test_grant_proposals.py:38: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'How does neuronal activity influence myelination in the developing brain?'\ntitle = 'How does neuronal activity influence myelination in the developing brain?'\ncontent = 'Investigating the relationship between neural firing and oligodendrocyte myelination'\ndescription = 'Investigating the relationship between neural firing and oligodendrocyte myelination'\npriority = <Priority.CRITICAL: 'critical'>\nkwargs = {'description': 'Investigating the relationship between neural firing and oligodendrocyte myelination', 'knowledge_gap...ty.CRITICAL: 'critical'>, 'question': 'How does neuronal activity influence myelination in the developing brain?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0008448776789009571, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_adding_items_incrementally", "lineno": 208, "outcome": "error", "keywords": ["test_adding_items_incrementally", "TestGrantProposals", "test_grant_proposals.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.001765524037182331, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_grant_proposals.py", "lineno": 38, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_grant_proposals.TestGrantProposals object at 0x7fe54fc005e0>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a826faf0>\n\n    @pytest.fixture\n    def sample_proposal_data(self, brain):\n        \"\"\"Fixture that creates sample data for a grant proposal.\"\"\"\n        # Create research questions\n>       question1_id = brain.create_research_question(\n            question=\"How does neuronal activity influence myelination in the developing brain?\",\n            description=\"Investigating the relationship between neural firing and oligodendrocyte myelination\",\n            priority=9\n        )\n\ntests/academic_researcher/test_grant_proposals.py:38: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'How does neuronal activity influence myelination in the developing brain?'\ntitle = 'How does neuronal activity influence myelination in the developing brain?'\ncontent = 'Investigating the relationship between neural firing and oligodendrocyte myelination'\ndescription = 'Investigating the relationship between neural firing and oligodendrocyte myelination'\npriority = <Priority.CRITICAL: 'critical'>\nkwargs = {'description': 'Investigating the relationship between neural firing and oligodendrocyte myelination', 'knowledge_gap...ty.CRITICAL: 'critical'>, 'question': 'How does neuronal activity influence myelination in the developing brain?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0007811090908944607, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_grant_proposal_status_progression", "lineno": 251, "outcome": "passed", "keywords": ["test_grant_proposal_status_progression", "TestGrantProposals", "test_grant_proposals.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0013460898771882057, "outcome": "passed"}, "call": {"duration": 0.011425219010561705, "outcome": "passed"}, "teardown": {"duration": 0.0006174240261316299, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_export_grant_proposal_to_markdown", "lineno": 289, "outcome": "error", "keywords": ["test_export_grant_proposal_to_markdown", "TestGrantProposals", "test_grant_proposals.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0014136740937829018, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_grant_proposals.py", "lineno": 38, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_grant_proposals.TestGrantProposals object at 0x7fe54fc01bd0>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a83a0040>\n\n    @pytest.fixture\n    def sample_proposal_data(self, brain):\n        \"\"\"Fixture that creates sample data for a grant proposal.\"\"\"\n        # Create research questions\n>       question1_id = brain.create_research_question(\n            question=\"How does neuronal activity influence myelination in the developing brain?\",\n            description=\"Investigating the relationship between neural firing and oligodendrocyte myelination\",\n            priority=9\n        )\n\ntests/academic_researcher/test_grant_proposals.py:38: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'How does neuronal activity influence myelination in the developing brain?'\ntitle = 'How does neuronal activity influence myelination in the developing brain?'\ncontent = 'Investigating the relationship between neural firing and oligodendrocyte myelination'\ndescription = 'Investigating the relationship between neural firing and oligodendrocyte myelination'\npriority = <Priority.CRITICAL: 'critical'>\nkwargs = {'description': 'Investigating the relationship between neural firing and oligodendrocyte myelination', 'knowledge_gap...ty.CRITICAL: 'critical'>, 'question': 'How does neuronal activity influence myelination in the developing brain?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0006565619260072708, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_export_grant_proposal_to_yaml", "lineno": 345, "outcome": "error", "keywords": ["test_export_grant_proposal_to_yaml", "TestGrantProposals", "test_grant_proposals.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0014231358654797077, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_grant_proposals.py", "lineno": 38, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_grant_proposals.TestGrantProposals object at 0x7fe54fc01db0>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a835c580>\n\n    @pytest.fixture\n    def sample_proposal_data(self, brain):\n        \"\"\"Fixture that creates sample data for a grant proposal.\"\"\"\n        # Create research questions\n>       question1_id = brain.create_research_question(\n            question=\"How does neuronal activity influence myelination in the developing brain?\",\n            description=\"Investigating the relationship between neural firing and oligodendrocyte myelination\",\n            priority=9\n        )\n\ntests/academic_researcher/test_grant_proposals.py:38: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'How does neuronal activity influence myelination in the developing brain?'\ntitle = 'How does neuronal activity influence myelination in the developing brain?'\ncontent = 'Investigating the relationship between neural firing and oligodendrocyte myelination'\ndescription = 'Investigating the relationship between neural firing and oligodendrocyte myelination'\npriority = <Priority.CRITICAL: 'critical'>\nkwargs = {'description': 'Investigating the relationship between neural firing and oligodendrocyte myelination', 'knowledge_gap...ty.CRITICAL: 'critical'>, 'question': 'How does neuronal activity influence myelination in the developing brain?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0006717289797961712, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_grant_proposal_version_history", "lineno": 413, "outcome": "error", "keywords": ["test_grant_proposal_version_history", "TestGrantProposals", "test_grant_proposals.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0014096586965024471, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_grant_proposals.py", "lineno": 38, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_grant_proposals.TestGrantProposals object at 0x7fe54fc01f90>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a85dc130>\n\n    @pytest.fixture\n    def sample_proposal_data(self, brain):\n        \"\"\"Fixture that creates sample data for a grant proposal.\"\"\"\n        # Create research questions\n>       question1_id = brain.create_research_question(\n            question=\"How does neuronal activity influence myelination in the developing brain?\",\n            description=\"Investigating the relationship between neural firing and oligodendrocyte myelination\",\n            priority=9\n        )\n\ntests/academic_researcher/test_grant_proposals.py:38: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'How does neuronal activity influence myelination in the developing brain?'\ntitle = 'How does neuronal activity influence myelination in the developing brain?'\ncontent = 'Investigating the relationship between neural firing and oligodendrocyte myelination'\ndescription = 'Investigating the relationship between neural firing and oligodendrocyte myelination'\npriority = <Priority.CRITICAL: 'critical'>\nkwargs = {'description': 'Investigating the relationship between neural firing and oligodendrocyte myelination', 'knowledge_gap...ty.CRITICAL: 'critical'>, 'question': 'How does neuronal activity influence myelination in the developing brain?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0006539998576045036, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_budget_and_timeline_management", "lineno": 516, "outcome": "failed", "keywords": ["test_budget_and_timeline_management", "TestGrantProposals", "test_grant_proposals.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.001252525020390749, "outcome": "passed"}, "call": {"duration": 0.003106652293354273, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 989, "message": "AttributeError: 'GrantProposal' object has no attribute 'update'"}, "traceback": [{"path": "tests/academic_researcher/test_grant_proposals.py", "lineno": 573, "message": ""}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 989, "message": "AttributeError"}], "longrepr": "self = <tests.academic_researcher.test_grant_proposals.TestGrantProposals object at 0x7fe54fc02170>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a8427a60>\n\n    def test_budget_and_timeline_management(self, brain):\n        \"\"\"Test managing budget items and project timeline in grant proposals.\"\"\"\n        # Create a grant proposal\n        grant_id = brain.create_grant_proposal(\n            title=\"Neural Circuit Development Study\",\n            funding_agency=\"Brain Research Foundation\",\n            description=\"Investigating developmental trajectories of neural circuits\",\n            deadline=datetime.now() + timedelta(days=45),\n            status=GrantStatus.DRAFTING,\n            amount=850000.0\n        )\n    \n        # Get the grant\n        grant = brain.storage.get(GrantProposal, grant_id)\n    \n        # Add budget items\n        budget_items = {\n            \"personnel\": {\n                \"PI\": 150000.0,\n                \"Postdoc\": 75000.0,\n                \"Graduate Students (2)\": 120000.0,\n                \"Research Assistant\": 60000.0\n            },\n            \"equipment\": {\n                \"Microscope\": 200000.0,\n                \"Computing Cluster\": 50000.0,\n                \"Lab Supplies\": 75000.0\n            },\n            \"travel\": {\n                \"Conferences\": 15000.0,\n                \"Collaborative Visits\": 10000.0\n            },\n            \"indirect_costs\": {\n                \"Facilities & Admin\": 95000.0\n            }\n        }\n    \n        # Add timeline milestones\n        timeline = {\n            \"year1\": {\n                \"q1\": \"Setup equipment and hire personnel\",\n                \"q2\": \"Begin preliminary experiments\",\n                \"q3\": \"Complete first set of experiments\",\n                \"q4\": \"Data analysis and preliminary report\"\n            },\n            \"year2\": {\n                \"q1\": \"Begin main experimental series\",\n                \"q2\": \"Continue experiments and begin data analysis\",\n                \"q3\": \"Final experiments and complete data collection\",\n                \"q4\": \"Analysis, paper writing, and final report\"\n            }\n        }\n    \n        # Update the grant with budget and timeline information\n        grant.budget_items = budget_items\n        grant.timeline = timeline\n>       grant.update()\n\ntests/academic_researcher/test_grant_proposals.py:573: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = GrantProposal(id=UUID('2607b0fa-90c7-447b-9677-5b42b9ce2aa0'), created_at=datetime.datetime(2025, 6, 16, 4, 15, 5, 627...inal experiments and complete data collection', 'q4': 'Analysis, paper writing, and final report'}}, export_history=[])\nitem = 'update'\n\n    def __getattr__(self, item: str) -> Any:\n        private_attributes = object.__getattribute__(self, '__private_attributes__')\n        if item in private_attributes:\n            attribute = private_attributes[item]\n            if hasattr(attribute, '__get__'):\n                return attribute.__get__(self, type(self))  # type: ignore\n    \n            try:\n                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items\n                return self.__pydantic_private__[item]  # type: ignore\n            except KeyError as exc:\n                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\n        else:\n            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.\n            # See `BaseModel.__repr_args__` for more details\n            try:\n                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')\n            except AttributeError:\n                pydantic_extra = None\n    \n            if pydantic_extra:\n                try:\n                    return pydantic_extra[item]\n                except KeyError as exc:\n                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\n            else:\n                if hasattr(self.__class__, item):\n                    return super().__getattribute__(item)  # Raises AttributeError if appropriate\n                else:\n                    # this is the current error\n>                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\nE                   AttributeError: 'GrantProposal' object has no attribute 'update'\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:989: AttributeError"}, "teardown": {"duration": 0.0006303167901933193, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_related_nodes_navigation", "lineno": 631, "outcome": "error", "keywords": ["test_related_nodes_navigation", "TestGrantProposals", "test_grant_proposals.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00142580084502697, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_grant_proposals.py", "lineno": 38, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_grant_proposals.TestGrantProposals object at 0x7fe54fc02380>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a8344c40>\n\n    @pytest.fixture\n    def sample_proposal_data(self, brain):\n        \"\"\"Fixture that creates sample data for a grant proposal.\"\"\"\n        # Create research questions\n>       question1_id = brain.create_research_question(\n            question=\"How does neuronal activity influence myelination in the developing brain?\",\n            description=\"Investigating the relationship between neural firing and oligodendrocyte myelination\",\n            priority=9\n        )\n\ntests/academic_researcher/test_grant_proposals.py:38: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'How does neuronal activity influence myelination in the developing brain?'\ntitle = 'How does neuronal activity influence myelination in the developing brain?'\ncontent = 'Investigating the relationship between neural firing and oligodendrocyte myelination'\ndescription = 'Investigating the relationship between neural firing and oligodendrocyte myelination'\npriority = <Priority.CRITICAL: 'critical'>\nkwargs = {'description': 'Investigating the relationship between neural firing and oligodendrocyte myelination', 'knowledge_gap...ty.CRITICAL: 'critical'>, 'question': 'How does neuronal activity influence myelination in the developing brain?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0007895049639046192, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_grant_proposals.py::TestGrantProposals::test_multi_grant_proposal_organization", "lineno": 689, "outcome": "error", "keywords": ["test_multi_grant_proposal_organization", "TestGrantProposals", "test_grant_proposals.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0015807109884917736, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_grant_proposals.py", "lineno": 38, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_grant_proposals.TestGrantProposals object at 0x7fe54fc02560>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a85dcaf0>\n\n    @pytest.fixture\n    def sample_proposal_data(self, brain):\n        \"\"\"Fixture that creates sample data for a grant proposal.\"\"\"\n        # Create research questions\n>       question1_id = brain.create_research_question(\n            question=\"How does neuronal activity influence myelination in the developing brain?\",\n            description=\"Investigating the relationship between neural firing and oligodendrocyte myelination\",\n            priority=9\n        )\n\ntests/academic_researcher/test_grant_proposals.py:38: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'How does neuronal activity influence myelination in the developing brain?'\ntitle = 'How does neuronal activity influence myelination in the developing brain?'\ncontent = 'Investigating the relationship between neural firing and oligodendrocyte myelination'\ndescription = 'Investigating the relationship between neural firing and oligodendrocyte myelination'\npriority = <Priority.CRITICAL: 'critical'>\nkwargs = {'description': 'Investigating the relationship between neural firing and oligodendrocyte myelination', 'knowledge_gap...ty.CRITICAL: 'critical'>, 'question': 'How does neuronal activity influence myelination in the developing brain?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0006959880702197552, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_main.py::test_main_module_imports", "lineno": 7, "outcome": "passed", "keywords": ["test_main_module_imports", "test_main.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0002089780755341053, "outcome": "passed"}, "call": {"duration": 0.0006445460021495819, "outcome": "passed"}, "teardown": {"duration": 0.00018130987882614136, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_main.py::test_main_execution", "lineno": 15, "outcome": "passed", "keywords": ["test_main_execution", "test_main.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.00021637091413140297, "outcome": "passed"}, "call": {"duration": 0.0010442789644002914, "outcome": "passed"}, "teardown": {"duration": 0.00018264725804328918, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_performance_optimized.py::TestOptimizedPerformance::test_basic_performance", "lineno": 108, "outcome": "error", "keywords": ["test_basic_performance", "TestOptimizedPerformance", "test_performance_optimized.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.13956421008333564, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.LOW: 'low'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_performance_optimized.py", "lineno": 38, "message": ""}, {"path": "tests/academic_researcher/test_performance_optimized.py", "lineno": 86, "message": "in _create_test_data"}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_performance_optimized.TestOptimizedPerformance object at 0x7fe54fc030a0>\ntemp_data_dir = '/tmp/tmpx4fpa6tw'\n\n    @pytest.fixture\n    def brain_with_data(self, temp_data_dir):\n        \"\"\"Fixture that creates a ResearchBrain instance with test data.\"\"\"\n        brain = ResearchBrain(temp_data_dir)\n    \n        # Create minimal test data - using smaller counts for faster tests\n>       self._create_test_data(brain, note_count=10, citation_count=5, question_count=2)\n\ntests/academic_researcher/test_performance_optimized.py:38: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/academic_researcher/test_performance_optimized.py:86: in _create_test_data\n    question_id = brain.create_research_question(\nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion(), question = 'Research question 0?'\ntitle = 'Research question 0?', content = 'Description for research question 0'\ndescription = 'Description for research question 0'\npriority = <Priority.LOW: 'low'>\nkwargs = {'description': 'Description for research question 0', 'knowledge_gaps': [], 'priority': <Priority.LOW: 'low'>, 'question': 'Research question 0?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.LOW: 'low'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0008331951685249805, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_performance_optimized.py::TestOptimizedPerformance::test_note_linking_performance", "lineno": 163, "outcome": "passed", "keywords": ["test_note_linking_performance", "TestOptimizedPerformance", "test_performance_optimized.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0003804219886660576, "outcome": "passed"}, "call": {"duration": 0.010307969059795141, "outcome": "passed"}, "teardown": {"duration": 0.0005824300460517406, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_performance_optimized.py::TestOptimizedPerformance::test_citation_processing", "lineno": 188, "outcome": "passed", "keywords": ["test_citation_processing", "TestOptimizedPerformance", "test_performance_optimized.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0011939359828829765, "outcome": "passed"}, "call": {"duration": 0.06865596538409591, "outcome": "passed", "stdout": "\nProcessed 5 papers in 0.07 seconds (74.23 papers/second)\n"}, "teardown": {"duration": 0.0006567919626832008, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions::test_create_research_question", "lineno": 30, "outcome": "failed", "keywords": ["test_create_research_question", "TestResearchQuestions", "test_research_questions.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0015216260217130184, "outcome": "passed"}, "call": {"duration": 0.00030319206416606903, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.HIGH: 'high'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_research_questions.py", "lineno": 34, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_research_questions.TestResearchQuestions object at 0x7fe54fc03c40>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a84011e0>\n\n    def test_create_research_question(self, brain):\n        \"\"\"Test creating a research question with all attributes.\"\"\"\n        # Create a research question with all fields\n>       question_id = brain.create_research_question(\n            question=\"What are the neural mechanisms of autobiographical memory?\",\n            description=\"Investigating the brain regions and processes involved in personal memory.\",\n            tags={\"memory\", \"neuroscience\", \"autobiographical\"},\n            status=\"open\",\n            priority=8\n        )\n\ntests/academic_researcher/test_research_questions.py:34: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'What are the neural mechanisms of autobiographical memory?'\ntitle = 'What are the neural mechanisms of autobiographical memory?'\ncontent = 'Investigating the brain regions and processes involved in personal memory.'\ndescription = 'Investigating the brain regions and processes involved in personal memory.'\npriority = <Priority.HIGH: 'high'>\nkwargs = {'description': 'Investigating the brain regions and processes involved in personal memory.', 'knowledge_gaps': [], 'priority': <Priority.HIGH: 'high'>, 'question': 'What are the neural mechanisms of autobiographical memory?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.HIGH: 'high'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0006161676719784737, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions::test_adding_supporting_evidence", "lineno": 60, "outcome": "failed", "keywords": ["test_adding_supporting_evidence", "TestResearchQuestions", "test_research_questions.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.001255825161933899, "outcome": "passed"}, "call": {"duration": 0.00028900615870952606, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.HIGH: 'high'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_research_questions.py", "lineno": 64, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_research_questions.TestResearchQuestions object at 0x7fe54fc03e20>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a84ba050>\n\n    def test_adding_supporting_evidence(self, brain):\n        \"\"\"Test adding supporting evidence to a research question.\"\"\"\n        # Create a research question\n>       question_id = brain.create_research_question(\n            question=\"Does caffeine improve cognitive performance?\",\n            priority=7\n        )\n\ntests/academic_researcher/test_research_questions.py:64: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'Does caffeine improve cognitive performance?'\ntitle = 'Does caffeine improve cognitive performance?', content = None\ndescription = None, priority = <Priority.HIGH: 'high'>\nkwargs = {'description': None, 'knowledge_gaps': [], 'priority': <Priority.HIGH: 'high'>, 'question': 'Does caffeine improve cognitive performance?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.HIGH: 'high'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0007518171332776546, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions::test_adding_contradicting_evidence", "lineno": 116, "outcome": "failed", "keywords": ["test_adding_contradicting_evidence", "TestResearchQuestions", "test_research_questions.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0013647549785673618, "outcome": "passed"}, "call": {"duration": 0.0003316449001431465, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_research_questions.py", "lineno": 120, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_research_questions.TestResearchQuestions object at 0x7fe54fc2c070>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a81ebe80>\n\n    def test_adding_contradicting_evidence(self, brain):\n        \"\"\"Test adding contradicting evidence to a research question.\"\"\"\n        # Create a research question\n>       question_id = brain.create_research_question(\n            question=\"Is meditation effective for reducing anxiety?\",\n            priority=9\n        )\n\ntests/academic_researcher/test_research_questions.py:120: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'Is meditation effective for reducing anxiety?'\ntitle = 'Is meditation effective for reducing anxiety?', content = None\ndescription = None, priority = <Priority.CRITICAL: 'critical'>\nkwargs = {'description': None, 'knowledge_gaps': [], 'priority': <Priority.CRITICAL: 'critical'>, 'question': 'Is meditation effective for reducing anxiety?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.000629388727247715, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions::test_balanced_evidence_evaluation", "lineno": 158, "outcome": "failed", "keywords": ["test_balanced_evidence_evaluation", "TestResearchQuestions", "test_research_questions.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012883427552878857, "outcome": "passed"}, "call": {"duration": 0.00032555917277932167, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.HIGH: 'high'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_research_questions.py", "lineno": 162, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_research_questions.TestResearchQuestions object at 0x7fe54fc2c2b0>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a70ade70>\n\n    def test_balanced_evidence_evaluation(self, brain):\n        \"\"\"Test evaluating both supporting and contradicting evidence for a question.\"\"\"\n        # Create a research question\n>       question_id = brain.create_research_question(\n            question=\"Do omega-3 supplements improve cardiovascular health?\",\n            description=\"Examining the effects of omega-3 fatty acid supplementation on heart health metrics.\",\n            priority=8\n        )\n\ntests/academic_researcher/test_research_questions.py:162: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'Do omega-3 supplements improve cardiovascular health?'\ntitle = 'Do omega-3 supplements improve cardiovascular health?'\ncontent = 'Examining the effects of omega-3 fatty acid supplementation on heart health metrics.'\ndescription = 'Examining the effects of omega-3 fatty acid supplementation on heart health metrics.'\npriority = <Priority.HIGH: 'high'>\nkwargs = {'description': 'Examining the effects of omega-3 fatty acid supplementation on heart health metrics.', 'knowledge_gaps': [], 'priority': <Priority.HIGH: 'high'>, 'question': 'Do omega-3 supplements improve cardiovascular health?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.HIGH: 'high'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.000616386067122221, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions::test_evidence_strength_levels", "lineno": 253, "outcome": "failed", "keywords": ["test_evidence_strength_levels", "TestResearchQuestions", "test_research_questions.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012526377104222775, "outcome": "passed"}, "call": {"duration": 0.00029480597004294395, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.HIGH: 'high'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_research_questions.py", "lineno": 257, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_research_questions.TestResearchQuestions object at 0x7fe54fc2c4c0>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a83db370>\n\n    def test_evidence_strength_levels(self, brain):\n        \"\"\"Test different evidence strength levels.\"\"\"\n        # Create a research question\n>       question_id = brain.create_research_question(\n            question=\"How effective are mindfulness interventions for depression?\",\n            priority=7\n        )\n\ntests/academic_researcher/test_research_questions.py:257: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'How effective are mindfulness interventions for depression?'\ntitle = 'How effective are mindfulness interventions for depression?'\ncontent = None, description = None, priority = <Priority.HIGH: 'high'>\nkwargs = {'description': None, 'knowledge_gaps': [], 'priority': <Priority.HIGH: 'high'>, 'question': 'How effective are mindfulness interventions for depression?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.HIGH: 'high'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0006201118230819702, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions::test_evidence_with_multiple_citations", "lineno": 337, "outcome": "failed", "keywords": ["test_evidence_with_multiple_citations", "TestResearchQuestions", "test_research_questions.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.001281099859625101, "outcome": "passed"}, "call": {"duration": 0.0002933940850198269, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.HIGH: 'high'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_research_questions.py", "lineno": 341, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_research_questions.TestResearchQuestions object at 0x7fe54fc2c6d0>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a85c8640>\n\n    def test_evidence_with_multiple_citations(self, brain):\n        \"\"\"Test evidence supported by multiple citations.\"\"\"\n        # Create a research question\n>       question_id = brain.create_research_question(\n            question=\"What dietary factors influence gut microbiome composition?\",\n            priority=8\n        )\n\ntests/academic_researcher/test_research_questions.py:341: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'What dietary factors influence gut microbiome composition?'\ntitle = 'What dietary factors influence gut microbiome composition?'\ncontent = None, description = None, priority = <Priority.HIGH: 'high'>\nkwargs = {'description': None, 'knowledge_gaps': [], 'priority': <Priority.HIGH: 'high'>, 'question': 'What dietary factors influence gut microbiome composition?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.HIGH: 'high'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0006661871448159218, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions::test_related_questions", "lineno": 385, "outcome": "failed", "keywords": ["test_related_questions", "TestResearchQuestions", "test_research_questions.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0013015256263315678, "outcome": "passed"}, "call": {"duration": 0.0003099101595580578, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_research_questions.py", "lineno": 389, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_research_questions.TestResearchQuestions object at 0x7fe54fc2c8e0>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a8402470>\n\n    def test_related_questions(self, brain):\n        \"\"\"Test connecting related research questions.\"\"\"\n        # Create multiple related research questions\n>       main_question_id = brain.create_research_question(\n            question=\"What are the neural correlates of consciousness?\",\n            priority=9\n        )\n\ntests/academic_researcher/test_research_questions.py:389: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'What are the neural correlates of consciousness?'\ntitle = 'What are the neural correlates of consciousness?', content = None\ndescription = None, priority = <Priority.CRITICAL: 'critical'>\nkwargs = {'description': None, 'knowledge_gaps': [], 'priority': <Priority.CRITICAL: 'critical'>, 'question': 'What are the neural correlates of consciousness?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0007918230257928371, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_research_questions.py::TestResearchQuestions::test_identifying_knowledge_gaps", "lineno": 504, "outcome": "failed", "keywords": ["test_identifying_knowledge_gaps", "TestResearchQuestions", "test_research_questions.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0014349729754030704, "outcome": "passed"}, "call": {"duration": 0.00035097217187285423, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_research_questions.py", "lineno": 508, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_research_questions.TestResearchQuestions object at 0x7fe54fc2caf0>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a8327340>\n\n    def test_identifying_knowledge_gaps(self, brain):\n        \"\"\"Test identifying knowledge gaps in research questions.\"\"\"\n        # Create a research question with knowledge gaps\n>       question_id = brain.create_research_question(\n            question=\"How do environmental factors interact with genetic predispositions in mental health?\",\n            description=\"Understanding gene-environment interactions in psychiatric disorders.\",\n            priority=9,\n            knowledge_gaps=[\n                \"Specific molecular pathways affected by environmental stress\",\n                \"Temporal windows of vulnerability to environmental factors\",\n                \"Reversibility of environmentally-induced epigenetic changes\"\n            ]\n        )\n\ntests/academic_researcher/test_research_questions.py:508: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'How do environmental factors interact with genetic predispositions in mental health?'\ntitle = 'How do environmental factors interact with genetic predispositions in mental health?'\ncontent = 'Understanding gene-environment interactions in psychiatric disorders.'\ndescription = 'Understanding gene-environment interactions in psychiatric disorders.'\npriority = <Priority.CRITICAL: 'critical'>\nkwargs = {'description': 'Understanding gene-environment interactions in psychiatric disorders.', 'knowledge_gaps': ['Specific ...: 'critical'>, 'question': 'How do environmental factors interact with genetic predispositions in mental health?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0007643480785191059, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_workflows.py::TestUserWorkflows::test_workflow_paper_import_to_note_creation", "lineno": 31, "outcome": "passed", "keywords": ["test_workflow_paper_import_to_note_creation", "TestUserWorkflows", "test_workflows.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0018783248960971832, "outcome": "passed"}, "call": {"duration": 0.025702122133225203, "outcome": "passed"}, "teardown": {"duration": 0.0006822329014539719, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_workflows.py::TestUserWorkflows::test_workflow_research_question_analysis", "lineno": 83, "outcome": "failed", "keywords": ["test_workflow_research_question_analysis", "TestUserWorkflows", "test_workflows.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012917299754917622, "outcome": "passed"}, "call": {"duration": 0.0003109341487288475, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_workflows.py", "lineno": 87, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_workflows.TestUserWorkflows object at 0x7fe54fc2db70>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a8323fd0>\n\n    def test_workflow_research_question_analysis(self, brain):\n        \"\"\"Test workflow: research question -> evidence evaluation with conflicting evidence.\"\"\"\n        # Create a research question\n>       question_id = brain.create_research_question(\n            question=\"Does neuroplasticity decline with age?\",\n            description=\"Investigating the relationship between aging and neural plasticity\",\n            priority=9\n        )\n\ntests/academic_researcher/test_workflows.py:87: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion(), question = 'Does neuroplasticity decline with age?'\ntitle = 'Does neuroplasticity decline with age?'\ncontent = 'Investigating the relationship between aging and neural plasticity'\ndescription = 'Investigating the relationship between aging and neural plasticity'\npriority = <Priority.CRITICAL: 'critical'>\nkwargs = {'description': 'Investigating the relationship between aging and neural plasticity', 'knowledge_gaps': [], 'priority': <Priority.CRITICAL: 'critical'>, 'question': 'Does neuroplasticity decline with age?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0006251148879528046, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_workflows.py::TestUserWorkflows::test_workflow_grant_proposal_assembly", "lineno": 158, "outcome": "failed", "keywords": ["test_workflow_grant_proposal_assembly", "TestUserWorkflows", "test_workflows.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.001268208958208561, "outcome": "passed"}, "call": {"duration": 0.00029958179220557213, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_workflows.py", "lineno": 164, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_workflows.TestUserWorkflows object at 0x7fe54fc2dd50>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a70b8f10>\n\n    def test_workflow_grant_proposal_assembly(self, brain):\n        \"\"\"Test workflow: grant proposal assembly from distributed knowledge elements.\"\"\"\n        # Create several elements that will go into a grant proposal\n    \n        # 1. Create research questions\n>       question1_id = brain.create_research_question(\n            question=\"How does intervention X affect cognition in elderly population?\",\n            priority=9\n        )\n\ntests/academic_researcher/test_workflows.py:164: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'How does intervention X affect cognition in elderly population?'\ntitle = 'How does intervention X affect cognition in elderly population?'\ncontent = None, description = None, priority = <Priority.CRITICAL: 'critical'>\nkwargs = {'description': None, 'knowledge_gaps': [], 'priority': <Priority.CRITICAL: 'critical'>, 'question': 'How does intervention X affect cognition in elderly population?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.CRITICAL: 'critical'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0006110398098826408, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_workflows.py::TestUserWorkflows::test_workflow_collaborative_annotation", "lineno": 263, "outcome": "failed", "keywords": ["test_workflow_collaborative_annotation", "TestUserWorkflows", "test_workflows.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0012499867007136345, "outcome": "passed"}, "call": {"duration": 0.002469527069479227, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError: cannot pickle 'property' object"}, "traceback": [{"path": "tests/academic_researcher/test_workflows.py", "lineno": 273, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 822, "message": "in create_collaborator"}, {"path": "researchbrain/core/storage.py", "lineno": 131, "message": "in save"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 253, "message": "in dump"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py", "lineno": 241, "message": "in dump_all"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 27, "message": "in represent"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 48, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 207, "message": "in represent_dict"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 118, "message": "in represent_mapping"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 52, "message": "in represent_data"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py", "lineno": 317, "message": "TypeError"}], "longrepr": "self = <tests.academic_researcher.test_workflows.TestUserWorkflows object at 0x7fe54fc2d7e0>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a8213370>\n\n    def test_workflow_collaborative_annotation(self, brain):\n        \"\"\"Test workflow: collaborative review and annotation of research materials.\"\"\"\n        # 1. Create a note to be annotated\n        note_id = brain.create_note(\n            title=\"Draft Manuscript\",\n            content=\"Abstract\\nIntroduction\\nMethods\\nResults\\nDiscussion\\nConclusion\"\n        )\n    \n        # 2. Create collaborators\n>       advisor_id = brain.create_collaborator(\n            name=\"Dr. Johnson\",\n            email=\"johnson@university.edu\",\n            affiliation=\"University of Science\",\n            role=\"advisor\"\n        )\n\ntests/academic_researcher/test_workflows.py:273: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:822: in create_collaborator\n    self.storage.save(collaborator)\nresearchbrain/core/storage.py:131: in save\n    yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:253: in dump\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/__init__.py:241: in dump_all\n    dumper.represent(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:27: in represent\n    node = self.represent_data(data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:48: in represent_data\n    node = self.yaml_representers[data_types[0]](self, data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:207: in represent_dict\n    return self.represent_mapping('tag:yaml.org,2002:map', data)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:118: in represent_mapping\n    node_value = self.represent_data(item_value)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:52: in represent_data\n    node = self.yaml_multi_representers[data_type](self, data)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <yaml.dumper.Dumper object at 0x7fe4a8212410>\ndata = <property object at 0x7fe5ffc8e4d0>\n\n    def represent_object(self, data):\n        # We use __reduce__ API to save the data. data.__reduce__ returns\n        # a tuple of length 2-5:\n        #   (function, args, state, listitems, dictitems)\n    \n        # For reconstructing, we calls function(*args), then set its state,\n        # listitems, and dictitems if they are not None.\n    \n        # A special case is when function.__name__ == '__newobj__'. In this\n        # case we create the object with args[0].__new__(*args).\n    \n        # Another special case is when __reduce__ returns a string - we don't\n        # support it.\n    \n        # We produce a !!python/object, !!python/object/new or\n        # !!python/object/apply node.\n    \n        cls = type(data)\n        if cls in copyreg.dispatch_table:\n            reduce = copyreg.dispatch_table[cls](data)\n        elif hasattr(data, '__reduce_ex__'):\n>           reduce = data.__reduce_ex__(2)\nE           TypeError: cannot pickle 'property' object\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/yaml/representer.py:317: TypeError"}, "teardown": {"duration": 0.0007911892607808113, "outcome": "passed"}}, {"nodeid": "tests/academic_researcher/test_workflows.py::TestUserWorkflows::test_workflow_experiment_documentation", "lineno": 333, "outcome": "failed", "keywords": ["test_workflow_experiment_documentation", "TestUserWorkflows", "test_workflows.py", "academic_researcher", "tests", "unified", ""], "setup": {"duration": 0.0013972818851470947, "outcome": "passed"}, "call": {"duration": 0.019035165198147297, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/researchbrain/core/models.py", "lineno": 136, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\npriority\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.HIGH: 'high'>, input_type=Priority]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing"}, "traceback": [{"path": "tests/academic_researcher/test_workflows.py", "lineno": 370, "message": ""}, {"path": "researchbrain/core/brain.py", "lineno": 531, "message": "in create_research_question"}, {"path": "researchbrain/core/models.py", "lineno": 136, "message": "ValidationError"}], "longrepr": "self = <tests.academic_researcher.test_workflows.TestUserWorkflows object at 0x7fe54fc2dae0>\nbrain = <researchbrain.core.brain.ResearchBrain object at 0x7fe4a710dd20>\n\n    def test_workflow_experiment_documentation(self, brain):\n        \"\"\"Test workflow: experiment documentation with template-guided metadata.\"\"\"\n        # 1. Create an experiment from a template\n        experiment_id = brain.create_experiment_from_template(\n            template_name=\"behavioral_experiment\",\n            title=\"Memory Recall Experiment\",\n            hypothesis=\"Regular exercise improves memory recall in older adults\",\n            participants=\"60 adults aged 65-80, randomized into exercise and control groups\",\n            independent_variables=\"Exercise regimen (none vs. 30 min daily walking)\",\n            dependent_variables=\"Memory recall scores on standardized tests\",\n            control_condition=\"No exercise regimen\",\n            procedure=\"Participants will be tested at baseline, 6 weeks, and 12 weeks\",\n            analysis_plan=\"Mixed ANOVA with follow-up t-tests\"\n        )\n    \n        assert experiment_id is not None\n    \n        # 2. Retrieve the experiment and verify template application\n        experiment = brain.storage.get(Experiment, experiment_id)\n    \n        assert experiment.title == \"Memory Recall Experiment\"\n        assert experiment.hypothesis == \"Regular exercise improves memory recall in older adults\"\n        assert \"Behavioral Experiment Methodology\" in experiment.methodology\n        assert \"60 adults aged 65-80\" in experiment.methodology\n        assert \"Exercise regimen\" in experiment.methodology\n        assert \"Memory recall scores\" in experiment.methodology\n        assert \"No exercise regimen\" in experiment.methodology\n        assert \"tested at baseline, 6 weeks, and 12 weeks\" in experiment.methodology\n        assert \"Mixed ANOVA\" in experiment.methodology\n    \n        assert experiment.status == ExperimentStatus.PLANNED\n        assert \"participants\" in experiment.variables\n        assert \"independent_vars\" in experiment.variables\n        assert \"dependent_vars\" in experiment.variables\n    \n        # 3. Create research question related to experiment\n>       question_id = brain.create_research_question(\n            question=\"How does exercise affect memory in older adults?\",\n            priority=8\n        )\n\ntests/academic_researcher/test_workflows.py:370: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nresearchbrain/core/brain.py:531: in create_research_question\n    research_question = ResearchQuestion(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = ResearchQuestion()\nquestion = 'How does exercise affect memory in older adults?'\ntitle = 'How does exercise affect memory in older adults?', content = None\ndescription = None, priority = <Priority.HIGH: 'high'>\nkwargs = {'description': None, 'knowledge_gaps': [], 'priority': <Priority.HIGH: 'high'>, 'question': 'How does exercise affect memory in older adults?', ...}\n\n    def __init__(self, question: str, title: str = \"\", content: str = \"\", description: Optional[str] = None, priority: int = 0, **kwargs):\n        \"\"\"Initialize ResearchQuestion with question as title if title not provided.\"\"\"\n        if not title:\n            title = question\n        if not content and description:\n            content = description\n    \n        kwargs['question'] = question\n        kwargs['description'] = description\n        kwargs['priority'] = priority\n>       super().__init__(title=title, content=content, **kwargs)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchQuestion\nE       priority\nE         Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value=<Priority.HIGH: 'high'>, input_type=Priority]\nE           For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n\nresearchbrain/core/models.py:136: ValidationError"}, "teardown": {"duration": 0.0006647277623414993, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_initialization", "lineno": 15, "outcome": "passed", "keywords": ["test_initialization", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.001607453916221857, "outcome": "passed"}, "call": {"duration": 0.0003199223428964615, "outcome": "passed"}, "teardown": {"duration": 0.0002882499247789383, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_add_competitor", "lineno": 28, "outcome": "passed", "keywords": ["test_add_competitor", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0011700759641826153, "outcome": "passed"}, "call": {"duration": 0.0005523110739886761, "outcome": "passed"}, "teardown": {"duration": 0.00031081587076187134, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_add_competitive_feature", "lineno": 53, "outcome": "passed", "keywords": ["test_add_competitive_feature", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013988730497658253, "outcome": "passed"}, "call": {"duration": 0.0005458458326756954, "outcome": "passed"}, "teardown": {"duration": 0.00034735025838017464, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_add_market_gap", "lineno": 78, "outcome": "passed", "keywords": ["test_add_market_gap", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0014374437741935253, "outcome": "passed"}, "call": {"duration": 0.0005086669698357582, "outcome": "passed"}, "teardown": {"duration": 0.0003461628220975399, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_get_competitor", "lineno": 103, "outcome": "passed", "keywords": ["test_get_competitor", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.001171792857348919, "outcome": "passed"}, "call": {"duration": 0.0005560899153351784, "outcome": "passed"}, "teardown": {"duration": 0.00031443312764167786, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_get_competitive_feature", "lineno": 120, "outcome": "passed", "keywords": ["test_get_competitive_feature", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0014033932238817215, "outcome": "passed"}, "call": {"duration": 0.0006268969736993313, "outcome": "passed"}, "teardown": {"duration": 0.00034743500873446465, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_get_market_gap", "lineno": 137, "outcome": "passed", "keywords": ["test_get_market_gap", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0014183931052684784, "outcome": "passed"}, "call": {"duration": 0.0005018538795411587, "outcome": "passed"}, "teardown": {"duration": 0.0003460468724370003, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_get_all_competitors", "lineno": 154, "outcome": "passed", "keywords": ["test_get_all_competitors", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0011402089148759842, "outcome": "passed"}, "call": {"duration": 0.0005592107772827148, "outcome": "passed"}, "teardown": {"duration": 0.0003171502612531185, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_get_all_competitive_features", "lineno": 169, "outcome": "passed", "keywords": ["test_get_all_competitive_features", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013843532651662827, "outcome": "passed"}, "call": {"duration": 0.0006315098144114017, "outcome": "passed"}, "teardown": {"duration": 0.00034343497827649117, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_get_all_market_gaps", "lineno": 184, "outcome": "passed", "keywords": ["test_get_all_market_gaps", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0014204648323357105, "outcome": "passed"}, "call": {"duration": 0.0005141273140907288, "outcome": "passed"}, "teardown": {"duration": 0.00034576375037431717, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_update_competitor_feature", "lineno": 199, "outcome": "passed", "keywords": ["test_update_competitor_feature", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0011426950804889202, "outcome": "passed"}, "call": {"duration": 0.000537618063390255, "outcome": "passed"}, "teardown": {"duration": 0.0003349953331053257, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_update_feature_implementation", "lineno": 227, "outcome": "passed", "keywords": ["test_update_feature_implementation", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.001357833854854107, "outcome": "passed"}, "call": {"duration": 0.0006106500513851643, "outcome": "passed"}, "teardown": {"duration": 0.0003665788099169731, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_compare_features", "lineno": 266, "outcome": "passed", "keywords": ["test_compare_features", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013815127313137054, "outcome": "passed"}, "call": {"duration": 0.0016540451906621456, "outcome": "passed"}, "teardown": {"duration": 0.00034969858825206757, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_identify_gaps", "lineno": 313, "outcome": "passed", "keywords": ["test_identify_gaps", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013838699087500572, "outcome": "passed"}, "call": {"duration": 0.0008943090215325356, "outcome": "passed"}, "teardown": {"duration": 0.00034476490691304207, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_create_market_gap_from_analysis", "lineno": 348, "outcome": "passed", "keywords": ["test_create_market_gap_from_analysis", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013961009681224823, "outcome": "passed"}, "call": {"duration": 0.0009725191630423069, "outcome": "passed"}, "teardown": {"duration": 0.00034634070470929146, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_generate_competitive_matrix", "lineno": 382, "outcome": "passed", "keywords": ["test_generate_competitive_matrix", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0011805691756308079, "outcome": "passed"}, "call": {"duration": 0.0009201890788972378, "outcome": "passed"}, "teardown": {"duration": 0.0003095981664955616, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_track_competitive_timeline", "lineno": 433, "outcome": "passed", "keywords": ["test_track_competitive_timeline", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.001411696895956993, "outcome": "passed"}, "call": {"duration": 0.0010485979728400707, "outcome": "passed"}, "teardown": {"duration": 0.00035606278106570244, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_generate_feature_parity_report", "lineno": 472, "outcome": "passed", "keywords": ["test_generate_feature_parity_report", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013962732627987862, "outcome": "passed"}, "call": {"duration": 0.0012502288445830345, "outcome": "passed"}, "teardown": {"duration": 0.0003438270650804043, "outcome": "passed"}}, {"nodeid": "tests/product_manager/competitive_analysis/test_system.py::TestCompetitiveAnalysisSystem::test_performance_with_large_dataset", "lineno": 530, "outcome": "passed", "keywords": ["test_performance_with_large_dataset", "TestCompetitiveAnalysisSystem", "test_system.py", "competitive_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0010321908630430698, "outcome": "passed"}, "call": {"duration": 0.005161079112440348, "outcome": "passed"}, "teardown": {"duration": 0.0003266246058046818, "outcome": "passed"}}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_initialization", "lineno": 15, "outcome": "passed", "keywords": ["test_initialization", "TestDecisionRegistry", "test_registry.py", "decision_registry", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013139992952346802, "outcome": "passed"}, "call": {"duration": 0.0002651861868798733, "outcome": "passed"}, "teardown": {"duration": 0.00028029317036271095, "outcome": "passed"}}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_add_decision", "lineno": 24, "outcome": "passed", "keywords": ["test_add_decision", "TestDecisionRegistry", "test_registry.py", "decision_registry", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013861283659934998, "outcome": "passed"}, "call": {"duration": 0.00048170192167162895, "outcome": "passed"}, "teardown": {"duration": 0.00030875392258167267, "outcome": "passed"}}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_get_decision", "lineno": 48, "outcome": "failed", "keywords": ["test_get_decision", "TestDecisionRegistry", "test_registry.py", "decision_registry", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013642562553286552, "outcome": "passed"}, "call": {"duration": 0.00048317620530724525, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 989, "message": "AttributeError: 'Decision' object has no attribute 'description'"}, "traceback": [{"path": "tests/product_manager/decision_registry/test_registry.py", "lineno": 60, "message": ""}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 989, "message": "AttributeError"}], "longrepr": "self = <tests.product_manager.decision_registry.test_registry.TestDecisionRegistry object at 0x7fe54fcd8670>\ntemp_data_dir = '/tmp/pytest-of-justinchiu_cohere_com/pytest-430/test_get_decision0/test_data'\ndecision_samples = [Decision(id=UUID('fe7b6837-9526-4b6e-9a2f-75d2b474f006'), created_at=datetime.datetime(2025, 6, 16, 4, 15, 7, 103469)...ad204e')], related_features=[UUID('fcae7a3f-98e8-47d1-962d-274008e900bf')], status='decided', outcome_assessment=None)]\n\n    def test_get_decision(self, temp_data_dir, decision_samples):\n        \"\"\"Test retrieving decisions.\"\"\"\n        registry = DecisionRegistry(storage_dir=temp_data_dir)\n        registry.add_decision(decision_samples)\n    \n        # Test retrieving existing decision\n        for decision in decision_samples:\n            retrieved = registry.get_decision(str(decision.id))\n            assert retrieved is not None\n            assert retrieved.id == decision.id\n            assert retrieved.title == decision.title\n>           assert retrieved.description == decision.description\n\ntests/product_manager/decision_registry/test_registry.py:60: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Decision(id=UUID('fe7b6837-9526-4b6e-9a2f-75d2b474f006'), created_at=datetime.datetime(2025, 6, 16, 4, 15, 7, 103469),...thin 3 months of launch. Development was completed in 4.5 months, slightly over schedule but within acceptable range.')\nitem = 'description'\n\n    def __getattr__(self, item: str) -> Any:\n        private_attributes = object.__getattribute__(self, '__private_attributes__')\n        if item in private_attributes:\n            attribute = private_attributes[item]\n            if hasattr(attribute, '__get__'):\n                return attribute.__get__(self, type(self))  # type: ignore\n    \n            try:\n                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items\n                return self.__pydantic_private__[item]  # type: ignore\n            except KeyError as exc:\n                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\n        else:\n            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.\n            # See `BaseModel.__repr_args__` for more details\n            try:\n                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')\n            except AttributeError:\n                pydantic_extra = None\n    \n            if pydantic_extra:\n                try:\n                    return pydantic_extra[item]\n                except KeyError as exc:\n                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\n            else:\n                if hasattr(self.__class__, item):\n                    return super().__getattribute__(item)  # Raises AttributeError if appropriate\n                else:\n                    # this is the current error\n>                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\nE                   AttributeError: 'Decision' object has no attribute 'description'\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:989: AttributeError"}, "teardown": {"duration": 0.0003581112250685692, "outcome": "passed"}}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_get_all_decisions", "lineno": 65, "outcome": "passed", "keywords": ["test_get_all_decisions", "TestDecisionRegistry", "test_registry.py", "decision_registry", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013799890875816345, "outcome": "passed"}, "call": {"duration": 0.00046412041410803795, "outcome": "passed"}, "teardown": {"duration": 0.00031675491482019424, "outcome": "passed"}}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_add_alternative_to_decision", "lineno": 80, "outcome": "passed", "keywords": ["test_add_alternative_to_decision", "TestDecisionRegistry", "test_registry.py", "decision_registry", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013869996182620525, "outcome": "passed"}, "call": {"duration": 0.0006255912594497204, "outcome": "passed"}, "teardown": {"duration": 0.00033436715602874756, "outcome": "passed"}}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_link_related_decisions", "lineno": 142, "outcome": "passed", "keywords": ["test_link_related_decisions", "TestDecisionRegistry", "test_registry.py", "decision_registry", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013935868628323078, "outcome": "passed"}, "call": {"duration": 0.00067945197224617, "outcome": "passed"}, "teardown": {"duration": 0.0003173071891069412, "outcome": "passed"}}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_record_outcome_assessment", "lineno": 186, "outcome": "passed", "keywords": ["test_record_outcome_assessment", "TestDecisionRegistry", "test_registry.py", "decision_registry", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013802098110318184, "outcome": "passed"}, "call": {"duration": 0.0005807247944176197, "outcome": "passed"}, "teardown": {"duration": 0.00031182775273919106, "outcome": "passed"}}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_search_decisions", "lineno": 210, "outcome": "passed", "keywords": ["test_search_decisions", "TestDecisionRegistry", "test_registry.py", "decision_registry", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013617640361189842, "outcome": "passed"}, "call": {"duration": 0.0006439080461859703, "outcome": "passed"}, "teardown": {"duration": 0.0003105239011347294, "outcome": "passed"}}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_get_decision_history", "lineno": 247, "outcome": "passed", "keywords": ["test_get_decision_history", "TestDecisionRegistry", "test_registry.py", "decision_registry", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013807681389153004, "outcome": "passed"}, "call": {"duration": 0.0005091880448162556, "outcome": "passed"}, "teardown": {"duration": 0.0003064572811126709, "outcome": "passed"}}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_build_decision_graph", "lineno": 278, "outcome": "passed", "keywords": ["test_build_decision_graph", "TestDecisionRegistry", "test_registry.py", "decision_registry", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013811802491545677, "outcome": "passed"}, "call": {"duration": 0.0006847870536148548, "outcome": "passed"}, "teardown": {"duration": 0.0003093639388680458, "outcome": "passed"}}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_analyze_alternatives", "lineno": 333, "outcome": "passed", "keywords": ["test_analyze_alternatives", "TestDecisionRegistry", "test_registry.py", "decision_registry", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013779220171272755, "outcome": "passed"}, "call": {"duration": 0.0005372706800699234, "outcome": "passed"}, "teardown": {"duration": 0.00031140493229031563, "outcome": "passed"}}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_generate_decision_template", "lineno": 375, "outcome": "passed", "keywords": ["test_generate_decision_template", "TestDecisionRegistry", "test_registry.py", "decision_registry", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0010461020283401012, "outcome": "passed"}, "call": {"duration": 0.0002848617732524872, "outcome": "passed"}, "teardown": {"duration": 0.00028977589681744576, "outcome": "passed"}}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_export_decision", "lineno": 413, "outcome": "failed", "keywords": ["test_export_decision", "TestDecisionRegistry", "test_registry.py", "decision_registry", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013978960923850536, "outcome": "passed"}, "call": {"duration": 0.00048162275925278664, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 989, "message": "AttributeError: 'Decision' object has no attribute 'description'"}, "traceback": [{"path": "tests/product_manager/decision_registry/test_registry.py", "lineno": 426, "message": ""}, {"path": "productmind/decision_registry/registry.py", "lineno": 721, "message": "in export_decision"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 989, "message": "AttributeError"}], "longrepr": "self = <tests.product_manager.decision_registry.test_registry.TestDecisionRegistry object at 0x7fe54fcdb760>\ntemp_data_dir = '/tmp/pytest-of-justinchiu_cohere_com/pytest-430/test_export_decision0/test_data'\ndecision_samples = [Decision(id=UUID('39758754-330f-43db-87c4-2f94fa9c9d5b'), created_at=datetime.datetime(2025, 6, 16, 4, 15, 7, 184790)...41abf1')], related_features=[UUID('07d98696-848d-4f83-99c7-6ef3dd32d180')], status='decided', outcome_assessment=None)]\n\n    def test_export_decision(self, temp_data_dir, decision_samples):\n        \"\"\"Test exporting a decision in different formats.\"\"\"\n        registry = DecisionRegistry(storage_dir=temp_data_dir)\n        registry.add_decision(decision_samples)\n    \n        if decision_samples:\n            decision_id = str(decision_samples[0].id)\n    \n            # Test exporting in different formats\n            formats = [\"text\", \"markdown\", \"json\"]\n    \n            for format in formats:\n>               exported = registry.export_decision(decision_id, format)\n\ntests/product_manager/decision_registry/test_registry.py:426: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nproductmind/decision_registry/registry.py:721: in export_decision\n    lines.append(decision.description)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Decision(id=UUID('39758754-330f-43db-87c4-2f94fa9c9d5b'), created_at=datetime.datetime(2025, 6, 16, 4, 15, 7, 184790),...thin 3 months of launch. Development was completed in 4.5 months, slightly over schedule but within acceptable range.')\nitem = 'description'\n\n    def __getattr__(self, item: str) -> Any:\n        private_attributes = object.__getattribute__(self, '__private_attributes__')\n        if item in private_attributes:\n            attribute = private_attributes[item]\n            if hasattr(attribute, '__get__'):\n                return attribute.__get__(self, type(self))  # type: ignore\n    \n            try:\n                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items\n                return self.__pydantic_private__[item]  # type: ignore\n            except KeyError as exc:\n                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\n        else:\n            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.\n            # See `BaseModel.__repr_args__` for more details\n            try:\n                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')\n            except AttributeError:\n                pydantic_extra = None\n    \n            if pydantic_extra:\n                try:\n                    return pydantic_extra[item]\n                except KeyError as exc:\n                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\n            else:\n                if hasattr(self.__class__, item):\n                    return super().__getattribute__(item)  # Raises AttributeError if appropriate\n                else:\n                    # this is the current error\n>                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\nE                   AttributeError: 'Decision' object has no attribute 'description'\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:989: AttributeError"}, "teardown": {"duration": 0.0003519989550113678, "outcome": "passed"}}, {"nodeid": "tests/product_manager/decision_registry/test_registry.py::TestDecisionRegistry::test_calculate_decision_stats", "lineno": 452, "outcome": "passed", "keywords": ["test_calculate_decision_stats", "TestDecisionRegistry", "test_registry.py", "decision_registry", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0014505032449960709, "outcome": "passed"}, "call": {"duration": 0.0004837340675294399, "outcome": "passed"}, "teardown": {"duration": 0.00031995493918657303, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_initialization", "lineno": 14, "outcome": "passed", "keywords": ["test_initialization", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013337037526071072, "outcome": "passed"}, "call": {"duration": 0.0003339909017086029, "outcome": "passed"}, "teardown": {"duration": 0.0002775690518319607, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_add_feedback", "lineno": 29, "outcome": "passed", "keywords": ["test_add_feedback", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0012867627665400505, "outcome": "passed"}, "call": {"duration": 0.000559953972697258, "outcome": "passed"}, "teardown": {"duration": 0.0003038509748876095, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_get_feedback", "lineno": 54, "outcome": "passed", "keywords": ["test_get_feedback", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0012744762934744358, "outcome": "passed"}, "call": {"duration": 0.0008372562006115913, "outcome": "passed"}, "teardown": {"duration": 0.00030668918043375015, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_get_all_feedback", "lineno": 71, "outcome": "passed", "keywords": ["test_get_all_feedback", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0012919362634420395, "outcome": "passed"}, "call": {"duration": 0.0008720988407731056, "outcome": "passed"}, "teardown": {"duration": 0.0003204462118446827, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_analyze_sentiment", "lineno": 86, "outcome": "passed", "keywords": ["test_analyze_sentiment", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0012986981309950352, "outcome": "passed"}, "call": {"duration": 0.0017185676842927933, "outcome": "passed"}, "teardown": {"duration": 0.0003315107896924019, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_cluster_feedback_kmeans", "lineno": 117, "outcome": "passed", "keywords": ["test_cluster_feedback_kmeans", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.001306920312345028, "outcome": "passed"}, "call": {"duration": 0.054308926686644554, "outcome": "passed"}, "teardown": {"duration": 0.00043738773092627525, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_cluster_feedback_dbscan", "lineno": 144, "outcome": "passed", "keywords": ["test_cluster_feedback_dbscan", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0016655242070555687, "outcome": "passed"}, "call": {"duration": 0.007165024057030678, "outcome": "passed"}, "teardown": {"duration": 0.0003970968537032604, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_extract_themes", "lineno": 164, "outcome": "passed", "keywords": ["test_extract_themes", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.001364134717732668, "outcome": "passed"}, "call": {"duration": 0.005720115266740322, "outcome": "passed"}, "teardown": {"duration": 0.0003254599869251251, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_detect_trends", "lineno": 192, "outcome": "passed", "keywords": ["test_detect_trends", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.001333689782768488, "outcome": "passed"}, "call": {"duration": 0.0011390331201255322, "outcome": "passed"}, "teardown": {"duration": 0.000325043685734272, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_get_cluster", "lineno": 229, "outcome": "passed", "keywords": ["test_get_cluster", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013056430034339428, "outcome": "passed"}, "call": {"duration": 0.008025399874895811, "outcome": "passed"}, "teardown": {"duration": 0.00041072117164731026, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_get_theme", "lineno": 248, "outcome": "passed", "keywords": ["test_get_theme", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0016646399162709713, "outcome": "passed"}, "call": {"duration": 0.00957444915547967, "outcome": "passed"}, "teardown": {"duration": 0.0003637252375483513, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_get_all_clusters", "lineno": 267, "outcome": "passed", "keywords": ["test_get_all_clusters", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013145068660378456, "outcome": "passed"}, "call": {"duration": 0.007958663161844015, "outcome": "passed"}, "teardown": {"duration": 0.0004128972068428993, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_get_all_themes", "lineno": 283, "outcome": "passed", "keywords": ["test_get_all_themes", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0017060101963579655, "outcome": "passed"}, "call": {"duration": 0.009644961915910244, "outcome": "passed"}, "teardown": {"duration": 0.0003517749719321728, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_search_feedback", "lineno": 299, "outcome": "passed", "keywords": ["test_search_feedback", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013453434221446514, "outcome": "passed"}, "call": {"duration": 0.0008810521103441715, "outcome": "passed"}, "teardown": {"duration": 0.0003130580298602581, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_get_feedback_by_theme", "lineno": 315, "outcome": "passed", "keywords": ["test_get_feedback_by_theme", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.00135416304692626, "outcome": "passed"}, "call": {"duration": 0.0009156628511846066, "outcome": "passed"}, "teardown": {"duration": 0.00032446999102830887, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_get_feedback_by_cluster", "lineno": 343, "outcome": "passed", "keywords": ["test_get_feedback_by_cluster", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013493229635059834, "outcome": "passed"}, "call": {"duration": 0.0075291418470442295, "outcome": "passed"}, "teardown": {"duration": 0.00042331311851739883, "outcome": "passed"}}, {"nodeid": "tests/product_manager/feedback_analysis/test_engine.py::TestFeedbackAnalysisEngine::test_performance_with_large_dataset", "lineno": 363, "outcome": "passed", "keywords": ["test_performance_with_large_dataset", "TestFeedbackAnalysisEngine", "test_engine.py", "feedback_analysis", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0014439430087804794, "outcome": "passed"}, "call": {"duration": 0.07088413089513779, "outcome": "passed"}, "teardown": {"duration": 0.00032404670491814613, "outcome": "passed"}}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_initialization", "lineno": 15, "outcome": "passed", "keywords": ["test_initialization", "TestPrioritizationFramework", "test_framework.py", "prioritization", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0014595198445022106, "outcome": "passed"}, "call": {"duration": 0.0002890210598707199, "outcome": "passed"}, "teardown": {"duration": 0.00029352540150284767, "outcome": "passed"}}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_add_feature", "lineno": 27, "outcome": "passed", "keywords": ["test_add_feature", "TestPrioritizationFramework", "test_framework.py", "prioritization", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0018938430584967136, "outcome": "passed"}, "call": {"duration": 0.0005717519670724869, "outcome": "passed"}, "teardown": {"duration": 0.00034881336614489555, "outcome": "passed"}}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_add_strategic_goal", "lineno": 52, "outcome": "passed", "keywords": ["test_add_strategic_goal", "TestPrioritizationFramework", "test_framework.py", "prioritization", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0012298510409891605, "outcome": "passed"}, "call": {"duration": 0.0005103619769215584, "outcome": "passed"}, "teardown": {"duration": 0.0003089243546128273, "outcome": "passed"}}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_get_feature", "lineno": 77, "outcome": "passed", "keywords": ["test_get_feature", "TestPrioritizationFramework", "test_framework.py", "prioritization", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0019017751328647137, "outcome": "passed"}, "call": {"duration": 0.0006597829051315784, "outcome": "passed"}, "teardown": {"duration": 0.0003595808520913124, "outcome": "passed"}}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_get_strategic_goal", "lineno": 94, "outcome": "passed", "keywords": ["test_get_strategic_goal", "TestPrioritizationFramework", "test_framework.py", "prioritization", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0012719398364424706, "outcome": "passed"}, "call": {"duration": 0.0005669421516358852, "outcome": "passed"}, "teardown": {"duration": 0.0003146096132695675, "outcome": "passed"}}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_get_all_features", "lineno": 111, "outcome": "passed", "keywords": ["test_get_all_features", "TestPrioritizationFramework", "test_framework.py", "prioritization", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0019451510161161423, "outcome": "passed"}, "call": {"duration": 0.0006902511231601238, "outcome": "passed"}, "teardown": {"duration": 0.00034841708838939667, "outcome": "passed"}}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_get_all_strategic_goals", "lineno": 126, "outcome": "passed", "keywords": ["test_get_all_strategic_goals", "TestPrioritizationFramework", "test_framework.py", "prioritization", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0012401030398905277, "outcome": "passed"}, "call": {"duration": 0.000612668227404356, "outcome": "passed"}, "teardown": {"duration": 0.000306176021695137, "outcome": "passed"}}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_map_strategic_alignment", "lineno": 141, "outcome": "passed", "keywords": ["test_map_strategic_alignment", "TestPrioritizationFramework", "test_framework.py", "prioritization", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0018982794135808945, "outcome": "passed"}, "call": {"duration": 0.0010777157731354237, "outcome": "passed"}, "teardown": {"duration": 0.00035532983019948006, "outcome": "passed"}}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_calculate_strategic_score", "lineno": 179, "outcome": "passed", "keywords": ["test_calculate_strategic_score", "TestPrioritizationFramework", "test_framework.py", "prioritization", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0019054580479860306, "outcome": "passed"}, "call": {"duration": 0.0012939800508320332, "outcome": "passed"}, "teardown": {"duration": 0.00036023790016770363, "outcome": "passed"}}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_prioritize_features_weighted", "lineno": 215, "outcome": "passed", "keywords": ["test_prioritize_features_weighted", "TestPrioritizationFramework", "test_framework.py", "prioritization", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.00192541116848588, "outcome": "passed"}, "call": {"duration": 0.002271370030939579, "outcome": "passed"}, "teardown": {"duration": 0.0003513679839670658, "outcome": "passed"}}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_prioritize_features_other_models", "lineno": 251, "outcome": "passed", "keywords": ["test_prioritize_features_other_models", "TestPrioritizationFramework", "test_framework.py", "prioritization", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0019298628903925419, "outcome": "passed"}, "call": {"duration": 0.004614843986928463, "outcome": "passed"}, "teardown": {"duration": 0.00035957014188170433, "outcome": "passed"}}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_analyze_dependencies", "lineno": 289, "outcome": "passed", "keywords": ["test_analyze_dependencies", "TestPrioritizationFramework", "test_framework.py", "prioritization", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0011226781643927097, "outcome": "passed"}, "call": {"duration": 0.0006803437136113644, "outcome": "passed"}, "teardown": {"duration": 0.0002956511452794075, "outcome": "passed"}}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_generate_roadmap", "lineno": 355, "outcome": "passed", "keywords": ["test_generate_roadmap", "TestPrioritizationFramework", "test_framework.py", "prioritization", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.001991668250411749, "outcome": "passed"}, "call": {"duration": 0.001706823706626892, "outcome": "passed"}, "teardown": {"duration": 0.0003608502447605133, "outcome": "passed"}}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_estimate_roi", "lineno": 389, "outcome": "passed", "keywords": ["test_estimate_roi", "TestPrioritizationFramework", "test_framework.py", "prioritization", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.00194656103849411, "outcome": "passed"}, "call": {"duration": 0.0005127671174705029, "outcome": "passed"}, "teardown": {"duration": 0.00034606456756591797, "outcome": "passed"}}, {"nodeid": "tests/product_manager/prioritization/test_framework.py::TestPrioritizationFramework::test_performance_with_large_dataset", "lineno": 439, "outcome": "passed", "keywords": ["test_performance_with_large_dataset", "TestPrioritizationFramework", "test_framework.py", "prioritization", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0011403090320527554, "outcome": "passed"}, "call": {"duration": 0.02064481284469366, "outcome": "passed"}, "teardown": {"duration": 0.0003053909167647362, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_initialization", "lineno": 14, "outcome": "passed", "keywords": ["test_initialization", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0014114347286522388, "outcome": "passed"}, "call": {"duration": 0.0003038272261619568, "outcome": "passed"}, "teardown": {"duration": 0.00029787933453917503, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_add_stakeholder", "lineno": 27, "outcome": "passed", "keywords": ["test_add_stakeholder", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013178871013224125, "outcome": "passed"}, "call": {"duration": 0.0005322517827153206, "outcome": "passed"}, "teardown": {"duration": 0.0003052428364753723, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_add_perspective", "lineno": 52, "outcome": "passed", "keywords": ["test_add_perspective", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0015738559886813164, "outcome": "passed"}, "call": {"duration": 0.0008683288469910622, "outcome": "passed"}, "teardown": {"duration": 0.000354887917637825, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_add_relationship", "lineno": 85, "outcome": "error", "keywords": ["test_add_relationship", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0014513800852000713, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/productmind/models.py", "lineno": 327, "message": "pydantic_core._pydantic_core.ValidationError: 2 validation errors for StakeholderRelationship\nstakeholder1_id\n  Field required [type=missing, input_value={'source_id': UUID('d048c...n technical priorities'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nstakeholder2_id\n  Field required [type=missing, input_value={'source_id': UUID('d048c...n technical priorities'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/product_manager/fixtures/test_data.py", "lineno": 1060, "message": ""}, {"path": "productmind/models.py", "lineno": 327, "message": "ValidationError"}], "longrepr": "stakeholder_samples = [Stakeholder(id=UUID('d048c830-f511-4993-a3ac-4e8b83b96090'), created_at=datetime.datetime(2025, 6, 16, 4, 15, 7, 5760...uence_level=0.7, perspectives=[], interests=['Customer acquisition', 'Competitive features', 'Sales enablement']), ...]\n\n    @pytest.fixture\n    def stakeholder_relationship_samples(stakeholder_samples):\n        \"\"\"Generate sample stakeholder relationships.\"\"\"\n        stakeholders = stakeholder_samples\n    \n        return [\n>           StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[0].id,  # CTO\n                stakeholder2_id=stakeholders[1].id,  # VP of Product\n                relationship_type=\"Peer\",\n                alignment_level=0.7,\n                notes=\"Good working relationship with some disagreements on technical priorities\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[1].id,  # VP of Product\n                stakeholder2_id=stakeholders[2].id,  # Product Manager\n                relationship_type=\"Manager-Report\",\n                alignment_level=0.9,\n                notes=\"Strong alignment on product direction\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[0].id,  # CTO\n                stakeholder2_id=stakeholders[3].id,  # Engineering Manager\n                relationship_type=\"Manager-Report\",\n                alignment_level=0.8,\n                notes=\"Well-aligned on technical direction\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[2].id,  # Product Manager\n                stakeholder2_id=stakeholders[4].id,  # UX Designer\n                relationship_type=\"Cross-functional\",\n                alignment_level=0.7,\n                notes=\"Generally aligned but some tension on design priorities\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[1].id,  # VP of Product\n                stakeholder2_id=stakeholders[5].id,  # Director of Sales\n                relationship_type=\"Cross-functional\",\n                alignment_level=0.6,\n                notes=\"Some disagreement on feature priorities vs. sales needs\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[2].id,  # Product Manager\n                stakeholder2_id=stakeholders[6].id,  # Customer Success Manager\n                relationship_type=\"Cross-functional\",\n                alignment_level=0.8,\n                notes=\"Strong collaboration on customer needs\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[5].id,  # Director of Sales\n                stakeholder2_id=stakeholders[7].id,  # Marketing Director\n                relationship_type=\"Peer\",\n                alignment_level=0.9,\n                notes=\"Very strong alignment on go-to-market strategy\"\n            )\n        ]\n\ntests/product_manager/fixtures/test_data.py:1060: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = StakeholderRelationship()\nstakeholder1_id = UUID('d048c830-f511-4993-a3ac-4e8b83b96090')\nstakeholder2_id = UUID('b9bf0288-03c4-4b60-ac9b-c4a491ecb1cf')\nrelationship_type = 'Peer'\nkwargs = {'alignment_level': 0.7, 'id': UUID('53bae13c-5954-491e-8d9f-aeba574a526c'), 'notes': 'Good working relationship with some disagreements on technical priorities'}\n\n    def __init__(self, stakeholder1_id: UUID, stakeholder2_id: UUID, relationship_type: str, **kwargs):\n>       super().__init__(\n            source_id=stakeholder1_id,\n            target_id=stakeholder2_id,\n            relationship_type=relationship_type,\n            **kwargs\n        )\nE       pydantic_core._pydantic_core.ValidationError: 2 validation errors for StakeholderRelationship\nE       stakeholder1_id\nE         Field required [type=missing, input_value={'source_id': UUID('d048c...n technical priorities'}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       stakeholder2_id\nE         Field required [type=missing, input_value={'source_id': UUID('d048c...n technical priorities'}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\nproductmind/models.py:327: ValidationError"}, "teardown": {"duration": 0.00038726674392819405, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_stakeholder", "lineno": 113, "outcome": "passed", "keywords": ["test_get_stakeholder", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013665510341525078, "outcome": "passed"}, "call": {"duration": 0.0007383278571069241, "outcome": "passed"}, "teardown": {"duration": 0.0003094766288995743, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_perspective", "lineno": 130, "outcome": "passed", "keywords": ["test_get_perspective", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0015251669101417065, "outcome": "passed"}, "call": {"duration": 0.0011157519184052944, "outcome": "passed"}, "teardown": {"duration": 0.0003488520160317421, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_relationship", "lineno": 148, "outcome": "error", "keywords": ["test_get_relationship", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0014256457798182964, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/productmind/models.py", "lineno": 327, "message": "pydantic_core._pydantic_core.ValidationError: 2 validation errors for StakeholderRelationship\nstakeholder1_id\n  Field required [type=missing, input_value={'source_id': UUID('cdf80...n technical priorities'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nstakeholder2_id\n  Field required [type=missing, input_value={'source_id': UUID('cdf80...n technical priorities'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/product_manager/fixtures/test_data.py", "lineno": 1060, "message": ""}, {"path": "productmind/models.py", "lineno": 327, "message": "ValidationError"}], "longrepr": "stakeholder_samples = [Stakeholder(id=UUID('cdf80347-c28b-490c-aeef-ee76e25779f0'), created_at=datetime.datetime(2025, 6, 16, 4, 15, 7, 6192...uence_level=0.7, perspectives=[], interests=['Customer acquisition', 'Competitive features', 'Sales enablement']), ...]\n\n    @pytest.fixture\n    def stakeholder_relationship_samples(stakeholder_samples):\n        \"\"\"Generate sample stakeholder relationships.\"\"\"\n        stakeholders = stakeholder_samples\n    \n        return [\n>           StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[0].id,  # CTO\n                stakeholder2_id=stakeholders[1].id,  # VP of Product\n                relationship_type=\"Peer\",\n                alignment_level=0.7,\n                notes=\"Good working relationship with some disagreements on technical priorities\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[1].id,  # VP of Product\n                stakeholder2_id=stakeholders[2].id,  # Product Manager\n                relationship_type=\"Manager-Report\",\n                alignment_level=0.9,\n                notes=\"Strong alignment on product direction\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[0].id,  # CTO\n                stakeholder2_id=stakeholders[3].id,  # Engineering Manager\n                relationship_type=\"Manager-Report\",\n                alignment_level=0.8,\n                notes=\"Well-aligned on technical direction\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[2].id,  # Product Manager\n                stakeholder2_id=stakeholders[4].id,  # UX Designer\n                relationship_type=\"Cross-functional\",\n                alignment_level=0.7,\n                notes=\"Generally aligned but some tension on design priorities\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[1].id,  # VP of Product\n                stakeholder2_id=stakeholders[5].id,  # Director of Sales\n                relationship_type=\"Cross-functional\",\n                alignment_level=0.6,\n                notes=\"Some disagreement on feature priorities vs. sales needs\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[2].id,  # Product Manager\n                stakeholder2_id=stakeholders[6].id,  # Customer Success Manager\n                relationship_type=\"Cross-functional\",\n                alignment_level=0.8,\n                notes=\"Strong collaboration on customer needs\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[5].id,  # Director of Sales\n                stakeholder2_id=stakeholders[7].id,  # Marketing Director\n                relationship_type=\"Peer\",\n                alignment_level=0.9,\n                notes=\"Very strong alignment on go-to-market strategy\"\n            )\n        ]\n\ntests/product_manager/fixtures/test_data.py:1060: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = StakeholderRelationship()\nstakeholder1_id = UUID('cdf80347-c28b-490c-aeef-ee76e25779f0')\nstakeholder2_id = UUID('892593bd-d252-4698-b029-d5d27bd57e3d')\nrelationship_type = 'Peer'\nkwargs = {'alignment_level': 0.7, 'id': UUID('8f16050f-d8f2-4651-a82b-0fd68daa2451'), 'notes': 'Good working relationship with some disagreements on technical priorities'}\n\n    def __init__(self, stakeholder1_id: UUID, stakeholder2_id: UUID, relationship_type: str, **kwargs):\n>       super().__init__(\n            source_id=stakeholder1_id,\n            target_id=stakeholder2_id,\n            relationship_type=relationship_type,\n            **kwargs\n        )\nE       pydantic_core._pydantic_core.ValidationError: 2 validation errors for StakeholderRelationship\nE       stakeholder1_id\nE         Field required [type=missing, input_value={'source_id': UUID('cdf80...n technical priorities'}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       stakeholder2_id\nE         Field required [type=missing, input_value={'source_id': UUID('cdf80...n technical priorities'}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\nproductmind/models.py:327: ValidationError"}, "teardown": {"duration": 0.00039068516343832016, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_all_stakeholders", "lineno": 166, "outcome": "passed", "keywords": ["test_get_all_stakeholders", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.001336943358182907, "outcome": "passed"}, "call": {"duration": 0.0007486781105399132, "outcome": "passed"}, "teardown": {"duration": 0.00031540077179670334, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_all_perspectives", "lineno": 181, "outcome": "passed", "keywords": ["test_get_all_perspectives", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0015051779337227345, "outcome": "passed"}, "call": {"duration": 0.0011284677311778069, "outcome": "passed"}, "teardown": {"duration": 0.0003496059216558933, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_all_relationships", "lineno": 197, "outcome": "error", "keywords": ["test_get_all_relationships", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0014368072152137756, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/productmind/models.py", "lineno": 327, "message": "pydantic_core._pydantic_core.ValidationError: 2 validation errors for StakeholderRelationship\nstakeholder1_id\n  Field required [type=missing, input_value={'source_id': UUID('2e461...n technical priorities'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nstakeholder2_id\n  Field required [type=missing, input_value={'source_id': UUID('2e461...n technical priorities'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/product_manager/fixtures/test_data.py", "lineno": 1060, "message": ""}, {"path": "productmind/models.py", "lineno": 327, "message": "ValidationError"}], "longrepr": "stakeholder_samples = [Stakeholder(id=UUID('2e461ee9-3711-4a2f-a060-c118052f90c6'), created_at=datetime.datetime(2025, 6, 16, 4, 15, 7, 6610...uence_level=0.7, perspectives=[], interests=['Customer acquisition', 'Competitive features', 'Sales enablement']), ...]\n\n    @pytest.fixture\n    def stakeholder_relationship_samples(stakeholder_samples):\n        \"\"\"Generate sample stakeholder relationships.\"\"\"\n        stakeholders = stakeholder_samples\n    \n        return [\n>           StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[0].id,  # CTO\n                stakeholder2_id=stakeholders[1].id,  # VP of Product\n                relationship_type=\"Peer\",\n                alignment_level=0.7,\n                notes=\"Good working relationship with some disagreements on technical priorities\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[1].id,  # VP of Product\n                stakeholder2_id=stakeholders[2].id,  # Product Manager\n                relationship_type=\"Manager-Report\",\n                alignment_level=0.9,\n                notes=\"Strong alignment on product direction\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[0].id,  # CTO\n                stakeholder2_id=stakeholders[3].id,  # Engineering Manager\n                relationship_type=\"Manager-Report\",\n                alignment_level=0.8,\n                notes=\"Well-aligned on technical direction\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[2].id,  # Product Manager\n                stakeholder2_id=stakeholders[4].id,  # UX Designer\n                relationship_type=\"Cross-functional\",\n                alignment_level=0.7,\n                notes=\"Generally aligned but some tension on design priorities\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[1].id,  # VP of Product\n                stakeholder2_id=stakeholders[5].id,  # Director of Sales\n                relationship_type=\"Cross-functional\",\n                alignment_level=0.6,\n                notes=\"Some disagreement on feature priorities vs. sales needs\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[2].id,  # Product Manager\n                stakeholder2_id=stakeholders[6].id,  # Customer Success Manager\n                relationship_type=\"Cross-functional\",\n                alignment_level=0.8,\n                notes=\"Strong collaboration on customer needs\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[5].id,  # Director of Sales\n                stakeholder2_id=stakeholders[7].id,  # Marketing Director\n                relationship_type=\"Peer\",\n                alignment_level=0.9,\n                notes=\"Very strong alignment on go-to-market strategy\"\n            )\n        ]\n\ntests/product_manager/fixtures/test_data.py:1060: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = StakeholderRelationship()\nstakeholder1_id = UUID('2e461ee9-3711-4a2f-a060-c118052f90c6')\nstakeholder2_id = UUID('134545be-7452-435d-8a7c-21f931785b1b')\nrelationship_type = 'Peer'\nkwargs = {'alignment_level': 0.7, 'id': UUID('5a3bd06f-8603-48cd-98df-1e72599f222c'), 'notes': 'Good working relationship with some disagreements on technical priorities'}\n\n    def __init__(self, stakeholder1_id: UUID, stakeholder2_id: UUID, relationship_type: str, **kwargs):\n>       super().__init__(\n            source_id=stakeholder1_id,\n            target_id=stakeholder2_id,\n            relationship_type=relationship_type,\n            **kwargs\n        )\nE       pydantic_core._pydantic_core.ValidationError: 2 validation errors for StakeholderRelationship\nE       stakeholder1_id\nE         Field required [type=missing, input_value={'source_id': UUID('2e461...n technical priorities'}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       stakeholder2_id\nE         Field required [type=missing, input_value={'source_id': UUID('2e461...n technical priorities'}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\nproductmind/models.py:327: ValidationError"}, "teardown": {"duration": 0.0004610177129507065, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_stakeholder_perspectives", "lineno": 213, "outcome": "passed", "keywords": ["test_get_stakeholder_perspectives", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0016859187744557858, "outcome": "passed"}, "call": {"duration": 0.00123271020129323, "outcome": "passed"}, "teardown": {"duration": 0.0003543929196894169, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_perspectives_by_topic", "lineno": 251, "outcome": "passed", "keywords": ["test_get_perspectives_by_topic", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0015333620831370354, "outcome": "passed"}, "call": {"duration": 0.0012658480554819107, "outcome": "passed"}, "teardown": {"duration": 0.00034140562638640404, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_get_stakeholder_relationships", "lineno": 282, "outcome": "error", "keywords": ["test_get_stakeholder_relationships", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.001439073123037815, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/productmind/models.py", "lineno": 327, "message": "pydantic_core._pydantic_core.ValidationError: 2 validation errors for StakeholderRelationship\nstakeholder1_id\n  Field required [type=missing, input_value={'source_id': UUID('6a1de...n technical priorities'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nstakeholder2_id\n  Field required [type=missing, input_value={'source_id': UUID('6a1de...n technical priorities'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/product_manager/fixtures/test_data.py", "lineno": 1060, "message": ""}, {"path": "productmind/models.py", "lineno": 327, "message": "ValidationError"}], "longrepr": "stakeholder_samples = [Stakeholder(id=UUID('6a1de386-2a36-4a89-a488-ba0ae5c60491'), created_at=datetime.datetime(2025, 6, 16, 4, 15, 7, 7573...uence_level=0.7, perspectives=[], interests=['Customer acquisition', 'Competitive features', 'Sales enablement']), ...]\n\n    @pytest.fixture\n    def stakeholder_relationship_samples(stakeholder_samples):\n        \"\"\"Generate sample stakeholder relationships.\"\"\"\n        stakeholders = stakeholder_samples\n    \n        return [\n>           StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[0].id,  # CTO\n                stakeholder2_id=stakeholders[1].id,  # VP of Product\n                relationship_type=\"Peer\",\n                alignment_level=0.7,\n                notes=\"Good working relationship with some disagreements on technical priorities\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[1].id,  # VP of Product\n                stakeholder2_id=stakeholders[2].id,  # Product Manager\n                relationship_type=\"Manager-Report\",\n                alignment_level=0.9,\n                notes=\"Strong alignment on product direction\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[0].id,  # CTO\n                stakeholder2_id=stakeholders[3].id,  # Engineering Manager\n                relationship_type=\"Manager-Report\",\n                alignment_level=0.8,\n                notes=\"Well-aligned on technical direction\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[2].id,  # Product Manager\n                stakeholder2_id=stakeholders[4].id,  # UX Designer\n                relationship_type=\"Cross-functional\",\n                alignment_level=0.7,\n                notes=\"Generally aligned but some tension on design priorities\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[1].id,  # VP of Product\n                stakeholder2_id=stakeholders[5].id,  # Director of Sales\n                relationship_type=\"Cross-functional\",\n                alignment_level=0.6,\n                notes=\"Some disagreement on feature priorities vs. sales needs\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[2].id,  # Product Manager\n                stakeholder2_id=stakeholders[6].id,  # Customer Success Manager\n                relationship_type=\"Cross-functional\",\n                alignment_level=0.8,\n                notes=\"Strong collaboration on customer needs\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[5].id,  # Director of Sales\n                stakeholder2_id=stakeholders[7].id,  # Marketing Director\n                relationship_type=\"Peer\",\n                alignment_level=0.9,\n                notes=\"Very strong alignment on go-to-market strategy\"\n            )\n        ]\n\ntests/product_manager/fixtures/test_data.py:1060: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = StakeholderRelationship()\nstakeholder1_id = UUID('6a1de386-2a36-4a89-a488-ba0ae5c60491')\nstakeholder2_id = UUID('1559ac15-49ab-41e3-a1ab-d7b96710008d')\nrelationship_type = 'Peer'\nkwargs = {'alignment_level': 0.7, 'id': UUID('049bbccb-23fe-47c3-a5d8-35641f883353'), 'notes': 'Good working relationship with some disagreements on technical priorities'}\n\n    def __init__(self, stakeholder1_id: UUID, stakeholder2_id: UUID, relationship_type: str, **kwargs):\n>       super().__init__(\n            source_id=stakeholder1_id,\n            target_id=stakeholder2_id,\n            relationship_type=relationship_type,\n            **kwargs\n        )\nE       pydantic_core._pydantic_core.ValidationError: 2 validation errors for StakeholderRelationship\nE       stakeholder1_id\nE         Field required [type=missing, input_value={'source_id': UUID('6a1de...n technical priorities'}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       stakeholder2_id\nE         Field required [type=missing, input_value={'source_id': UUID('6a1de...n technical priorities'}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\nproductmind/models.py:327: ValidationError"}, "teardown": {"duration": 0.0003874017857015133, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_detect_conflicts", "lineno": 326, "outcome": "passed", "keywords": ["test_detect_conflicts", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0015707658603787422, "outcome": "passed"}, "call": {"duration": 0.0016815070994198322, "outcome": "passed"}, "teardown": {"duration": 0.0003504818305373192, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_identify_consensus", "lineno": 388, "outcome": "passed", "keywords": ["test_identify_consensus", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.001524368766695261, "outcome": "passed"}, "call": {"duration": 0.0012930030934512615, "outcome": "passed"}, "teardown": {"duration": 0.0003743697889149189, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_integrate_perspectives", "lineno": 438, "outcome": "passed", "keywords": ["test_integrate_perspectives", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0015229820273816586, "outcome": "passed"}, "call": {"duration": 0.0015004947781562805, "outcome": "passed"}, "teardown": {"duration": 0.0003878711722791195, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_generate_stakeholder_map", "lineno": 491, "outcome": "error", "keywords": ["test_generate_stakeholder_map", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0014373348094522953, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/personal_knowledge_management/unified/productmind/models.py", "lineno": 327, "message": "pydantic_core._pydantic_core.ValidationError: 2 validation errors for StakeholderRelationship\nstakeholder1_id\n  Field required [type=missing, input_value={'source_id': UUID('c50f8...n technical priorities'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nstakeholder2_id\n  Field required [type=missing, input_value={'source_id': UUID('c50f8...n technical priorities'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/product_manager/fixtures/test_data.py", "lineno": 1060, "message": ""}, {"path": "productmind/models.py", "lineno": 327, "message": "ValidationError"}], "longrepr": "stakeholder_samples = [Stakeholder(id=UUID('c50f8c4d-2a6b-41b8-a353-fd403f765d6f'), created_at=datetime.datetime(2025, 6, 16, 4, 15, 7, 8048...uence_level=0.7, perspectives=[], interests=['Customer acquisition', 'Competitive features', 'Sales enablement']), ...]\n\n    @pytest.fixture\n    def stakeholder_relationship_samples(stakeholder_samples):\n        \"\"\"Generate sample stakeholder relationships.\"\"\"\n        stakeholders = stakeholder_samples\n    \n        return [\n>           StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[0].id,  # CTO\n                stakeholder2_id=stakeholders[1].id,  # VP of Product\n                relationship_type=\"Peer\",\n                alignment_level=0.7,\n                notes=\"Good working relationship with some disagreements on technical priorities\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[1].id,  # VP of Product\n                stakeholder2_id=stakeholders[2].id,  # Product Manager\n                relationship_type=\"Manager-Report\",\n                alignment_level=0.9,\n                notes=\"Strong alignment on product direction\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[0].id,  # CTO\n                stakeholder2_id=stakeholders[3].id,  # Engineering Manager\n                relationship_type=\"Manager-Report\",\n                alignment_level=0.8,\n                notes=\"Well-aligned on technical direction\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[2].id,  # Product Manager\n                stakeholder2_id=stakeholders[4].id,  # UX Designer\n                relationship_type=\"Cross-functional\",\n                alignment_level=0.7,\n                notes=\"Generally aligned but some tension on design priorities\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[1].id,  # VP of Product\n                stakeholder2_id=stakeholders[5].id,  # Director of Sales\n                relationship_type=\"Cross-functional\",\n                alignment_level=0.6,\n                notes=\"Some disagreement on feature priorities vs. sales needs\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[2].id,  # Product Manager\n                stakeholder2_id=stakeholders[6].id,  # Customer Success Manager\n                relationship_type=\"Cross-functional\",\n                alignment_level=0.8,\n                notes=\"Strong collaboration on customer needs\"\n            ),\n            StakeholderRelationship(\n                id=uuid4(),\n                stakeholder1_id=stakeholders[5].id,  # Director of Sales\n                stakeholder2_id=stakeholders[7].id,  # Marketing Director\n                relationship_type=\"Peer\",\n                alignment_level=0.9,\n                notes=\"Very strong alignment on go-to-market strategy\"\n            )\n        ]\n\ntests/product_manager/fixtures/test_data.py:1060: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = StakeholderRelationship()\nstakeholder1_id = UUID('c50f8c4d-2a6b-41b8-a353-fd403f765d6f')\nstakeholder2_id = UUID('14e7d927-6351-45c5-ae7f-6baacb7ef1dc')\nrelationship_type = 'Peer'\nkwargs = {'alignment_level': 0.7, 'id': UUID('2fb9e299-781c-4854-9c2b-8674549227ea'), 'notes': 'Good working relationship with some disagreements on technical priorities'}\n\n    def __init__(self, stakeholder1_id: UUID, stakeholder2_id: UUID, relationship_type: str, **kwargs):\n>       super().__init__(\n            source_id=stakeholder1_id,\n            target_id=stakeholder2_id,\n            relationship_type=relationship_type,\n            **kwargs\n        )\nE       pydantic_core._pydantic_core.ValidationError: 2 validation errors for StakeholderRelationship\nE       stakeholder1_id\nE         Field required [type=missing, input_value={'source_id': UUID('c50f8...n technical priorities'}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       stakeholder2_id\nE         Field required [type=missing, input_value={'source_id': UUID('c50f8...n technical priorities'}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\nproductmind/models.py:327: ValidationError"}, "teardown": {"duration": 0.0003940802998840809, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_analyze_stakeholder_influence", "lineno": 536, "outcome": "passed", "keywords": ["test_analyze_stakeholder_influence", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0013616019859910011, "outcome": "passed"}, "call": {"duration": 0.0007942169904708862, "outcome": "passed"}, "teardown": {"duration": 0.0003100750036537647, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_generate_stakeholder_matrix", "lineno": 565, "outcome": "passed", "keywords": ["test_generate_stakeholder_matrix", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0015278230421245098, "outcome": "passed"}, "call": {"duration": 0.0015850281342864037, "outcome": "passed"}, "teardown": {"duration": 0.0004019639454782009, "outcome": "passed"}}, {"nodeid": "tests/product_manager/stakeholder_insights/test_manager.py::TestStakeholderInsightManager::test_analyze_perspective_alignment", "lineno": 616, "outcome": "passed", "keywords": ["test_analyze_perspective_alignment", "TestStakeholderInsightManager", "test_manager.py", "stakeholder_insights", "product_manager", "tests", "unified", ""], "setup": {"duration": 0.0016125733964145184, "outcome": "passed"}, "call": {"duration": 0.0013692202046513557, "outcome": "passed"}, "teardown": {"duration": 0.0020677316933870316, "outcome": "passed"}}], "warnings": [{"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=<property object at 0x7fe5ffc8e4d0>, input_type=property])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 463}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=<property object at 0x7fe5ffc8e4d0>, input_type=property])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 463}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=<property object at 0x7fe5ffc8e4d0>, input_type=property])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 463}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=<property object at 0x7fe5ffc8e4d0>, input_type=property])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 463}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=<property object at 0x7fe5ffc8e4d0>, input_type=property])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 463}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=<property object at 0x7fe5ffc8e4d0>, input_type=property])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 463}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=<property object at 0x7fe5ffc8e4d0>, input_type=property])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 463}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=<property object at 0x7fe5ffc8e4d0>, input_type=property])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 463}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=<property object at 0x7fe5ffc8e4d0>, input_type=property])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 463}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=<property object at 0x7fe5ffc8e4d0>, input_type=property])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 463}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=<property object at 0x7fe5ffc8e4d0>, input_type=property])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 463}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=<property object at 0x7fe5ffc8e4d0>, input_type=property])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 463}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `datetime` - serialized value may not be as expected [input_value='2023-06-01', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 463}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `datetime` - serialized value may not be as expected [input_value='2023-08-15', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 463}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=<property object at 0x7fe5ffc8e4d0>, input_type=property])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 463}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='03687944-517f-4a10-8bf7-9811e02afb3f', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='52007cd0-22af-4feb-a3a2-26e3bd77bb53', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='0d6db156-bcfc-45b4-bb14-af5bffb41e6b', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='768b9a0a-444c-41e3-8935-8f75e018db20', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.", "category": "ConvergenceWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/sklearn/base.py", "lineno": 1389}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='40df47c3-1a93-4fa5-bcfb-385e15f08759', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='a364fd7c-0d8d-45ff-92c9-f999f0a9220e', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='e5c114e6-cb3c-4f3e-b878-e49da4a370d7', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='1784afd0-ce9c-4c44-9ca8-8a49b6f2577e', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='1cddb2f6-f17b-4c15-abd1-620aee831145', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='490d2c38-97b3-473b-a002-fd57f32238c9', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='02b09c58-567a-47fd-bc4c-7b111cd0a581', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='33e1d613-a3d9-4a51-bca9-a5e6f5e5fd76', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='0b907343-ce4d-42ce-a6f9-4fa659c79e6a', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='4f78a9a7-a72f-4b57-b382-16f5fa6a2f1e', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='6ba38aae-c670-49f4-b4d2-84662a8890c6', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='bf07a116-0921-47f4-b32a-6be07ff8ba53', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='5745c5c5-f6ba-4451-9157-7bf898d31e05', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='bb4d261a-68e6-4913-91d9-b433bb812e32', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='7044689b-30d0-4ae4-8bd0-3d02e02600e5', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='6ba38aae-c670-49f4-b4d2-84662a8890c6', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='6ba38aae-c670-49f4-b4d2-84662a8890c6', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='bf07a116-0921-47f4-b32a-6be07ff8ba53', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='6ba38aae-c670-49f4-b4d2-84662a8890c6', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='5745c5c5-f6ba-4451-9157-7bf898d31e05', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='6ba38aae-c670-49f4-b4d2-84662a8890c6', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='bf07a116-0921-47f4-b32a-6be07ff8ba53', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='bb4d261a-68e6-4913-91d9-b433bb812e32', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='6ba38aae-c670-49f4-b4d2-84662a8890c6', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='7044689b-30d0-4ae4-8bd0-3d02e02600e5', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='6ba38aae-c670-49f4-b4d2-84662a8890c6', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='bf07a116-0921-47f4-b32a-6be07ff8ba53', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='5745c5c5-f6ba-4451-9157-7bf898d31e05', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='6ba38aae-c670-49f4-b4d2-84662a8890c6', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='bf07a116-0921-47f4-b32a-6be07ff8ba53', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='7044689b-30d0-4ae4-8bd0-3d02e02600e5', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='6ba38aae-c670-49f4-b4d2-84662a8890c6', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='bf07a116-0921-47f4-b32a-6be07ff8ba53', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='5745c5c5-f6ba-4451-9157-7bf898d31e05', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='bb4d261a-68e6-4913-91d9-b433bb812e32', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='6ba38aae-c670-49f4-b4d2-84662a8890c6', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='5745c5c5-f6ba-4451-9157-7bf898d31e05', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='7044689b-30d0-4ae4-8bd0-3d02e02600e5', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='6ba38aae-c670-49f4-b4d2-84662a8890c6', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='bf07a116-0921-47f4-b32a-6be07ff8ba53', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='bb4d261a-68e6-4913-91d9-b433bb812e32', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='7044689b-30d0-4ae4-8bd0-3d02e02600e5', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}, {"message": "Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='6ba38aae-c670-49f4-b4d2-84662a8890c6', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='bf07a116-0921-47f4-b32a-6be07ff8ba53', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='5745c5c5-f6ba-4451-9157-7bf898d31e05', input_type=str])\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='7044689b-30d0-4ae4-8bd0-3d02e02600e5', input_type=str])", "category": "UserWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 519}]}