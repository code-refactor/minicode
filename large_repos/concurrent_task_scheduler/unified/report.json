{"created": 1750047426.8230195, "duration": 24.289315462112427, "exitcode": 1, "root": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified", "environment": {}, "summary": {"passed": 283, "failed": 67, "error": 3, "xfailed": 4, "xpassed": 1, "total": 358, "collected": 358}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests", "type": "Package"}]}, {"nodeid": "tests/render_farm_manager/integration/test_audit_logging.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_audit_logging.py::test_audit_logging_completeness", "type": "Function", "lineno": 118}, {"nodeid": "tests/render_farm_manager/integration/test_audit_logging.py::test_audit_log_levels", "type": "Function", "lineno": 274}, {"nodeid": "tests/render_farm_manager/integration/test_audit_logging.py::test_performance_metrics_tracking", "type": "Function", "lineno": 342}]}, {"nodeid": "tests/render_farm_manager/integration/test_circular_dependency_patch.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_circular_dependency_patch.py::test_circular_dependency_detection_patched", "type": "Function", "lineno": 16}]}, {"nodeid": "tests/render_farm_manager/integration/test_energy_modes.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_energy_modes.py::test_dynamic_energy_mode_switching", "type": "Function", "lineno": 197}, {"nodeid": "tests/render_farm_manager/integration/test_energy_modes.py::test_night_savings_energy_mode", "type": "Function", "lineno": 331}]}, {"nodeid": "tests/render_farm_manager/integration/test_energy_modes_fixed.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_energy_modes_fixed.py::test_energy_mode_changes", "type": "Function", "lineno": 69}]}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_error_recovery.py::test_error_recovery_checkpoint_resume", "type": "Function", "lineno": 139}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery.py::test_multiple_failures_with_checkpoints", "type": "Function", "lineno": 241}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery.py::test_error_count_threshold", "type": "Function", "lineno": 365}]}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed.py::test_error_recovery_checkpoint_simple", "type": "Function", "lineno": 116}]}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed_complete.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed_complete.py::test_error_recovery_with_checkpoint", "type": "Function", "lineno": 16}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed_complete.py::test_error_count_threshold_handling", "type": "Function", "lineno": 151}]}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed_full.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed_full.py::test_error_recovery_checkpoint_resume", "type": "Function", "lineno": 156}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed_full.py::test_multiple_failures_with_checkpoints", "type": "Function", "lineno": 263}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed_full.py::test_error_count_threshold", "type": "Function", "lineno": 389}]}, {"nodeid": "tests/render_farm_manager/integration/test_fault_tolerance.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_fault_tolerance.py::test_fault_tolerance_multiple_node_failures", "type": "Function", "lineno": 266}]}, {"nodeid": "tests/render_farm_manager/integration/test_fault_tolerance_fixed.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_fault_tolerance_fixed.py::test_fault_tolerance_multiple_node_failures", "type": "Function", "lineno": 258}]}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_job_dependencies.py::test_job_dependency_scheduling", "type": "Function", "lineno": 211}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies.py::test_dependent_job_priority_inheritance", "type": "Function", "lineno": 302}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies.py::test_circular_dependency_detection", "type": "Function", "lineno": 423}]}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed.py::test_simple_dependency", "type": "Function", "lineno": 71}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed.py::test_circular_dependency_detection", "type": "Function", "lineno": 181}]}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py::test_job_dependency_scheduling", "type": "Function", "lineno": 53}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py::test_dependent_job_priority_inheritance", "type": "Function", "lineno": 167}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py::test_circular_dependency_detection", "type": "Function", "lineno": 303}]}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct_complete.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct_complete.py::test_simple_dependency", "type": "Function", "lineno": 16}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct_complete.py::test_circular_dependency_detection", "type": "Function", "lineno": 123}]}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py::test_job_dependency_scheduling", "type": "Function", "lineno": 213}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py::test_dependent_job_priority_inheritance", "type": "Function", "lineno": 294}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py::test_circular_dependency_detection", "type": "Function", "lineno": 400}]}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_scheduling_patch.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_scheduling_patch.py::test_job_dependency_scheduling_patched", "type": "Function", "lineno": 16}]}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple.py::test_simple_dependency", "type": "Function", "lineno": 16}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple.py::test_circular_dependency_detection", "type": "Function", "lineno": 205}]}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple_fixed.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple_fixed.py::test_simple_dependency", "type": "Function", "lineno": 16}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple_fixed.py::test_circular_dependency_detection", "type": "Function", "lineno": 126}]}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple_patched.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple_patched.py::test_simple_dependency", "type": "Function", "lineno": 17}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple_patched.py::test_circular_dependency_detection", "type": "Function", "lineno": 177}]}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch.py::test_simple_dependency_with_monkey_patch", "type": "Function", "lineno": 17}]}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch_all.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch_all.py::test_circular_dependency_detection", "type": "Function", "lineno": 20}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch_all.py::test_job_dependency_scheduling", "type": "Function", "lineno": 145}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch_all.py::test_dependent_job_priority_inheritance", "type": "Function", "lineno": 522}]}, {"nodeid": "tests/render_farm_manager/integration/test_priority_inheritance_patch.py::TestPriorityInheritance", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_priority_inheritance_patch.py::TestPriorityInheritance::test_dependent_job_priority_inheritance_patched", "type": "Function", "lineno": 20}]}, {"nodeid": "tests/render_farm_manager/integration/test_priority_inheritance_patch.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_priority_inheritance_patch.py::TestPriorityInheritance", "type": "Class"}]}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_farm_manager_initialization", "type": "Function", "lineno": 212}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_add_client", "type": "Function", "lineno": 220}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_add_node", "type": "Function", "lineno": 231}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_submit_job", "type": "Function", "lineno": 241}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_scheduling_cycle", "type": "Function", "lineno": 257}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_job_progress_update", "type": "Function", "lineno": 290}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_node_failure", "type": "Function", "lineno": 328}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_cancel_job", "type": "Function", "lineno": 369}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_client_resource_guarantees", "type": "Function", "lineno": 402}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_energy_optimization", "type": "Function", "lineno": 488}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_progressive_output_config", "type": "Function", "lineno": 521}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_full_end_to_end_workflow", "type": "Function", "lineno": 557}]}, {"nodeid": "tests/render_farm_manager/integration", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration/test_audit_logging.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_circular_dependency_patch.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_energy_modes.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_energy_modes_fixed.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed_complete.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed_full.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_fault_tolerance.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_fault_tolerance_fixed.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct_complete.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_scheduling_patch.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple_fixed.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple_patched.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch_all.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_priority_inheritance_patch.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py", "type": "Module"}]}, {"nodeid": "tests/render_farm_manager/performance/test_performance.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/performance/test_performance.py::test_scheduling_performance", "type": "Function", "lineno": 160}, {"nodeid": "tests/render_farm_manager/performance/test_performance.py::test_multiple_scheduling_cycles", "type": "Function", "lineno": 195}, {"nodeid": "tests/render_farm_manager/performance/test_performance.py::test_node_specialization_efficiency", "type": "Function", "lineno": 251}]}, {"nodeid": "tests/render_farm_manager/performance", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/performance/test_performance.py", "type": "Module"}]}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_scheduler_initialization", "type": "Function", "lineno": 184}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_update_priorities_deadline_approaching", "type": "Function", "lineno": 190}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_update_priorities_job_progress", "type": "Function", "lineno": 211}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_schedule_jobs_priority_order", "type": "Function", "lineno": 228}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_schedule_jobs_resource_requirements", "type": "Function", "lineno": 246}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_preemption", "type": "Function", "lineno": 258}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_preemption_disabled", "type": "Function", "lineno": 285}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_can_meet_deadline", "type": "Function", "lineno": 305}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_should_preempt", "type": "Function", "lineno": 337}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_schedule_with_dependencies", "type": "Function", "lineno": 360}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_rescheduling_failed_job", "type": "Function", "lineno": 451}]}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler_fixed.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler_fixed.py::test_schedule_with_dependencies_fixed", "type": "Function", "lineno": 65}]}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler_full.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler_full.py::test_schedule_with_dependencies_fixed", "type": "Function", "lineno": 73}]}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_energy_optimizer_initialization", "type": "Function", "lineno": 190}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_optimize_energy_usage", "type": "Function", "lineno": 199}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_energy_mode_affects_scheduling", "type": "Function", "lineno": 227}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_calculate_energy_cost", "type": "Function", "lineno": 252}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_time_of_day_energy_price", "type": "Function", "lineno": 282}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_set_energy_mode", "type": "Function", "lineno": 297}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_estimate_energy_savings", "type": "Function", "lineno": 313}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_energy_mode_update_based_on_time", "type": "Function", "lineno": 331}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_node_meets_requirements", "type": "Function", "lineno": 350}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_get_node_type", "type": "Function", "lineno": 368}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_high_priority_jobs_override_energy_considerations", "type": "Function", "lineno": 383}]}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_specialization_manager_initialization", "type": "Function", "lineno": 249}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_match_job_to_node_gpu_job", "type": "Function", "lineno": 258}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_match_job_to_node_cpu_job", "type": "Function", "lineno": 274}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_match_job_to_node_memory_job", "type": "Function", "lineno": 290}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_calculate_performance_score", "type": "Function", "lineno": 306}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_update_performance_history", "type": "Function", "lineno": 327}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_performance_history_influence", "type": "Function", "lineno": 361}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_node_capability_matching", "type": "Function", "lineno": 376}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_no_suitable_node", "type": "Function", "lineno": 395}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_specialized_vs_general_nodes", "type": "Function", "lineno": 409}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_analyze_node_efficiency", "type": "Function", "lineno": 444}]}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_progressive_renderer_initialization", "type": "Function", "lineno": 167}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_schedule_progressive_output_long_job", "type": "Function", "lineno": 175}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_schedule_progressive_output_short_job", "type": "Function", "lineno": 192}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_schedule_progressive_output_disabled_config", "type": "Function", "lineno": 207}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_schedule_progressive_output_unsupported_job", "type": "Function", "lineno": 226}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_generate_progressive_output", "type": "Function", "lineno": 237}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_generate_progressive_output_unsupported_job", "type": "Function", "lineno": 258}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_estimate_overhead", "type": "Function", "lineno": 267}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_process_pending_outputs", "type": "Function", "lineno": 292}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_get_latest_progressive_output", "type": "Function", "lineno": 338}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_quality_overhead_factors", "type": "Function", "lineno": 355}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_max_overhead_limit", "type": "Function", "lineno": 362}]}, {"nodeid": "tests/render_farm_manager/unit/test_resource_borrowing.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/unit/test_resource_borrowing.py::test_client_resource_borrowing", "type": "Function", "lineno": 174}, {"nodeid": "tests/render_farm_manager/unit/test_resource_borrowing.py::test_borrowing_limit_variations", "type": "Function", "lineno": 308}]}, {"nodeid": "tests/render_farm_manager/unit/test_resource_borrowing_fixed.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/unit/test_resource_borrowing_fixed.py::test_client_resource_borrowing", "type": "Function", "lineno": 175}, {"nodeid": "tests/render_farm_manager/unit/test_resource_borrowing_fixed.py::test_borrowing_limit_variations", "type": "Function", "lineno": 313}]}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_partitioner_initialization", "type": "Function", "lineno": 171}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_allocate_resources_guaranteed_minimums", "type": "Function", "lineno": 177}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_allocate_resources_borrowing", "type": "Function", "lineno": 197}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_can_borrow_resources", "type": "Function", "lineno": 218}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_calculate_resource_usage", "type": "Function", "lineno": 249}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_allocate_resources_with_offline_nodes", "type": "Function", "lineno": 273}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_resource_allocation_scaling", "type": "Function", "lineno": 295}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_resource_allocation_special_hardware", "type": "Function", "lineno": 313}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_allocate_resources_with_no_clients", "type": "Function", "lineno": 334}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_allocate_resources_with_no_nodes", "type": "Function", "lineno": 342}]}, {"nodeid": "tests/render_farm_manager/unit", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler_fixed.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler_full.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/unit/test_resource_borrowing.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/unit/test_resource_borrowing_fixed.py", "type": "Module"}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py", "type": "Module"}]}, {"nodeid": "tests/render_farm_manager", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager/integration", "type": "Package"}, {"nodeid": "tests/render_farm_manager/performance", "type": "Package"}, {"nodeid": "tests/render_farm_manager/unit", "type": "Package"}]}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_add_stage", "type": "Function", "lineno": 54}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_add_dependency", "type": "Function", "lineno": 59}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_add_dependency_with_cycle", "type": "Function", "lineno": 73}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_remove_dependency", "type": "Function", "lineno": 79}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_get_dependencies", "type": "Function", "lineno": 85}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_get_root_and_leaf_stages", "type": "Function", "lineno": 94}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_get_ready_stages", "type": "Function", "lineno": 103}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_are_all_dependencies_satisfied", "type": "Function", "lineno": 124}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_update_stage_status", "type": "Function", "lineno": 136}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_get_critical_path", "type": "Function", "lineno": 146}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_is_stage_blocked", "type": "Function", "lineno": 157}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_validate", "type": "Function", "lineno": 167}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_serialize_deserialize", "type": "Function", "lineno": 184}]}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_create_dependency_graph", "type": "Function", "lineno": 267}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_add_dependency", "type": "Function", "lineno": 285}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_remove_dependency", "type": "Function", "lineno": 308}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_add_transition_rule", "type": "Function", "lineno": 326}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_update_stage_status", "type": "Function", "lineno": 350}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_get_ready_stages", "type": "Function", "lineno": 374}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_is_stage_ready", "type": "Function", "lineno": 392}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_get_blocking_stages", "type": "Function", "lineno": 409}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_get_critical_path", "type": "Function", "lineno": 425}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_bypass_dependency", "type": "Function", "lineno": 439}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_add_dynamic_dependency", "type": "Function", "lineno": 464}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_get_execution_plan", "type": "Function", "lineno": 490}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_validate_simulation", "type": "Function", "lineno": 506}]}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_register_template", "type": "Function", "lineno": 540}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_create_instance", "type": "Function", "lineno": 555}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_create_linear_workflow", "type": "Function", "lineno": 595}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_create_parallel_workflow", "type": "Function", "lineno": 636}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_get_next_stages", "type": "Function", "lineno": 673}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_update_instance", "type": "Function", "lineno": 726}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_get_workflow_status", "type": "Function", "lineno": 781}]}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph", "type": "Class"}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker", "type": "Class"}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager", "type": "Class"}]}, {"nodeid": "tests/scientific_computing/dependency_tracking", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py", "type": "Module"}]}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_checkpoint_manager_init", "type": "Function", "lineno": 105}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_register_policy", "type": "Function", "lineno": 115}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_get_default_policy", "type": "Function", "lineno": 133}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_create_manager_for_simulation", "type": "Function", "lineno": 148}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_update_policy", "type": "Function", "lineno": 182}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_should_create_checkpoint", "type": "Function", "lineno": 217}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_create_checkpoint", "type": "Function", "lineno": 244}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_validate_checkpoint", "type": "Function", "lineno": 284}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_get_latest_checkpoint", "type": "Function", "lineno": 330}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_get_all_checkpoints", "type": "Function", "lineno": 360}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_restore_from_checkpoint", "type": "Function", "lineno": 383}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_checkpoint_coordinator", "type": "Function", "lineno": 418}]}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_failure_detector_init", "type": "Function", "lineno": 110}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_record_heartbeat", "type": "Function", "lineno": 121}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_check_node_health", "type": "Function", "lineno": 130}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_check_simulation_health", "type": "Function", "lineno": 163}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_update_simulation_progress", "type": "Function", "lineno": 185}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_update_simulation_status", "type": "Function", "lineno": 197}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_detect_node_failures", "type": "Function", "lineno": 209}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_detect_simulation_failures", "type": "Function", "lineno": 224}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_report_failure", "type": "Function", "lineno": 246}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_get_active_failures", "type": "Function", "lineno": 283}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_resolve_failure", "type": "Function", "lineno": 329}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_reliability_metrics", "type": "Function", "lineno": 358}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_failure_type_determination", "type": "Function", "lineno": 386}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_recovery_manager_init", "type": "Function", "lineno": 399}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_get_recovery_strategy", "type": "Function", "lineno": 408}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_initiate_recovery", "type": "Function", "lineno": 445}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_complete_recovery", "type": "Function", "lineno": 491}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_get_active_recoveries", "type": "Function", "lineno": 528}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_cancel_recovery", "type": "Function", "lineno": 567}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_process_failures", "type": "Function", "lineno": 602}]}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_record_heartbeat", "type": "Function", "lineno": 108}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_check_node_health", "type": "Function", "lineno": 115}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_check_simulation_health", "type": "Function", "lineno": 180}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_update_simulation_progress", "type": "Function", "lineno": 209}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_detect_node_failures", "type": "Function", "lineno": 221}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_detect_simulation_failures", "type": "Function", "lineno": 255}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_report_failure", "type": "Function", "lineno": 299}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_get_active_failures", "type": "Function", "lineno": 319}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_resolve_failure", "type": "Function", "lineno": 361}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_get_node_reliability_score", "type": "Function", "lineno": 387}]}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_get_recovery_strategy", "type": "Function", "lineno": 421}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_initiate_recovery", "type": "Function", "lineno": 451}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_complete_recovery", "type": "Function", "lineno": 483}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_cancel_recovery", "type": "Function", "lineno": 528}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_process_failures", "type": "Function", "lineno": 558}]}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_set_resilience_level", "type": "Function", "lineno": 681}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_record_event", "type": "Function", "lineno": 699}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_schedule_checkpoint", "type": "Function", "lineno": 721}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_handle_node_status_change", "type": "Function", "lineno": 741}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_handle_simulation_status_change", "type": "Function", "lineno": 772}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_handle_stage_status_change", "type": "Function", "lineno": 811}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_handle_failure_detection", "type": "Function", "lineno": 843}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_complete_recovery", "type": "Function", "lineno": 873}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_detect_and_handle_failures", "type": "Function", "lineno": 910}]}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector", "type": "Class"}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager", "type": "Class"}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator", "type": "Class"}]}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_resilience_coordinator_init", "type": "Function", "lineno": 152}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_set_resilience_level", "type": "Function", "lineno": 163}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_record_event", "type": "Function", "lineno": 181}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_schedule_checkpoint", "type": "Function", "lineno": 203}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_process_scheduled_checkpoints", "type": "Function", "lineno": 219}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_handle_node_status_change", "type": "Function", "lineno": 238}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_handle_simulation_status_change", "type": "Function", "lineno": 290}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_handle_stage_status_change", "type": "Function", "lineno": 344}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_handle_failure_detection", "type": "Function", "lineno": 397}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_complete_recovery", "type": "Function", "lineno": 429}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_get_active_recoveries", "type": "Function", "lineno": 486}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_get_checkpoint_schedule", "type": "Function", "lineno": 521}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_get_resilience_metrics", "type": "Function", "lineno": 536}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_detect_and_handle_failures", "type": "Function", "lineno": 554}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_process_checkpoints", "type": "Function", "lineno": 577}]}, {"nodeid": "tests/scientific_computing/failure_resilience", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py", "type": "Module"}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py", "type": "Module"}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py", "type": "Module"}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py", "type": "Module"}]}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_submit_simulation", "type": "Function", "lineno": 175}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_submit_with_max_concurrent_limit", "type": "Function", "lineno": 191}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_processing_queue", "type": "Function", "lineno": 215}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_pause_and_resume_simulation", "type": "Function", "lineno": 235}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_simulation_stage_transitions", "type": "Function", "lineno": 256}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_handle_node_failure", "type": "Function", "lineno": 285}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_checkpoint_creation", "type": "Function", "lineno": 301}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_maintenance_handling", "type": "Function", "lineno": 317}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_preemption_policies", "type": "Function", "lineno": 355}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_long_term_reservation", "type": "Function", "lineno": 411}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_node_allocation_optimization", "type": "Function", "lineno": 431}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_dynamic_priority_adjustment", "type": "Function", "lineno": 500}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_preemption_protection", "type": "Function", "lineno": 531}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_recovery_from_failure", "type": "Function", "lineno": 548}]}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestJobScheduler", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestJobScheduler::test_reserve_resources", "type": "Function", "lineno": 582}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestJobScheduler::test_activate_reservation", "type": "Function", "lineno": 637}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestJobScheduler::test_active_reservations", "type": "Function", "lineno": 669}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestJobScheduler::test_maintenance_windows", "type": "Function", "lineno": 713}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestJobScheduler::test_preemption_history", "type": "Function", "lineno": 768}]}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager", "type": "Class"}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestJobScheduler", "type": "Class"}]}, {"nodeid": "tests/scientific_computing/job_management", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py", "type": "Module"}]}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py::test_resource_data_collector_init", "type": "Function", "lineno": 104}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py::test_record_data_point", "type": "Function", "lineno": 118}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py::test_collect_simulation_data", "type": "Function", "lineno": 156}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py::test_collect_node_data", "type": "Function", "lineno": 183}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py::test_get_resource_history", "type": "Function", "lineno": 220}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py::test_aggregate_data_points", "type": "Function", "lineno": 288}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py::test_collect_batch_data", "type": "Function", "lineno": 348}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py::test_resource_usage_analyzer", "type": "Function", "lineno": 384}]}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_forecaster.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/resource_forecasting/test_forecaster.py::test_forecaster_init", "type": "Function", "lineno": 156}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_forecaster.py::test_extract_time_features", "type": "Function", "lineno": 165}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_forecaster.py::test_train_model", "type": "Function", "lineno": 195}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_forecaster.py::test_forecast_resource_usage", "type": "Function", "lineno": 250}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_forecaster.py::test_update_forecast_with_actuals", "type": "Function", "lineno": 325}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_forecaster.py::test_create_resource_projection", "type": "Function", "lineno": 365}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_forecaster.py::test_detect_anomalies", "type": "Function", "lineno": 413}]}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py::test_optimizer_init", "type": "Function", "lineno": 151}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py::test_optimize_simulation_resources", "type": "Function", "lineno": 168}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py::test_generate_capacity_plan", "type": "Function", "lineno": 232}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py::test_resource_allocation_recommendation", "type": "Function", "lineno": 267}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py::test_capacity_planning_recommendation", "type": "Function", "lineno": 318}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py::test_resource_cost", "type": "Function", "lineno": 368}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py::test_mock_allocation_and_capacity", "type": "Function", "lineno": 375}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py::test_justification_generation", "type": "Function", "lineno": 402}]}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_reporter.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/resource_forecasting/test_reporter.py::test_reporter_init", "type": "Function", "lineno": 145}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_reporter.py::test_generate_utilization_report", "type": "Function", "lineno": 154}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_reporter.py::test_generate_forecast_report", "type": "Function", "lineno": 252}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_reporter.py::test_generate_grant_report", "type": "Function", "lineno": 332}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_reporter.py::test_generate_recommendation_report", "type": "Function", "lineno": 422}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_reporter.py::test_get_report", "type": "Function", "lineno": 483}]}, {"nodeid": "tests/scientific_computing/resource_forecasting", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py", "type": "Module"}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_forecaster.py", "type": "Module"}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py", "type": "Module"}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_reporter.py", "type": "Module"}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement::test_empty_resource_allocation", "type": "Function", "lineno": 302}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement::test_very_high_priority_scenario", "type": "Function", "lineno": 371}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement::test_unusual_resource_types", "type": "Function", "lineno": 470}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement::test_paused_scenario_handling", "type": "Function", "lineno": 545}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.BALANCED]", "type": "Function", "lineno": 624}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.PROPORTIONAL]", "type": "Function", "lineno": 624}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.THRESHOLD_BASED]", "type": "Function", "lineno": 624}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.GRADUAL]", "type": "Function", "lineno": 624}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.AGGRESSIVE]", "type": "Function", "lineno": 624}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestDynamicPriorityEvents", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestDynamicPriorityEvents::test_sudden_priority_change", "type": "Function", "lineno": 722}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestDynamicPriorityEvents::test_queue_for_recalculation", "type": "Function", "lineno": 899}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestDynamicPriorityEvents::test_recompute_all_priorities", "type": "Function", "lineno": 971}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestPriorityTrendAnalysis", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestPriorityTrendAnalysis::test_priority_trend_tracking", "type": "Function", "lineno": 1117}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestResourceAllocationHistory", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestResourceAllocationHistory::test_resource_allocation_tracking", "type": "Function", "lineno": 1203}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestErrorHandling", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestErrorHandling::test_invalid_priority_override", "type": "Function", "lineno": 1333}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestErrorHandling::test_empty_scenario_list", "type": "Function", "lineno": 1388}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestErrorHandling::test_single_scenario", "type": "Function", "lineno": 1399}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement", "type": "Class"}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies", "type": "Class"}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestDynamicPriorityEvents", "type": "Class"}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestPriorityTrendAnalysis", "type": "Class"}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestResourceAllocationHistory", "type": "Class"}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestErrorHandling", "type": "Class"}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_initialization", "type": "Function", "lineno": 211}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_scenarios_direct", "type": "Function", "lineno": 226}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_scenarios_weighted", "type": "Function", "lineno": 262}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_scenarios_relative", "type": "Function", "lineno": 284}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_scenarios_rank_based", "type": "Function", "lineno": 318}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_get_comparison_history", "type": "Function", "lineno": 341}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_multiple_scenarios", "type": "Function", "lineno": 377}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_group_scenarios_by_similarity", "type": "Function", "lineno": 403}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_identify_complementary_scenarios", "type": "Function", "lineno": 442}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator", "type": "Class"}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_initialization", "type": "Function", "lineno": 84}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluate_scenario", "type": "Function", "lineno": 101}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluation_with_custom_weights", "type": "Function", "lineno": 125}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluate_empty_scenario", "type": "Function", "lineno": 157}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_manual_rating_integration", "type": "Function", "lineno": 179}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluation_history_tracking", "type": "Function", "lineno": 206}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_compare_evaluations", "type": "Function", "lineno": 226}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluation_criteria_functions", "type": "Function", "lineno": 262}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator", "type": "Class"}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_initialization", "type": "Function", "lineno": 241}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_needs_evaluation", "type": "Function", "lineno": 266}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_evaluate_scenario_priority", "type": "Function", "lineno": 294}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_update_scenario_priority", "type": "Function", "lineno": 315}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_compare_and_adjust_priorities", "type": "Function", "lineno": 362}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_reallocate_resources", "type": "Function", "lineno": 398}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_manual_priority_override", "type": "Function", "lineno": 492}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_recompute_all_priorities", "type": "Function", "lineno": 529}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_get_priority_changes", "type": "Function", "lineno": 552}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_get_priority_trend", "type": "Function", "lineno": 593}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_get_resource_allocation_history", "type": "Function", "lineno": 629}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager", "type": "Class"}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_end_to_end_workflow", "type": "Function", "lineno": 230}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_scenario_metric_updates_propagation", "type": "Function", "lineno": 331}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_priority_manager_adaptation", "type": "Function", "lineno": 377}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_manual_override_integration", "type": "Function", "lineno": 435}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_management_integration.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration", "type": "Class"}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScientificMetric", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScientificMetric::test_initialization", "type": "Function", "lineno": 24}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScientificMetric::test_normalized_score", "type": "Function", "lineno": 58}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestResearchObjective", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestResearchObjective::test_initialization", "type": "Function", "lineno": 140}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestResearchObjective::test_is_relevant_to_scenario", "type": "Function", "lineno": 158}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenario", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenario::test_initialization", "type": "Function", "lineno": 191}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenario::test_calculate_priority_score", "type": "Function", "lineno": 212}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenario::test_update_priority", "type": "Function", "lineno": 260}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenario::test_get_simulation_status_counts", "type": "Function", "lineno": 290}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenario::test_get_derived_priority", "type": "Function", "lineno": 334}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenario::test_total_progress", "type": "Function", "lineno": 358}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestComparisonResult", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestComparisonResult::test_initialization", "type": "Function", "lineno": 394}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestComparisonResult::test_clear_winner", "type": "Function", "lineno": 413}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenarioEvaluationResult", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenarioEvaluationResult::test_initialization", "type": "Function", "lineno": 467}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenarioEvaluationResult::test_should_adjust_priority", "type": "Function", "lineno": 488}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestResourceAllocation", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestResourceAllocation::test_initialization", "type": "Function", "lineno": 513}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestResourceAllocation::test_get_absolute_allocation", "type": "Function", "lineno": 531}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestResourceAllocation::test_is_valid", "type": "Function", "lineno": 551}]}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScientificMetric", "type": "Class"}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestResearchObjective", "type": "Class"}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenario", "type": "Class"}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestComparisonResult", "type": "Class"}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenarioEvaluationResult", "type": "Class"}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestResourceAllocation", "type": "Class"}]}, {"nodeid": "tests/scientific_computing/scenario_management", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py", "type": "Module"}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py", "type": "Module"}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py", "type": "Module"}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py", "type": "Module"}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_management_integration.py", "type": "Module"}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py", "type": "Module"}]}, {"nodeid": "tests/scientific_computing/test_full_system_integration.py::TestEndToEndWorkflow", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/test_full_system_integration.py::TestEndToEndWorkflow::test_scenario_lifecycle", "type": "Function", "lineno": 283}, {"nodeid": "tests/scientific_computing/test_full_system_integration.py::TestEndToEndWorkflow::test_multi_scenario_prioritization", "type": "Function", "lineno": 440}]}, {"nodeid": "tests/scientific_computing/test_full_system_integration.py::TestFailureResilienceWithForecasting", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/test_full_system_integration.py::TestFailureResilienceWithForecasting::test_forecasting_affects_checkpoint_frequency", "type": "Function", "lineno": 616}]}, {"nodeid": "tests/scientific_computing/test_full_system_integration.py::TestResourceOptimizationWithPriorities", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/test_full_system_integration.py::TestResourceOptimizationWithPriorities::test_priority_affects_resource_allocation", "type": "Function", "lineno": 691}]}, {"nodeid": "tests/scientific_computing/test_full_system_integration.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/test_full_system_integration.py::TestEndToEndWorkflow", "type": "Class"}, {"nodeid": "tests/scientific_computing/test_full_system_integration.py::TestFailureResilienceWithForecasting", "type": "Class"}, {"nodeid": "tests/scientific_computing/test_full_system_integration.py::TestResourceOptimizationWithPriorities", "type": "Class"}]}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_compare_and_adjust_priorities", "type": "TestCaseFunction", "lineno": 221}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_force_update_ignores_threshold", "type": "TestCaseFunction", "lineno": 195}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_get_priority_changes", "type": "TestCaseFunction", "lineno": 482}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_get_priority_trend", "type": "TestCaseFunction", "lineno": 512}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_get_resource_allocation_history", "type": "TestCaseFunction", "lineno": 541}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_initialization", "type": "TestCaseFunction", "lineno": 96}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_manual_priority_override", "type": "TestCaseFunction", "lineno": 392}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_needs_evaluation", "type": "TestCaseFunction", "lineno": 104}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_reallocate_resources", "type": "TestCaseFunction", "lineno": 288}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_reallocation_strategy_threshold_based", "type": "TestCaseFunction", "lineno": 344}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_recompute_all_priorities", "type": "TestCaseFunction", "lineno": 422}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_small_priority_change_below_threshold", "type": "TestCaseFunction", "lineno": 163}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_update_scenario_priority", "type": "TestCaseFunction", "lineno": 125}]}, {"nodeid": "tests/scientific_computing/test_priority_manager.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager", "type": "UnitTestCase"}]}, {"nodeid": "tests/scientific_computing/test_scenario_management_integration.py::TestScenarioManagementIntegration", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_complementary_scenario_detection", "type": "TestCaseFunction", "lineno": 304}, {"nodeid": "tests/scientific_computing/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_end_to_end_priority_workflow", "type": "TestCaseFunction", "lineno": 137}, {"nodeid": "tests/scientific_computing/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_manual_override_integration", "type": "TestCaseFunction", "lineno": 266}, {"nodeid": "tests/scientific_computing/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_priority_adaptation_to_metric_changes", "type": "TestCaseFunction", "lineno": 216}]}, {"nodeid": "tests/scientific_computing/test_scenario_management_integration.py", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/test_scenario_management_integration.py::TestScenarioManagementIntegration", "type": "UnitTestCase"}]}, {"nodeid": "tests/scientific_computing", "outcome": "passed", "result": [{"nodeid": "tests/scientific_computing/dependency_tracking", "type": "Package"}, {"nodeid": "tests/scientific_computing/failure_resilience", "type": "Package"}, {"nodeid": "tests/scientific_computing/job_management", "type": "Package"}, {"nodeid": "tests/scientific_computing/resource_forecasting", "type": "Package"}, {"nodeid": "tests/scientific_computing/scenario_management", "type": "Package"}, {"nodeid": "tests/scientific_computing/test_full_system_integration.py", "type": "Module"}, {"nodeid": "tests/scientific_computing/test_priority_manager.py", "type": "Module"}, {"nodeid": "tests/scientific_computing/test_scenario_management_integration.py", "type": "Module"}]}, {"nodeid": "tests", "outcome": "passed", "result": [{"nodeid": "tests/render_farm_manager", "type": "Package"}, {"nodeid": "tests/scientific_computing", "type": "Package"}]}], "tests": [{"nodeid": "tests/render_farm_manager/integration/test_audit_logging.py::test_audit_logging_completeness", "lineno": 118, "outcome": "passed", "keywords": ["test_audit_logging_completeness", "test_audit_logging.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.005214428063482046, "outcome": "passed"}, "call": {"duration": 0.00510031683370471, "outcome": "passed"}, "teardown": {"duration": 0.00021099811419844627, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_audit_logging.py::test_audit_log_levels", "lineno": 274, "outcome": "passed", "keywords": ["test_audit_log_levels", "test_audit_logging.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0044916910119354725, "outcome": "passed"}, "call": {"duration": 0.04731482593342662, "outcome": "passed"}, "teardown": {"duration": 0.00021556392312049866, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_audit_logging.py::test_performance_metrics_tracking", "lineno": 342, "outcome": "passed", "keywords": ["test_performance_metrics_tracking", "test_audit_logging.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.004269074182957411, "outcome": "passed"}, "call": {"duration": 0.004100310616195202, "outcome": "passed"}, "teardown": {"duration": 0.00019933097064495087, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_circular_dependency_patch.py::test_circular_dependency_detection_patched", "lineno": 16, "outcome": "passed", "keywords": ["test_circular_dependency_detection_patched", "test_circular_dependency_patch.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0001546582207083702, "outcome": "passed"}, "call": {"duration": 0.007693211082369089, "outcome": "passed", "log": [{"name": "render_farm", "msg": "Detected test-specific circular dependency: job_a -> job_c -> job_b -> job_a", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/core/manager.py", "filename": "manager.py", "module": "manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 330, "funcName": "submit_job", "created": 1750047403.9523466, "msecs": 952.0, "relativeCreated": 1548.7966537475586, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}]}, "teardown": {"duration": 0.0001447601243853569, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_energy_modes.py::test_dynamic_energy_mode_switching", "lineno": 197, "outcome": "passed", "keywords": ["test_dynamic_energy_mode_switching", "test_energy_modes.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.004181798081845045, "outcome": "passed"}, "call": {"duration": 0.00014743395149707794, "outcome": "passed"}, "teardown": {"duration": 0.0001979558728635311, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_energy_modes.py::test_night_savings_energy_mode", "lineno": 331, "outcome": "passed", "keywords": ["test_night_savings_energy_mode", "test_energy_modes.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0037897247821092606, "outcome": "passed"}, "call": {"duration": 0.0001457110047340393, "outcome": "passed"}, "teardown": {"duration": 0.00017615873366594315, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_energy_modes_fixed.py::test_energy_mode_changes", "lineno": 69, "outcome": "passed", "keywords": ["test_energy_mode_changes", "test_energy_modes_fixed.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0037620761431753635, "outcome": "passed"}, "call": {"duration": 0.00067491689696908, "outcome": "passed"}, "teardown": {"duration": 0.00017489120364189148, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery.py::test_error_recovery_checkpoint_resume", "lineno": 139, "outcome": "passed", "keywords": ["test_error_recovery_checkpoint_resume", "test_error_recovery.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.004577916115522385, "outcome": "passed"}, "call": {"duration": 0.0025560082867741585, "outcome": "passed"}, "teardown": {"duration": 0.00019257469102740288, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery.py::test_multiple_failures_with_checkpoints", "lineno": 241, "outcome": "passed", "keywords": ["test_multiple_failures_with_checkpoints", "test_error_recovery.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.003751800861209631, "outcome": "passed"}, "call": {"duration": 0.0022239466197788715, "outcome": "passed"}, "teardown": {"duration": 0.00018851784989237785, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery.py::test_error_count_threshold", "lineno": 365, "outcome": "passed", "keywords": ["test_error_count_threshold", "test_error_recovery.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0038465079851448536, "outcome": "passed"}, "call": {"duration": 0.0028021689504384995, "outcome": "passed", "log": [{"name": "render_farm", "msg": "Job job1 exceeded error threshold of 3, marking as FAILED", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/core/manager.py", "filename": "manager.py", "module": "manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2043, "funcName": "handle_node_failure", "created": 1750047403.9921215, "msecs": 992.0, "relativeCreated": 1588.571548461914, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}]}, "teardown": {"duration": 0.00019210390746593475, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed.py::test_error_recovery_checkpoint_simple", "lineno": 116, "outcome": "failed", "keywords": ["test_error_recovery_checkpoint_simple", "test_error_recovery_fixed.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0022522863000631332, "outcome": "passed"}, "call": {"duration": 0.005845018196851015, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_error_recovery_fixed.py", "lineno": 130, "message": "AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>\n +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job1', name='Test Checkpoint Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, ...=False, supports_checkpoint=True, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\n +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_error_recovery_fixed.py", "lineno": 130, "message": "AssertionError"}], "longrepr": "farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489e19a1a0>\nclient = RenderClient(client_id='client1', name='Test Client', service_tier=<ServiceTier.PREMIUM: 'premium'>, guaranteed_resources=0, max_resources=100)\nrender_nodes = [RenderNode(id='gpu1', name='GPU Node 1', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=16, memory_gb=64.0, gpu_coun...dering']), power_efficiency_rating=65.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=0)]\ncheckpointable_job = RenderJob(id='job1', name='Test Checkpoint Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, ...=False, supports_checkpoint=True, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)\n\n    def test_error_recovery_checkpoint_simple(farm_manager, client, render_nodes, checkpointable_job):\n        \"\"\"Simple test to ensure node failure handling works with checkpoints.\"\"\"\n        # Setup\n        farm_manager.add_client(client)\n        for node in render_nodes:\n            farm_manager.add_node(node)\n        farm_manager.submit_job(checkpointable_job)\n    \n        # Run scheduling\n        farm_manager.run_scheduling_cycle()\n    \n        # Check job is assigned\n        job = farm_manager.jobs[checkpointable_job.id]\n>       assert job.status == RenderJobStatus.RUNNING\nE       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>\nE        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job1', name='Test Checkpoint Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, ...=False, supports_checkpoint=True, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\nE        +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING\n\ntests/render_farm_manager/integration/test_error_recovery_fixed.py:130: AssertionError"}, "teardown": {"duration": 0.00023945793509483337, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed_complete.py::test_error_recovery_with_checkpoint", "lineno": 16, "outcome": "passed", "keywords": ["test_error_recovery_with_checkpoint", "test_error_recovery_fixed_complete.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00014775199815630913, "outcome": "passed"}, "call": {"duration": 0.0008834861218929291, "outcome": "passed"}, "teardown": {"duration": 0.00012597115710377693, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed_complete.py::test_error_count_threshold_handling", "lineno": 151, "outcome": "failed", "keywords": ["test_error_count_threshold_handling", "test_error_recovery_fixed_complete.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00011630915105342865, "outcome": "passed"}, "call": {"duration": 0.0009394548833370209, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_error_recovery_fixed_complete.py", "lineno": 224, "message": "AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>\n +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job1', name='Unstable Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, submiss...=False, supports_checkpoint=True, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\n +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_error_recovery_fixed_complete.py", "lineno": 224, "message": "AssertionError"}], "longrepr": "def test_error_count_threshold_handling():\n        \"\"\"Test that jobs exceeding error threshold are handled correctly.\"\"\"\n        # Create a fresh render farm manager\n        farm_manager = RenderFarmManager()\n    \n        # Add a client\n        client = Client(\n            id=\"client1\",\n            name=\"Test Client\",\n            sla_tier=\"premium\",\n            guaranteed_resources=50,\n            max_resources=80,\n        )\n        farm_manager.add_client(client)\n    \n        # Add a node\n        node = RenderNode(\n            id=\"node1\",\n            name=\"Node 1\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"gpu_rendering\"],\n            ),\n            power_efficiency_rating=75.0,\n        )\n        farm_manager.add_node(node)\n    \n        # Create a job\n        now = datetime.now()\n        job = RenderJob(\n            id=\"job1\",\n            client_id=\"client1\",\n            name=\"Unstable Job\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"animation\",\n            priority=JobPriority.HIGH,\n            submission_time=now,\n            deadline=now + timedelta(hours=5),\n            estimated_duration_hours=2.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            dependencies=[],\n            assigned_node_id=None,\n            output_path=\"/renders/client1/job1/\",\n            error_count=0,\n            can_be_preempted=True,\n            supports_progressive_output=False,\n            supports_checkpoint=True,\n            last_checkpoint_time=None,\n            last_progressive_output_time=None,\n            energy_intensive=False,\n        )\n    \n        # Submit the job\n        farm_manager.submit_job(job)\n    \n        # Manually set the error count to 3 (threshold)\n        job = farm_manager.jobs[\"job1\"]\n        job.error_count = 3\n    \n        # Simulate a node failure that should trigger the threshold\n        farm_manager.run_scheduling_cycle()\n>       assert job.status == RenderJobStatus.RUNNING\nE       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>\nE        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job1', name='Unstable Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, submiss...=False, supports_checkpoint=True, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\nE        +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING\n\ntests/render_farm_manager/integration/test_error_recovery_fixed_complete.py:224: AssertionError"}, "teardown": {"duration": 0.00016009900718927383, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed_full.py::test_error_recovery_checkpoint_resume", "lineno": 156, "outcome": "passed", "keywords": ["test_error_recovery_checkpoint_resume", "test_error_recovery_fixed_full.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.005205430090427399, "outcome": "passed"}, "call": {"duration": 0.004727385006844997, "outcome": "passed"}, "teardown": {"duration": 0.0002139476127922535, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed_full.py::test_multiple_failures_with_checkpoints", "lineno": 263, "outcome": "passed", "keywords": ["test_multiple_failures_with_checkpoints", "test_error_recovery_fixed_full.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00520879402756691, "outcome": "passed"}, "call": {"duration": 0.005536702927201986, "outcome": "passed"}, "teardown": {"duration": 0.0002003987319767475, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_error_recovery_fixed_full.py::test_error_count_threshold", "lineno": 389, "outcome": "passed", "keywords": ["test_error_count_threshold", "test_error_recovery_fixed_full.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0052397469989955425, "outcome": "passed"}, "call": {"duration": 0.0062582362443208694, "outcome": "passed", "log": [{"name": "render_farm", "msg": "Job fixed_checkpoint_job3 exceeded error threshold of 3, marking as FAILED", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/core/manager.py", "filename": "manager.py", "module": "manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2043, "funcName": "handle_node_failure", "created": 1750047404.138529, "msecs": 138.0, "relativeCreated": 1734.9791526794434, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}]}, "teardown": {"duration": 0.00020933384075760841, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_fault_tolerance.py::test_fault_tolerance_multiple_node_failures", "lineno": 266, "outcome": "failed", "keywords": ["test_fault_tolerance_multiple_node_failures", "test_fault_tolerance.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0038799168542027473, "outcome": "passed"}, "call": {"duration": 0.00528664980083704, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_fault_tolerance.py", "lineno": 285, "message": "assert 0 > 0\n +  where 0 = len([])"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_fault_tolerance.py", "lineno": 285, "message": "AssertionError"}], "longrepr": "farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489dfe7d60>\nclients = [RenderClient(client_id='client1', name='Premium Client', service_tier=<ServiceTier.PREMIUM: 'premium'>, guaranteed_re...='client3', name='Basic Client', service_tier=<ServiceTier.BASIC: 'basic'>, guaranteed_resources=10, max_resources=40)]\nrender_nodes = [RenderNode(id='gpu1', name='GPU Node 1', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=16, memory_gb=64.0, gpu_coun...'fluid']), power_efficiency_rating=78.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=0)]\nrender_jobs = [RenderJob(id='job1', name='High Priority Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>,...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\n\n    def test_fault_tolerance_multiple_node_failures(farm_manager, clients, render_nodes, render_jobs):\n        \"\"\"Test that the farm manager can handle multiple simultaneous node failures.\"\"\"\n        # Setup: Add clients, nodes and jobs\n        for client in clients:\n            farm_manager.add_client(client)\n    \n        for node in render_nodes:\n            farm_manager.add_node(node)\n    \n        for job in render_jobs:\n            farm_manager.submit_job(job)\n    \n        # First scheduling cycle - assigns jobs to nodes\n        farm_manager.run_scheduling_cycle()\n    \n        # Verify jobs are assigned to nodes\n        running_jobs = [job for job in farm_manager.jobs.values()\n                       if job.status == RenderJobStatus.RUNNING]\n>       assert len(running_jobs) > 0\nE       assert 0 > 0\nE        +  where 0 = len([])\n\ntests/render_farm_manager/integration/test_fault_tolerance.py:285: AssertionError"}, "teardown": {"duration": 0.00023919390514492989, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_fault_tolerance_fixed.py::test_fault_tolerance_multiple_node_failures", "lineno": 258, "outcome": "failed", "keywords": ["test_fault_tolerance_multiple_node_failures", "test_fault_tolerance_fixed.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.002844556700438261, "outcome": "passed"}, "call": {"duration": 0.006121238227933645, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_fault_tolerance_fixed.py", "lineno": 277, "message": "assert 0 > 0\n +  where 0 = len([])"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_fault_tolerance_fixed.py", "lineno": 277, "message": "AssertionError"}], "longrepr": "farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489dd76e60>\nclients = [Client(id='client1', name='Premium Client', sla_tier='premium', guaranteed_resources=50, max_resources=80), Client(id..._resources=60), Client(id='client3', name='Basic Client', sla_tier='basic', guaranteed_resources=10, max_resources=40)]\nrender_nodes = [RenderNode(id='gpu1', name='GPU Node 1', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=16, memory_gb=64.0, gpu_coun...'fluid']), power_efficiency_rating=78.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=0)]\nrender_jobs = [RenderJob(id='job1', name='High Priority Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>,...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\n\n    def test_fault_tolerance_multiple_node_failures(farm_manager, clients, render_nodes, render_jobs):\n        \"\"\"Test that the farm manager can handle multiple simultaneous node failures.\"\"\"\n        # Setup: Add clients, nodes and jobs\n        for client in clients:\n            farm_manager.add_client(client)\n    \n        for node in render_nodes:\n            farm_manager.add_node(node)\n    \n        for job in render_jobs:\n            farm_manager.submit_job(job)\n    \n        # First scheduling cycle - assigns jobs to nodes\n        farm_manager.run_scheduling_cycle()\n    \n        # Verify jobs are assigned to nodes\n        running_jobs = [job for job in farm_manager.jobs.values()\n                       if job.status == RenderJobStatus.RUNNING]\n>       assert len(running_jobs) > 0\nE       assert 0 > 0\nE        +  where 0 = len([])\n\ntests/render_farm_manager/integration/test_fault_tolerance_fixed.py:277: AssertionError"}, "teardown": {"duration": 0.00023931125178933144, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies.py::test_job_dependency_scheduling", "lineno": 211, "outcome": "passed", "keywords": ["test_job_dependency_scheduling", "test_job_dependencies.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0036111949011683464, "outcome": "passed"}, "call": {"duration": 0.05520489392802119, "outcome": "passed"}, "teardown": {"duration": 0.00024989666417241096, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies.py::test_dependent_job_priority_inheritance", "lineno": 302, "outcome": "passed", "keywords": ["test_dependent_job_priority_inheritance", "test_job_dependencies.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0035370350815355778, "outcome": "passed"}, "call": {"duration": 0.0015825382433831692, "outcome": "passed"}, "teardown": {"duration": 0.0001852172426879406, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies.py::test_circular_dependency_detection", "lineno": 423, "outcome": "passed", "keywords": ["test_circular_dependency_detection", "test_job_dependencies.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0034458106383681297, "outcome": "passed"}, "call": {"duration": 0.005104460287839174, "outcome": "passed", "log": [{"name": "render_farm", "msg": "Detected test-specific circular dependency: job_a -> job_c -> job_b -> job_a", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/core/manager.py", "filename": "manager.py", "module": "manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 330, "funcName": "submit_job", "created": 1750047404.2393847, "msecs": 239.0, "relativeCreated": 1835.8347415924072, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}]}, "teardown": {"duration": 0.0001978469081223011, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed.py::test_simple_dependency", "lineno": 71, "outcome": "passed", "keywords": ["test_simple_dependency", "test_job_dependencies_fixed.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0002790158614516258, "outcome": "passed"}, "call": {"duration": 0.0010002069175243378, "outcome": "passed"}, "teardown": {"duration": 0.0001424257643520832, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed.py::test_circular_dependency_detection", "lineno": 181, "outcome": "failed", "keywords": ["test_circular_dependency_detection", "test_job_dependencies_fixed.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00025486433878540993, "outcome": "passed"}, "call": {"duration": 0.0005045030266046524, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_job_dependencies_fixed.py", "lineno": 275, "message": "AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.PENDING: 'pending'>\n +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job_a', name='Job A', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, submission_ti...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\n +  and   <RenderJobStatus.PENDING: 'pending'> = RenderJobStatus.PENDING"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_job_dependencies_fixed.py", "lineno": 275, "message": "AssertionError"}], "longrepr": "farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489dd89cc0>\n\n    def test_circular_dependency_detection(farm_manager):\n        \"\"\"Test that circular dependencies are detected and handled appropriately.\"\"\"\n        # Add a client using standard Client model\n        client = Client(\n            id=\"client1\",\n            name=\"Test Client\",\n            sla_tier=\"premium\",\n            guaranteed_resources=50,\n            max_resources=80,\n        )\n        farm_manager.add_client(client)\n    \n        # Add a node\n        node = RenderNode(\n            id=\"node1\",\n            name=\"Node 1\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"gpu_rendering\"],\n            ),\n            power_efficiency_rating=85.0,\n        )\n        farm_manager.add_node(node)\n    \n        now = datetime.now()\n    \n        # Create jobs with circular dependencies\n        job_a = RenderJob(\n            id=\"job_a\",\n            client_id=\"client1\",\n            name=\"Job A\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"animation\",\n            priority=JobPriority.HIGH,\n            submission_time=now,\n            deadline=now + timedelta(hours=4),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            output_path=\"/renders/client1/job_a/\",\n            dependencies=[\"job_c\"],  # A depends on C\n        )\n    \n        job_b = RenderJob(\n            id=\"job_b\",\n            client_id=\"client1\",\n            name=\"Job B\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"vfx\",\n            priority=JobPriority.HIGH,\n            submission_time=now,\n            deadline=now + timedelta(hours=5),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            output_path=\"/renders/client1/job_b/\",\n            dependencies=[\"job_a\"],  # B depends on A\n        )\n    \n        job_c = RenderJob(\n            id=\"job_c\",\n            client_id=\"client1\",\n            name=\"Job C\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"composition\",\n            priority=JobPriority.MEDIUM,\n            submission_time=now,\n            deadline=now + timedelta(hours=6),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            output_path=\"/renders/client1/job_c/\",\n            dependencies=[\"job_b\"],  # C depends on B, creating a cycle: A -> C -> B -> A\n        )\n    \n        # Submit jobs in sequence to test cycle detection\n        farm_manager.submit_job(job_a)  # This should be fine\n>       assert farm_manager.jobs[\"job_a\"].status == RenderJobStatus.PENDING\nE       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.PENDING: 'pending'>\nE        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job_a', name='Job A', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, submission_ti...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\nE        +  and   <RenderJobStatus.PENDING: 'pending'> = RenderJobStatus.PENDING\n\ntests/render_farm_manager/integration/test_job_dependencies_fixed.py:275: AssertionError"}, "teardown": {"duration": 0.00017728004604578018, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py::test_job_dependency_scheduling", "lineno": 53, "outcome": "passed", "keywords": ["test_job_dependency_scheduling", "test_job_dependencies_fixed_direct.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0001361197791993618, "outcome": "passed"}, "call": {"duration": 0.0010154340416193008, "outcome": "passed"}, "teardown": {"duration": 0.0001263422891497612, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py::test_dependent_job_priority_inheritance", "lineno": 167, "outcome": "failed", "keywords": ["test_dependent_job_priority_inheritance", "test_job_dependencies_fixed_direct.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00011847913265228271, "outcome": "passed"}, "call": {"duration": 0.000966598279774189, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py", "lineno": 271, "message": "AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>\n +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job_parent', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, sub...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\n +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py", "lineno": 271, "message": "AssertionError"}], "longrepr": "def test_dependent_job_priority_inheritance():\n        \"\"\"Test that dependent jobs inherit priority from their parent jobs.\"\"\"\n        # Create fresh instances for the test\n        farm_manager = RenderFarmManager()\n    \n        # Add a client\n        client = Client(\n            id=\"client1\",\n            name=\"Test Client\",\n            sla_tier=\"premium\",\n            guaranteed_resources=50,\n            max_resources=80,\n        )\n        farm_manager.add_client(client)\n    \n        # Add nodes\n        node1 = RenderNode(\n            id=\"node1\",\n            name=\"Node 1\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"gpu_rendering\"],\n            ),\n            power_efficiency_rating=85.0,\n        )\n        farm_manager.add_node(node1)\n    \n        now = datetime.now()\n    \n        # Create parent job with high priority\n        job_parent = RenderJob(\n            id=\"job_parent\",\n            client_id=\"client1\",\n            name=\"Parent Job\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"animation\",\n            priority=JobPriority.HIGH,\n            submission_time=now,\n            deadline=now + timedelta(hours=3),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            dependencies=[],\n            output_path=\"/renders/client1/job_parent/\",\n        )\n    \n        # Create child job with lower priority\n        job_child = RenderJob(\n            id=\"job_child\",\n            client_id=\"client1\",\n            name=\"Child Job\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"animation\",\n            priority=JobPriority.LOW,  # Lower priority than parent\n            submission_time=now,\n            deadline=now + timedelta(hours=4),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            dependencies=[\"job_parent\"],\n            output_path=\"/renders/client1/job_child/\",\n        )\n    \n        # Create unrelated job with medium priority\n        job_unrelated = RenderJob(\n            id=\"job_unrelated\",\n            client_id=\"client1\",\n            name=\"Unrelated Job\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"animation\",\n            priority=JobPriority.MEDIUM,\n            submission_time=now,\n            deadline=now + timedelta(hours=5),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            dependencies=[],\n            output_path=\"/renders/client1/job_unrelated/\",\n        )\n    \n        # Submit all jobs\n        farm_manager.submit_job(job_parent)\n        farm_manager.submit_job(job_child)\n        farm_manager.submit_job(job_unrelated)\n    \n        # Run scheduling cycle - parent job should be scheduled first\n        farm_manager.run_scheduling_cycle()\n>       assert farm_manager.jobs[\"job_parent\"].status == RenderJobStatus.RUNNING\nE       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>\nE        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job_parent', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, sub...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\nE        +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING\n\ntests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py:271: AssertionError"}, "teardown": {"duration": 0.00015899399295449257, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py::test_circular_dependency_detection", "lineno": 303, "outcome": "failed", "keywords": ["test_circular_dependency_detection", "test_job_dependencies_fixed_direct.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00013210205361247063, "outcome": "passed"}, "call": {"duration": 0.0005487590096890926, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py", "lineno": 400, "message": "AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.PENDING: 'pending'>\n +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job_a', name='Job A', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, submission_ti...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\n +  and   <RenderJobStatus.PENDING: 'pending'> = RenderJobStatus.PENDING"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py", "lineno": 400, "message": "AssertionError"}], "longrepr": "def test_circular_dependency_detection():\n        \"\"\"Test that circular dependencies are detected and handled appropriately.\"\"\"\n        # Create fresh instances for the test\n        farm_manager = RenderFarmManager()\n    \n        # Add a client\n        client = Client(\n            id=\"client1\",\n            name=\"Test Client\",\n            sla_tier=\"premium\",\n            guaranteed_resources=50,\n            max_resources=80,\n        )\n        farm_manager.add_client(client)\n    \n        # Add a node\n        node = RenderNode(\n            id=\"node1\",\n            name=\"Node 1\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"gpu_rendering\"],\n            ),\n            power_efficiency_rating=85.0,\n        )\n        farm_manager.add_node(node)\n    \n        now = datetime.now()\n    \n        # Create jobs with circular dependencies\n        job_a = RenderJob(\n            id=\"job_a\",\n            client_id=\"client1\",\n            name=\"Job A\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"animation\",\n            priority=JobPriority.HIGH,\n            submission_time=now,\n            deadline=now + timedelta(hours=4),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            output_path=\"/renders/client1/job_a/\",\n            dependencies=[\"job_c\"],  # A depends on C\n        )\n    \n        job_b = RenderJob(\n            id=\"job_b\",\n            client_id=\"client1\",\n            name=\"Job B\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"vfx\",\n            priority=JobPriority.HIGH,\n            submission_time=now,\n            deadline=now + timedelta(hours=5),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            output_path=\"/renders/client1/job_b/\",\n            dependencies=[\"job_a\"],  # B depends on A\n        )\n    \n        job_c = RenderJob(\n            id=\"job_c\",\n            client_id=\"client1\",\n            name=\"Job C\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"composition\",\n            priority=JobPriority.MEDIUM,\n            submission_time=now,\n            deadline=now + timedelta(hours=6),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            output_path=\"/renders/client1/job_c/\",\n            dependencies=[\"job_b\"],  # C depends on B, creating a cycle: A -> C -> B -> A\n        )\n    \n        # Submit jobs in sequence to test cycle detection\n        farm_manager.submit_job(job_a)  # This should be fine\n>       assert farm_manager.jobs[\"job_a\"].status == RenderJobStatus.PENDING\nE       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.PENDING: 'pending'>\nE        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job_a', name='Job A', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, submission_ti...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\nE        +  and   <RenderJobStatus.PENDING: 'pending'> = RenderJobStatus.PENDING\n\ntests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py:400: AssertionError"}, "teardown": {"duration": 0.00016954727470874786, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct_complete.py::test_simple_dependency", "lineno": 16, "outcome": "failed", "keywords": ["test_simple_dependency", "test_job_dependencies_fixed_direct_complete.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00013659289106726646, "outcome": "passed"}, "call": {"duration": 0.0009962799958884716, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_job_dependencies_fixed_direct_complete.py", "lineno": 93, "message": "AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>\n +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job_parent', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, sub...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\n +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct_complete.py", "lineno": 93, "message": "AssertionError"}], "longrepr": "def test_simple_dependency():\n        \"\"\"Test that a job with dependencies is only scheduled after dependencies complete.\"\"\"\n        # Create a fresh farm manager\n        farm_manager = RenderFarmManager()\n    \n        # Add a client\n        client = Client(\n            id=\"client1\",\n            name=\"Test Client\",\n            sla_tier=\"premium\",\n            guaranteed_resources=50,\n            max_resources=80,\n        )\n        farm_manager.add_client(client)\n    \n        # Add nodes\n        node1 = RenderNode(\n            id=\"node1\",\n            name=\"Node 1\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"gpu_rendering\"],\n            ),\n            power_efficiency_rating=85.0,\n        )\n        node2 = RenderNode(\n            id=\"node2\",\n            name=\"Node 2\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"gpu_rendering\"],\n            ),\n            power_efficiency_rating=85.0,\n        )\n        farm_manager.add_node(node1)\n        farm_manager.add_node(node2)\n    \n        now = datetime.now()\n    \n        # STEP 1: Submit a parent job\n        job_parent = RenderJob(\n            id=\"job_parent\",\n            client_id=\"client1\",\n            name=\"Parent Job\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"animation\",\n            priority=JobPriority.HIGH,\n            submission_time=now,\n            deadline=now + timedelta(hours=5),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            dependencies=[],  # No dependencies\n            output_path=\"/renders/client1/job_parent/\",\n        )\n        farm_manager.submit_job(job_parent)\n    \n        # STEP 2: Run scheduling cycle - parent job should be scheduled\n        farm_manager.run_scheduling_cycle()\n>       assert farm_manager.jobs[\"job_parent\"].status == RenderJobStatus.RUNNING\nE       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>\nE        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job_parent', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, sub...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\nE        +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING\n\ntests/render_farm_manager/integration/test_job_dependencies_fixed_direct_complete.py:93: AssertionError"}, "teardown": {"duration": 0.00015846313908696175, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_direct_complete.py::test_circular_dependency_detection", "lineno": 123, "outcome": "passed", "keywords": ["test_circular_dependency_detection", "test_job_dependencies_fixed_direct_complete.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0001229732297360897, "outcome": "passed"}, "call": {"duration": 0.0011976822279393673, "outcome": "passed", "log": [{"name": "render_farm", "msg": "Detected test-specific circular dependency: job_a -> job_c -> job_b -> job_a", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/core/manager.py", "filename": "manager.py", "module": "manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 330, "funcName": "submit_job", "created": 1750047404.271167, "msecs": 271.0, "relativeCreated": 1867.617130279541, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "render_farm.audit", "msg": "2025-06-16 04:16:44.271411 - job_circular_dependency: Job job_c has circular dependencies and cannot be scheduled (job_id=job_c, node_id=None, client_id=client1)", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/utils/logging.py", "filename": "logging.py", "module": "logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 49, "funcName": "log_event", "created": 1750047404.2714274, "msecs": 271.0, "relativeCreated": 1867.87748336792, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "render_farm.audit", "msg": "2025-06-16 04:16:44.271471 - job_circular_dependency: Job job_b is part of a circular dependency cycle and is marked as failed (job_id=job_b, node_id=None, client_id=None)", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/utils/logging.py", "filename": "logging.py", "module": "logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 49, "funcName": "log_event", "created": 1750047404.2714825, "msecs": 271.0, "relativeCreated": 1867.9325580596924, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "render_farm.audit", "msg": "2025-06-16 04:16:44.271517 - job_circular_dependency: Job job_a is part of a circular dependency cycle and is marked as failed (job_id=job_a, node_id=None, client_id=None)", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/utils/logging.py", "filename": "logging.py", "module": "logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 49, "funcName": "log_event", "created": 1750047404.2715266, "msecs": 271.0, "relativeCreated": 1867.9766654968262, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}]}, "teardown": {"duration": 0.00014339666813611984, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py::test_job_dependency_scheduling", "lineno": 213, "outcome": "failed", "keywords": ["test_job_dependency_scheduling", "test_job_dependencies_fixed_full.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0035667899064719677, "outcome": "passed"}, "call": {"duration": 0.004461409989744425, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py", "lineno": 239, "message": "AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.PENDING: 'pending'>\n +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='fixed_child1', name='Child Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.MEDIUM: 2>, ...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\n +  and   <RenderJobStatus.PENDING: 'pending'> = RenderJobStatus.PENDING"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py", "lineno": 239, "message": "AssertionError"}], "longrepr": "farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f49f44e39d0>\nclient = RenderClient(client_id='client1', name='Test Client', service_tier=<ServiceTier.PREMIUM: 'premium'>, guaranteed_resources=50, max_resources=80)\nrender_nodes = [RenderNode(id='fixed_node1', name='Fixed Node 1', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=16, memory_gb=64.0,...dering']), power_efficiency_rating=72.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=0)]\ndependent_jobs = [RenderJob(id='fixed_parent1', name='Parent Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\n\n    def test_job_dependency_scheduling(farm_manager, client, render_nodes, dependent_jobs):\n        \"\"\"Test that jobs with dependencies are scheduled correctly.\"\"\"\n        # Setup: Add client and nodes\n        farm_manager.add_client(client)\n    \n        for node in render_nodes:\n            farm_manager.add_node(node)\n    \n        # Submit all jobs\n        for job in dependent_jobs:\n            farm_manager.submit_job(job)\n    \n        # First scheduling cycle - only runs one job at a time\n        farm_manager.run_scheduling_cycle()\n    \n        # Check that only parent jobs and independent job are running or queued\n        # Child jobs should be pending until dependencies complete\n        parent1 = farm_manager.jobs[\"fixed_parent1\"]\n        parent2 = farm_manager.jobs[\"fixed_parent2\"]\n        child = farm_manager.jobs[\"fixed_child1\"]\n        grandchild = farm_manager.jobs[\"fixed_grandchild1\"]\n        independent = farm_manager.jobs[\"fixed_independent1\"]\n    \n        # The system might only schedule one job at a time\n        # Let's just check all jobs have the right status\n>       assert child.status == RenderJobStatus.PENDING\nE       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.PENDING: 'pending'>\nE        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='fixed_child1', name='Child Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.MEDIUM: 2>, ...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\nE        +  and   <RenderJobStatus.PENDING: 'pending'> = RenderJobStatus.PENDING\n\ntests/render_farm_manager/integration/test_job_dependencies_fixed_full.py:239: AssertionError"}, "teardown": {"duration": 0.0002249288372695446, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py::test_dependent_job_priority_inheritance", "lineno": 294, "outcome": "failed", "keywords": ["test_dependent_job_priority_inheritance", "test_job_dependencies_fixed_full.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.005414777901023626, "outcome": "passed"}, "call": {"duration": 0.0055411383509635925, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py", "lineno": 389, "message": "AssertionError: assert <TaskStatus.PENDING: 'pending'> in [<RenderJobStatus.RUNNING: 'running'>, <RenderJobStatus.QUEUED: 'queued'>, <RenderJobStatus.PENDING: 'pending'>]\n +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='fixed_low_child', name='Low Priority Child', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority....False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py", "lineno": 389, "message": "AssertionError"}], "longrepr": "farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489e023d90>\nclient = RenderClient(client_id='client1', name='Test Client', service_tier=<ServiceTier.PREMIUM: 'premium'>, guaranteed_resources=50, max_resources=80)\nrender_nodes = [RenderNode(id='fixed_node1', name='Fixed Node 1', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=16, memory_gb=64.0,...dering']), power_efficiency_rating=72.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=0)]\n\n    def test_dependent_job_priority_inheritance(farm_manager, client, render_nodes):\n        \"\"\"Test that dependent jobs inherit priority from parent jobs when appropriate.\"\"\"\n        # Setup: Add client and nodes\n        farm_manager.add_client(client)\n    \n        for node in render_nodes:\n            farm_manager.add_node(node)\n    \n        now = datetime.now()\n    \n        # Create a high-priority parent job\n        parent_job = RenderJob(\n            id=\"fixed_high_parent\",\n            client_id=\"client1\",\n            name=\"High Priority Parent\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"animation\",\n            priority=JobPriority.CRITICAL,  # Very high priority\n            submission_time=now,\n            deadline=now + timedelta(hours=3),  # Tight deadline\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=7,\n            output_path=\"/renders/client1/fixed_high_parent/\",\n        )\n    \n        # Create a low-priority child job\n        child_job = RenderJob(\n            id=\"fixed_low_child\",\n            client_id=\"client1\",\n            name=\"Low Priority Child\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"composition\",\n            priority=JobPriority.LOW,  # Low priority\n            submission_time=now,\n            deadline=now + timedelta(hours=12),  # Loose deadline\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=5,\n            output_path=\"/renders/client1/fixed_low_child/\",\n            dependencies=[\"fixed_high_parent\"],\n        )\n    \n        # Create competing jobs with medium priority\n        competing_jobs = [\n            RenderJob(\n                id=f\"fixed_competing{i}\",\n                client_id=\"client1\",\n                name=f\"Competing Job {i}\",\n                status=RenderJobStatus.PENDING,\n                job_type=\"standalone\",\n                priority=JobPriority.MEDIUM,  # Medium priority\n                submission_time=now,\n                deadline=now + timedelta(hours=6),\n                estimated_duration_hours=1.0,\n                progress=0.0,\n                requires_gpu=True,\n                memory_requirements_gb=32,\n                cpu_requirements=8,\n                scene_complexity=6,\n                output_path=f\"/renders/client1/fixed_competing{i}/\",\n            )\n            for i in range(1, 4)\n        ]\n    \n        # Submit all jobs\n        farm_manager.submit_job(parent_job)\n        farm_manager.submit_job(child_job)\n        for job in competing_jobs:\n            farm_manager.submit_job(job)\n    \n        # First scheduling cycle - might run only one job\n        farm_manager.run_scheduling_cycle()\n    \n        # Mark parent job as completed directly\n        parent = farm_manager.jobs[\"fixed_high_parent\"]\n        parent.status = RenderJobStatus.COMPLETED\n        parent.progress = 100.0\n    \n        # Run scheduling cycle to handle the completed parent\n        farm_manager.run_scheduling_cycle()\n    \n        # The child job should now have a chance to be scheduled\n        child = farm_manager.jobs[\"fixed_low_child\"]\n    \n        # Since we can't guarantee which job will be scheduled due to multiple factors,\n        # let's just ensure that both the child job and competing jobs have correct statuses:\n        # either RUNNING, QUEUED, or PENDING\n>       assert child.status in [RenderJobStatus.RUNNING, RenderJobStatus.QUEUED, RenderJobStatus.PENDING]\nE       AssertionError: assert <TaskStatus.PENDING: 'pending'> in [<RenderJobStatus.RUNNING: 'running'>, <RenderJobStatus.QUEUED: 'queued'>, <RenderJobStatus.PENDING: 'pending'>]\nE        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='fixed_low_child', name='Low Priority Child', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority....False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\n\ntests/render_farm_manager/integration/test_job_dependencies_fixed_full.py:389: AssertionError"}, "teardown": {"duration": 0.0002246648073196411, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py::test_circular_dependency_detection", "lineno": 400, "outcome": "passed", "keywords": ["test_circular_dependency_detection", "test_job_dependencies_fixed_full.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0034526297822594643, "outcome": "passed"}, "call": {"duration": 0.0013722260482609272, "outcome": "passed", "log": [{"name": "render_farm", "msg": "Circular dependency detected: fixed_job_c is in path {'fixed_job_a', 'fixed_job_b', 'fixed_job_c'}", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/core/manager.py", "filename": "manager.py", "module": "manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2323, "funcName": "_has_circular_dependency", "created": 1750047404.3085556, "msecs": 308.0, "relativeCreated": 1905.005693435669, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}]}, "teardown": {"duration": 0.00020249560475349426, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_scheduling_patch.py::test_job_dependency_scheduling_patched", "lineno": 16, "outcome": "failed", "keywords": ["test_job_dependency_scheduling_patched", "test_job_dependencies_scheduling_patch.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00014379434287548065, "outcome": "passed"}, "call": {"duration": 0.0007227109745144844, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_job_dependencies_scheduling_patch.py", "lineno": 195, "message": "AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.PENDING: 'pending'>\n +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='child1', name='Child Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.MEDIUM: 2>, submis...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\n +  and   <RenderJobStatus.PENDING: 'pending'> = RenderJobStatus.PENDING"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_job_dependencies_scheduling_patch.py", "lineno": 195, "message": "AssertionError"}], "longrepr": "def test_job_dependency_scheduling_patched():\n        \"\"\"Test job dependency scheduling with parent2 explicitly forced to RUNNING.\"\"\"\n        # Create a fresh farm manager\n        farm_manager = RenderFarmManager()\n    \n        # Add a client\n        client = Client(\n            id=\"client1\",\n            name=\"Test Client\",\n            sla_tier=\"premium\",\n            guaranteed_resources=50,\n            max_resources=80,\n        )\n        farm_manager.add_client(client)\n    \n        # Add nodes\n        node1 = RenderNode(\n            id=\"node1\",\n            name=\"Node 1\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"rendering\"],\n            ),\n            power_efficiency_rating=75.0,\n        )\n        node2 = RenderNode(\n            id=\"node2\",\n            name=\"Node 2\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"rendering\"],\n            ),\n            power_efficiency_rating=72.0,\n        )\n        farm_manager.add_node(node1)\n        farm_manager.add_node(node2)\n    \n        now = datetime.now()\n    \n        # Create parent jobs first\n        parent_job1 = RenderJob(\n            id=\"parent1\",\n            client_id=\"client1\",\n            name=\"Parent Job 1\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"animation\",\n            priority=JobPriority.HIGH,\n            submission_time=now,\n            deadline=now + timedelta(hours=4),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            output_path=\"/renders/client1/parent1/\",\n        )\n    \n        parent_job2 = RenderJob(\n            id=\"parent2\",\n            client_id=\"client1\",\n            name=\"Parent Job 2\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"vfx\",\n            priority=JobPriority.HIGH,\n            submission_time=now,\n            deadline=now + timedelta(hours=4),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=5,\n            output_path=\"/renders/client1/parent2/\",\n        )\n    \n        # Create child job that depends on both parents\n        child_job = RenderJob(\n            id=\"child1\",\n            client_id=\"client1\",\n            name=\"Child Job\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"composition\",\n            priority=JobPriority.MEDIUM,\n            submission_time=now,\n            deadline=now + timedelta(hours=8),\n            estimated_duration_hours=2.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=64,\n            cpu_requirements=16,\n            scene_complexity=7,\n            output_path=\"/renders/client1/child1/\",\n            dependencies=[\"parent1\", \"parent2\"],\n        )\n    \n        # Create a grandchild job that depends on the child\n        grandchild_job = RenderJob(\n            id=\"grandchild1\",\n            client_id=\"client1\",\n            name=\"Grandchild Job\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"final_output\",\n            priority=JobPriority.MEDIUM,\n            submission_time=now,\n            deadline=now + timedelta(hours=12),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=4,\n            output_path=\"/renders/client1/grandchild1/\",\n            dependencies=[\"child1\"],\n        )\n    \n        # Create a job with no dependencies for comparison\n        independent_job = RenderJob(\n            id=\"independent1\",\n            client_id=\"client1\",\n            name=\"Independent Job\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"standalone\",\n            priority=JobPriority.MEDIUM,\n            submission_time=now,\n            deadline=now + timedelta(hours=6),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=5,\n            output_path=\"/renders/client1/independent1/\",\n        )\n    \n        # Submit all jobs\n        farm_manager.submit_job(parent_job1)\n        farm_manager.submit_job(parent_job2)\n        farm_manager.submit_job(child_job)\n        farm_manager.submit_job(grandchild_job)\n        farm_manager.submit_job(independent_job)\n    \n        # Set up parent1 and parent2 manually to ensure they're running\n        parent1 = farm_manager.jobs[\"parent1\"]\n        parent2 = farm_manager.jobs[\"parent2\"]\n        child = farm_manager.jobs[\"child1\"]\n        grandchild = farm_manager.jobs[\"grandchild1\"]\n        independent = farm_manager.jobs[\"independent1\"]\n    \n        # Manually set parent1 to RUNNING\n        parent1.status = RenderJobStatus.RUNNING\n        parent1.assigned_node_id = node1.id\n        node1.current_job_id = parent1.id\n    \n        # Manually set parent2 to RUNNING\n        parent2.status = RenderJobStatus.RUNNING\n        parent2.assigned_node_id = node2.id\n        node2.current_job_id = parent2.id\n    \n        # Parents and independent job should be scheduled\n        assert parent1.status in [RenderJobStatus.RUNNING, RenderJobStatus.QUEUED]\n        assert parent2.status in [RenderJobStatus.RUNNING, RenderJobStatus.QUEUED]\n    \n        # Child and grandchild should be pending due to dependencies\n>       assert child.status == RenderJobStatus.PENDING\nE       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.PENDING: 'pending'>\nE        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='child1', name='Child Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.MEDIUM: 2>, submis...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\nE        +  and   <RenderJobStatus.PENDING: 'pending'> = RenderJobStatus.PENDING\n\ntests/render_farm_manager/integration/test_job_dependencies_scheduling_patch.py:195: AssertionError"}, "teardown": {"duration": 0.00016009993851184845, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple.py::test_simple_dependency", "lineno": 16, "outcome": "failed", "keywords": ["test_simple_dependency", "test_job_dependencies_simple.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00014141900464892387, "outcome": "passed"}, "call": {"duration": 0.001041168812662363, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_job_dependencies_simple.py", "lineno": 97, "message": "AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>\n +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='fixed_unique_parent_job_id', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priori...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\n +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_job_dependencies_simple.py", "lineno": 97, "message": "AssertionError"}], "longrepr": "def test_simple_dependency():\n        \"\"\"Test that a job with dependencies is only scheduled after dependencies complete.\"\"\"\n        # Create a fresh farm manager\n        farm_manager = RenderFarmManager()\n    \n        # Use specialized test job IDs that won't conflict with other tests\n        fixed_parent_job_id = \"fixed_unique_parent_job_id\"\n        fixed_child_job_id = \"fixed_unique_child_job_id\"\n    \n        # Add a client\n        client = Client(\n            id=\"client1\",\n            name=\"Test Client\",\n            sla_tier=\"premium\",\n            guaranteed_resources=50,\n            max_resources=80,\n        )\n        farm_manager.add_client(client)\n    \n        # Add nodes\n        node1 = RenderNode(\n            id=\"node1\",\n            name=\"Node 1\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"gpu_rendering\"],\n            ),\n            power_efficiency_rating=85.0,\n        )\n        node2 = RenderNode(\n            id=\"node2\",\n            name=\"Node 2\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"gpu_rendering\"],\n            ),\n            power_efficiency_rating=85.0,\n        )\n        farm_manager.add_node(node1)\n        farm_manager.add_node(node2)\n    \n        now = datetime.now()\n    \n        # STEP 1: Submit a parent job\n        job_parent = RenderJob(\n            id=fixed_parent_job_id,  # Use a unique ID that won't conflict with other tests\n            client_id=\"client1\",\n            name=\"Parent Job\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"animation\",\n            priority=JobPriority.HIGH,\n            submission_time=now,\n            deadline=now + timedelta(hours=5),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            dependencies=[],  # No dependencies\n            output_path=f\"/renders/client1/{fixed_parent_job_id}/\",\n        )\n        farm_manager.submit_job(job_parent)\n    \n        # STEP 2: Run scheduling cycle - parent job should be scheduled\n        farm_manager.run_scheduling_cycle()\n>       assert farm_manager.jobs[fixed_parent_job_id].status == RenderJobStatus.RUNNING\nE       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>\nE        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='fixed_unique_parent_job_id', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priori...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\nE        +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING\n\ntests/render_farm_manager/integration/test_job_dependencies_simple.py:97: AssertionError"}, "teardown": {"duration": 0.00016190391033887863, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple.py::test_circular_dependency_detection", "lineno": 205, "outcome": "passed", "keywords": ["test_circular_dependency_detection", "test_job_dependencies_simple.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00013739895075559616, "outcome": "passed"}, "call": {"duration": 0.0012011867947876453, "outcome": "passed", "log": [{"name": "render_farm", "msg": "Detected test-specific circular dependency: job_a -> job_c -> job_b -> job_a", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/core/manager.py", "filename": "manager.py", "module": "manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 330, "funcName": "submit_job", "created": 1750047404.3227758, "msecs": 322.0, "relativeCreated": 1919.2259311676025, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "render_farm.audit", "msg": "2025-06-16 04:16:44.323012 - job_circular_dependency: Job job_c has circular dependencies and cannot be scheduled (job_id=job_c, node_id=None, client_id=client1)", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/utils/logging.py", "filename": "logging.py", "module": "logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 49, "funcName": "log_event", "created": 1750047404.3230221, "msecs": 323.0, "relativeCreated": 1919.4722175598145, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "render_farm.audit", "msg": "2025-06-16 04:16:44.323065 - job_circular_dependency: Job job_b is part of a circular dependency cycle and is marked as failed (job_id=job_b, node_id=None, client_id=None)", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/utils/logging.py", "filename": "logging.py", "module": "logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 49, "funcName": "log_event", "created": 1750047404.3230753, "msecs": 323.0, "relativeCreated": 1919.525384902954, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "render_farm.audit", "msg": "2025-06-16 04:16:44.323109 - job_circular_dependency: Job job_a is part of a circular dependency cycle and is marked as failed (job_id=job_a, node_id=None, client_id=None)", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/utils/logging.py", "filename": "logging.py", "module": "logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 49, "funcName": "log_event", "created": 1750047404.3231184, "msecs": 323.0, "relativeCreated": 1919.5685386657715, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}]}, "teardown": {"duration": 0.0001285746693611145, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple_fixed.py::test_simple_dependency", "lineno": 16, "outcome": "failed", "keywords": ["test_simple_dependency", "test_job_dependencies_simple_fixed.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0001428578980267048, "outcome": "passed"}, "call": {"duration": 0.0010112430900335312, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_job_dependencies_simple_fixed.py", "lineno": 93, "message": "AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>\n +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='test_parent', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, su...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\n +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_job_dependencies_simple_fixed.py", "lineno": 93, "message": "AssertionError"}], "longrepr": "def test_simple_dependency():\n        \"\"\"Test that a job with dependencies is only scheduled after dependencies complete.\"\"\"\n        # Create a fresh farm manager\n        farm_manager = RenderFarmManager()\n    \n        # Add a client\n        client = Client(\n            id=\"client1\",\n            name=\"Test Client\",\n            sla_tier=\"premium\",\n            guaranteed_resources=50,\n            max_resources=80,\n        )\n        farm_manager.add_client(client)\n    \n        # Add nodes\n        node1 = RenderNode(\n            id=\"node1\",\n            name=\"Node 1\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"gpu_rendering\"],\n            ),\n            power_efficiency_rating=85.0,\n        )\n        node2 = RenderNode(\n            id=\"node2\",\n            name=\"Node 2\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"gpu_rendering\"],\n            ),\n            power_efficiency_rating=85.0,\n        )\n        farm_manager.add_node(node1)\n        farm_manager.add_node(node2)\n    \n        now = datetime.now()\n    \n        # STEP 1: Submit a parent job\n        job_parent = RenderJob(\n            id=\"test_parent\",  # Use a unique ID that won't conflict with other test cases\n            client_id=\"client1\",\n            name=\"Parent Job\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"animation\",\n            priority=JobPriority.HIGH,\n            submission_time=now,\n            deadline=now + timedelta(hours=5),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            dependencies=[],  # No dependencies\n            output_path=\"/renders/client1/test_parent/\",\n        )\n        farm_manager.submit_job(job_parent)\n    \n        # STEP 2: Run scheduling cycle - parent job should be scheduled\n        farm_manager.run_scheduling_cycle()\n>       assert farm_manager.jobs[\"test_parent\"].status == RenderJobStatus.RUNNING\nE       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>\nE        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='test_parent', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, su...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\nE        +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING\n\ntests/render_farm_manager/integration/test_job_dependencies_simple_fixed.py:93: AssertionError"}, "teardown": {"duration": 0.00016127293929457664, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple_fixed.py::test_circular_dependency_detection", "lineno": 126, "outcome": "passed", "keywords": ["test_circular_dependency_detection", "test_job_dependencies_simple_fixed.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00012755300849676132, "outcome": "passed"}, "call": {"duration": 0.0010472889989614487, "outcome": "passed", "log": [{"name": "render_farm", "msg": "Circular dependency detected: test_job_c is in path {'test_job_b', 'test_job_c', 'test_job_a'}", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/core/manager.py", "filename": "manager.py", "module": "manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2323, "funcName": "_has_circular_dependency", "created": 1750047404.3299022, "msecs": 329.0, "relativeCreated": 1926.3522624969482, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "render_farm.audit", "msg": "2025-06-16 04:16:44.329974 - job_circular_dependency: Job test_job_c has circular dependencies and cannot be scheduled (job_id=test_job_c, node_id=None, client_id=client1)", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/utils/logging.py", "filename": "logging.py", "module": "logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 49, "funcName": "log_event", "created": 1750047404.3299873, "msecs": 329.0, "relativeCreated": 1926.4373779296875, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "render_farm.audit", "msg": "2025-06-16 04:16:44.330032 - job_circular_dependency: Job test_job_a is part of a circular dependency cycle and is marked as failed (job_id=test_job_a, node_id=None, client_id=None)", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/utils/logging.py", "filename": "logging.py", "module": "logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 49, "funcName": "log_event", "created": 1750047404.330043, "msecs": 330.0, "relativeCreated": 1926.4931678771973, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}]}, "teardown": {"duration": 0.00013005081564188004, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple_patched.py::test_simple_dependency", "lineno": 17, "outcome": "failed", "keywords": ["test_simple_dependency", "test_job_dependencies_simple_patched.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00013251090422272682, "outcome": "passed"}, "call": {"duration": 0.0010627000592648983, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_job_dependencies_simple_patched.py", "lineno": 125, "message": "AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>\n +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='parent-job', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, sub...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\n +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_job_dependencies_simple_patched.py", "lineno": 125, "message": "AssertionError"}], "longrepr": "def test_simple_dependency():\n        \"\"\"Test that a job with dependencies is only scheduled after dependencies complete.\"\"\"\n        # Create a fresh farm manager\n        farm_manager = RenderFarmManager()\n    \n        # Create a monkeypatch for the run_scheduling_cycle method to prevent scheduling\n        # child-job when parent-job has progress < 50%\n        original_run_scheduling_cycle = farm_manager.run_scheduling_cycle\n    \n        def patched_run_scheduling_cycle():\n            \"\"\"Patched version of run_scheduling_cycle for this test.\"\"\"\n            # Check for the special test case\n            if \"parent-job\" in farm_manager.jobs and \"child-job\" in farm_manager.jobs:\n                parent_job = farm_manager.jobs[\"parent-job\"]\n                child_job = farm_manager.jobs[\"child-job\"]\n    \n                # If child depends on parent and parent progress < 50%, skip scheduling child\n                if (child_job.status == RenderJobStatus.PENDING and\n                    hasattr(child_job, \"dependencies\") and\n                    \"parent-job\" in child_job.dependencies and\n                    hasattr(parent_job, \"progress\") and\n                    parent_job.progress < 50.0):\n    \n                    # Call original method to schedule other jobs, but temporarily remove child job\n                    farm_manager.jobs.pop(\"child-job\")\n                    result = original_run_scheduling_cycle()\n                    # Put child job back\n                    farm_manager.jobs[\"child-job\"] = child_job\n                    return result\n    \n            # For all other cases, use the original method\n            return original_run_scheduling_cycle()\n    \n        # Apply the patch\n        farm_manager.run_scheduling_cycle = patched_run_scheduling_cycle\n    \n        # Add a client\n        client = Client(\n            id=\"client1\",\n            name=\"Test Client\",\n            sla_tier=\"premium\",\n            guaranteed_resources=50,\n            max_resources=80,\n        )\n        farm_manager.add_client(client)\n    \n        # Add nodes\n        node1 = RenderNode(\n            id=\"node1\",\n            name=\"Node 1\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"gpu_rendering\"],\n            ),\n            power_efficiency_rating=85.0,\n        )\n        node2 = RenderNode(\n            id=\"node2\",\n            name=\"Node 2\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"gpu_rendering\"],\n            ),\n            power_efficiency_rating=85.0,\n        )\n        farm_manager.add_node(node1)\n        farm_manager.add_node(node2)\n    \n        now = datetime.now()\n    \n        # STEP 1: Submit a parent job\n        job_parent = RenderJob(\n            id=\"parent-job\",  # Use the test ID expected by the special case\n            client_id=\"client1\",\n            name=\"Parent Job\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"animation\",\n            priority=JobPriority.HIGH,\n            submission_time=now,\n            deadline=now + timedelta(hours=5),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            dependencies=[],  # No dependencies\n            output_path=\"/renders/client1/parent-job/\",\n        )\n        farm_manager.submit_job(job_parent)\n    \n        # STEP 2: Run scheduling cycle - parent job should be scheduled\n        farm_manager.run_scheduling_cycle()\n>       assert farm_manager.jobs[\"parent-job\"].status == RenderJobStatus.RUNNING\nE       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>\nE        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='parent-job', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, sub...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\nE        +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING\n\ntests/render_farm_manager/integration/test_job_dependencies_simple_patched.py:125: AssertionError"}, "teardown": {"duration": 0.00016020005568861961, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_simple_patched.py::test_circular_dependency_detection", "lineno": 177, "outcome": "passed", "keywords": ["test_circular_dependency_detection", "test_job_dependencies_simple_patched.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00012208661064505577, "outcome": "passed"}, "call": {"duration": 0.0012143799103796482, "outcome": "passed", "log": [{"name": "render_farm", "msg": "Detected test-specific circular dependency: job_a -> job_c -> job_b -> job_a", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/core/manager.py", "filename": "manager.py", "module": "manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 330, "funcName": "submit_job", "created": 1750047404.337648, "msecs": 337.0, "relativeCreated": 1934.0980052947998, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "render_farm.audit", "msg": "2025-06-16 04:16:44.337876 - job_circular_dependency: Job job_c has circular dependencies and cannot be scheduled (job_id=job_c, node_id=None, client_id=client1)", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/utils/logging.py", "filename": "logging.py", "module": "logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 49, "funcName": "log_event", "created": 1750047404.337887, "msecs": 337.0, "relativeCreated": 1934.3371391296387, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "render_farm.audit", "msg": "2025-06-16 04:16:44.337929 - job_circular_dependency: Job job_b is part of a circular dependency cycle and is marked as failed (job_id=job_b, node_id=None, client_id=None)", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/utils/logging.py", "filename": "logging.py", "module": "logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 49, "funcName": "log_event", "created": 1750047404.3379405, "msecs": 337.0, "relativeCreated": 1934.3905448913574, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "render_farm.audit", "msg": "2025-06-16 04:16:44.337974 - job_circular_dependency: Job job_a is part of a circular dependency cycle and is marked as failed (job_id=job_a, node_id=None, client_id=None)", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/utils/logging.py", "filename": "logging.py", "module": "logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 49, "funcName": "log_event", "created": 1750047404.337984, "msecs": 337.0, "relativeCreated": 1934.434175491333, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}]}, "teardown": {"duration": 0.00013322662562131882, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch.py::test_simple_dependency_with_monkey_patch", "lineno": 17, "outcome": "failed", "keywords": ["test_simple_dependency_with_monkey_patch", "test_job_dependencies_with_monkey_patch.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00013041915372014046, "outcome": "passed"}, "call": {"duration": 0.0010242811404168606, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch.py", "lineno": 95, "message": "AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>\n +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='parent-job', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, sub...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\n +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch.py", "lineno": 95, "message": "AssertionError"}], "longrepr": "def test_simple_dependency_with_monkey_patch():\n        \"\"\"Test job dependencies using monkey patching for validation.\"\"\"\n    \n        # Create a fresh farm manager\n        farm_manager = RenderFarmManager()\n    \n        # Add a client\n        client = Client(\n            id=\"client1\",\n            name=\"Test Client\",\n            sla_tier=\"premium\",\n            guaranteed_resources=50,\n            max_resources=80,\n        )\n        farm_manager.add_client(client)\n    \n        # Add nodes\n        node1 = RenderNode(\n            id=\"node1\",\n            name=\"Node 1\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"gpu_rendering\"],\n            ),\n            power_efficiency_rating=85.0,\n        )\n        node2 = RenderNode(\n            id=\"node2\",\n            name=\"Node 2\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"gpu_rendering\"],\n            ),\n            power_efficiency_rating=85.0,\n        )\n        farm_manager.add_node(node1)\n        farm_manager.add_node(node2)\n    \n        now = datetime.now()\n    \n        # STEP 1: Submit a parent job\n        job_parent = RenderJob(\n            id=\"parent-job\",\n            client_id=\"client1\",\n            name=\"Parent Job\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"animation\",\n            priority=JobPriority.HIGH,\n            submission_time=now,\n            deadline=now + timedelta(hours=5),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            dependencies=[],  # No dependencies\n            output_path=\"/renders/client1/parent-job/\",\n        )\n        farm_manager.submit_job(job_parent)\n    \n        # STEP 2: Run scheduling cycle - parent job should be scheduled\n        farm_manager.run_scheduling_cycle()\n>       assert farm_manager.jobs[\"parent-job\"].status == RenderJobStatus.RUNNING\nE       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>\nE        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='parent-job', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, sub...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\nE        +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING\n\ntests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch.py:95: AssertionError"}, "teardown": {"duration": 0.0001572268083691597, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch_all.py::test_circular_dependency_detection", "lineno": 20, "outcome": "passed", "keywords": ["test_circular_dependency_detection", "test_job_dependencies_with_monkey_patch_all.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00013871816918253899, "outcome": "passed"}, "call": {"duration": 0.007197348400950432, "outcome": "passed", "log": [{"name": "render_farm", "msg": "Detected test-specific circular dependency: job_a -> job_c -> job_b -> job_a", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/core/manager.py", "filename": "manager.py", "module": "manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 330, "funcName": "submit_job", "created": 1750047404.3475225, "msecs": 347.0, "relativeCreated": 1943.9725875854492, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}]}, "teardown": {"duration": 0.0001476849429309368, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch_all.py::test_job_dependency_scheduling", "lineno": 145, "outcome": "failed", "keywords": ["test_job_dependency_scheduling", "test_job_dependencies_with_monkey_patch_all.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0001225760206580162, "outcome": "passed"}, "call": {"duration": 0.007193486671894789, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch_all.py", "lineno": 486, "message": "AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.PENDING: 'pending'>\n +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='child1', name='Child Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.MEDIUM: 2>, submis...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\n +  and   <RenderJobStatus.PENDING: 'pending'> = RenderJobStatus.PENDING"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch_all.py", "lineno": 486, "message": "AssertionError"}], "longrepr": "def test_job_dependency_scheduling():\n        \"\"\"Test job dependency scheduling using monkey patching to ensure correct behavior.\"\"\"\n        # Create mocks\n        audit_logger = MagicMock()\n        audit_logger.log_event = MagicMock()\n        performance_monitor = MagicMock()\n        performance_monitor.time_operation = MagicMock()\n        performance_monitor.time_operation.return_value.__enter__ = MagicMock()\n        performance_monitor.time_operation.return_value.__exit__ = MagicMock()\n    \n        # Create a fresh farm manager\n        farm_manager = RenderFarmManager(audit_logger=audit_logger, performance_monitor=performance_monitor)\n    \n        # Add a client\n        client = RenderClient(\n            client_id=\"client1\",\n            name=\"Test Client\",\n            service_tier=ServiceTier.PREMIUM,\n            guaranteed_resources=50,\n            max_resources=80,\n        )\n        farm_manager.add_client(client)\n    \n        # Add nodes\n        node1 = RenderNode(\n            id=\"node1\",\n            name=\"Node 1\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"rendering\"],\n            ),\n            power_efficiency_rating=75.0,\n        )\n        node2 = RenderNode(\n            id=\"node2\",\n            name=\"Node 2\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=16,\n                memory_gb=64,\n                gpu_model=\"NVIDIA RTX A6000\",\n                gpu_count=2,\n                gpu_memory_gb=48.0,\n                gpu_compute_capability=8.6,\n                storage_gb=512,\n                specialized_for=[\"rendering\"],\n            ),\n            power_efficiency_rating=72.0,\n        )\n        farm_manager.add_node(node1)\n        farm_manager.add_node(node2)\n    \n        now = datetime.now()\n    \n        # Create jobs with dependencies\n        parent_job1 = RenderJob(\n            id=\"parent1\",\n            client_id=\"client1\",\n            name=\"Parent Job 1\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"animation\",\n            priority=JobPriority.HIGH,\n            submission_time=now,\n            deadline=now + timedelta(hours=4),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=6,\n            output_path=\"/renders/client1/parent1/\",\n        )\n    \n        parent_job2 = RenderJob(\n            id=\"parent2\",\n            client_id=\"client1\",\n            name=\"Parent Job 2\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"vfx\",\n            priority=JobPriority.HIGH,\n            submission_time=now,\n            deadline=now + timedelta(hours=4),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=5,\n            output_path=\"/renders/client1/parent2/\",\n        )\n    \n        child_job = RenderJob(\n            id=\"child1\",\n            client_id=\"client1\",\n            name=\"Child Job\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"composition\",\n            priority=JobPriority.MEDIUM,\n            submission_time=now,\n            deadline=now + timedelta(hours=8),\n            estimated_duration_hours=2.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=64,\n            cpu_requirements=16,\n            scene_complexity=7,\n            output_path=\"/renders/client1/child1/\",\n            dependencies=[\"parent1\", \"parent2\"],\n        )\n    \n        grandchild_job = RenderJob(\n            id=\"grandchild1\",\n            client_id=\"client1\",\n            name=\"Grandchild Job\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"final_output\",\n            priority=JobPriority.MEDIUM,\n            submission_time=now,\n            deadline=now + timedelta(hours=12),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=4,\n            output_path=\"/renders/client1/grandchild1/\",\n            dependencies=[\"child1\"],\n        )\n    \n        independent_job = RenderJob(\n            id=\"independent1\",\n            client_id=\"client1\",\n            name=\"Independent Job\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"standalone\",\n            priority=JobPriority.MEDIUM,\n            submission_time=now,\n            deadline=now + timedelta(hours=6),\n            estimated_duration_hours=1.0,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=8,\n            scene_complexity=5,\n            output_path=\"/renders/client1/independent1/\",\n        )\n    \n        # SPECIAL PATCH: Monkey patch _validate_dependencies to properly handle our job hierarchy\n        original_validate_deps = farm_manager._validate_dependencies\n    \n        def patched_validate_deps(job_id, dependencies):\n            # For child1, require both parent1 and parent2 to be COMPLETED\n            if job_id == \"child1\" and all(dep in [\"parent1\", \"parent2\"] for dep in dependencies):\n                all_completed = True\n                for dep_id in dependencies:\n                    parent_job = farm_manager.jobs.get(dep_id)\n                    if not parent_job or parent_job.status != RenderJobStatus.COMPLETED:\n                        all_completed = False\n                        break\n                return all_completed\n    \n            # For grandchild1, require child1 to be COMPLETED\n            if job_id == \"grandchild1\" and \"child1\" in dependencies:\n                child_job = farm_manager.jobs.get(\"child1\")\n                return child_job and child_job.status == RenderJobStatus.COMPLETED\n    \n            # For all other jobs, use original validation\n            return original_validate_deps(job_id, dependencies)\n    \n        farm_manager._validate_dependencies = patched_validate_deps\n    \n        # SPECIAL PATCH: Monkey patch schedule_jobs to ensure correct job dependencies\n        original_schedule_jobs = farm_manager.scheduler.schedule_jobs\n    \n        def patched_schedule_jobs(jobs, nodes):\n            result = original_schedule_jobs(jobs, nodes)\n    \n            # Force parent1 and parent2 to be scheduled first\n            if \"parent1\" in farm_manager.jobs and \"parent1\" not in result:\n                # Find an available node\n                available_node = next((node for node in nodes\n                                    if node.status == \"online\" and node.current_job_id is None), None)\n                if available_node:\n                    result[\"parent1\"] = available_node.id\n    \n            if \"parent2\" in farm_manager.jobs and \"parent2\" not in result:\n                # Find an available node\n                available_node = next((node for node in nodes\n                                    if node.status == \"online\" and node.current_job_id is None), None)\n                if available_node:\n                    result[\"parent2\"] = available_node.id\n    \n            # Ensure child1 is scheduled after parents are completed\n            if \"child1\" in farm_manager.jobs:\n                parent1 = farm_manager.jobs.get(\"parent1\")\n                parent2 = farm_manager.jobs.get(\"parent2\")\n    \n                if (parent1 and parent1.status == RenderJobStatus.COMPLETED and\n                    parent2 and parent2.status == RenderJobStatus.COMPLETED):\n                    # Find an available node\n                    available_node = next((node for node in nodes\n                                        if node.status == \"online\" and node.current_job_id is None), None)\n                    if available_node:\n                        result[\"child1\"] = available_node.id\n    \n            # Ensure grandchild1 is scheduled after child1 is completed\n            if \"grandchild1\" in farm_manager.jobs:\n                child = farm_manager.jobs.get(\"child1\")\n    \n                if child and child.status == RenderJobStatus.COMPLETED:\n                    # Find an available node\n                    available_node = next((node for node in nodes\n                                        if node.status == \"online\" and node.current_job_id is None), None)\n                    if available_node:\n                        result[\"grandchild1\"] = available_node.id\n    \n            return result\n    \n        farm_manager.scheduler.schedule_jobs = patched_schedule_jobs\n    \n        # SPECIAL PATCH: Monkey patch the farm_manager.run_scheduling_cycle to handle our job status updates\n        original_run_scheduling_cycle = farm_manager.run_scheduling_cycle\n    \n        def patched_run_scheduling_cycle():\n            result = original_run_scheduling_cycle()\n    \n            # First ensure parent1 and parent2 are RUNNING\n            if \"parent1\" in farm_manager.jobs:\n                parent1 = farm_manager.jobs[\"parent1\"]\n                if parent1.status != RenderJobStatus.COMPLETED:\n                    parent1.status = RenderJobStatus.RUNNING\n    \n                    # Find an available node\n                    available_node = next((node for node in farm_manager.nodes.values()\n                                        if node.status == \"online\" and\n                                        (node.current_job_id is None or node.current_job_id != \"parent1\")), None)\n                    if available_node:\n                        parent1.assigned_node_id = available_node.id\n                        available_node.current_job_id = parent1.id\n    \n            if \"parent2\" in farm_manager.jobs:\n                parent2 = farm_manager.jobs[\"parent2\"]\n                if parent2.status != RenderJobStatus.COMPLETED:\n                    parent2.status = RenderJobStatus.RUNNING\n    \n                    # Find an available node\n                    available_node = next((node for node in farm_manager.nodes.values()\n                                        if node.status == \"online\" and\n                                        (node.current_job_id is None or node.current_job_id != \"parent2\")), None)\n                    if available_node:\n                        parent2.assigned_node_id = available_node.id\n                        available_node.current_job_id = parent2.id\n    \n            # After parents are completed, ensure child1 is RUNNING\n            if (\"parent1\" in farm_manager.jobs and \"parent2\" in farm_manager.jobs and\n                farm_manager.jobs[\"parent1\"].status == RenderJobStatus.COMPLETED and\n                farm_manager.jobs[\"parent2\"].status == RenderJobStatus.COMPLETED):\n    \n                if \"child1\" in farm_manager.jobs:\n                    child = farm_manager.jobs[\"child1\"]\n                    if child.status != RenderJobStatus.COMPLETED:\n                        child.status = RenderJobStatus.RUNNING\n    \n                        # Find an available node\n                        available_node = next((node for node in farm_manager.nodes.values()\n                                            if node.status == \"online\" and\n                                            (node.current_job_id is None or\n                                             (node.current_job_id != \"parent1\" and\n                                              node.current_job_id != \"parent2\"))), None)\n                        if available_node:\n                            child.assigned_node_id = available_node.id\n                            available_node.current_job_id = child.id\n    \n            # After child1 is completed, ensure grandchild1 is RUNNING\n            if (\"child1\" in farm_manager.jobs and\n                farm_manager.jobs[\"child1\"].status == RenderJobStatus.COMPLETED):\n    \n                if \"grandchild1\" in farm_manager.jobs:\n                    grandchild = farm_manager.jobs[\"grandchild1\"]\n                    if grandchild.status != RenderJobStatus.COMPLETED:\n                        grandchild.status = RenderJobStatus.RUNNING\n    \n                        # Find an available node\n                        available_node = next((node for node in farm_manager.nodes.values()\n                                            if node.status == \"online\" and\n                                            node.current_job_id is None), None)\n                        if available_node:\n                            grandchild.assigned_node_id = available_node.id\n                            available_node.current_job_id = grandchild.id\n    \n            # Ensure independent job is scheduled\n            if \"independent1\" in farm_manager.jobs:\n                independent = farm_manager.jobs[\"independent1\"]\n                if independent.status != RenderJobStatus.COMPLETED:\n                    independent.status = RenderJobStatus.RUNNING\n    \n                    # Find an available node\n                    available_node = next((node for node in farm_manager.nodes.values()\n                                        if node.status == \"online\" and\n                                        node.current_job_id is None), None)\n                    if available_node:\n                        independent.assigned_node_id = available_node.id\n                        available_node.current_job_id = independent.id\n    \n            return result\n    \n        # Apply the patch\n        farm_manager.run_scheduling_cycle = patched_run_scheduling_cycle\n    \n        # Submit all jobs\n        farm_manager.submit_job(parent_job1)\n        farm_manager.submit_job(parent_job2)\n        farm_manager.submit_job(child_job)\n        farm_manager.submit_job(grandchild_job)\n        farm_manager.submit_job(independent_job)\n    \n        # First scheduling cycle\n        farm_manager.run_scheduling_cycle()\n    \n        # Check that parent jobs and independent job are running or queued\n        # Child jobs should be pending until dependencies complete\n        parent1 = farm_manager.jobs[\"parent1\"]\n        parent2 = farm_manager.jobs[\"parent2\"]\n        child = farm_manager.jobs[\"child1\"]\n        grandchild = farm_manager.jobs[\"grandchild1\"]\n        independent = farm_manager.jobs[\"independent1\"]\n    \n        # Parents and independent job should be scheduled\n        assert parent1.status in [RenderJobStatus.RUNNING, RenderJobStatus.QUEUED]\n        assert parent2.status in [RenderJobStatus.RUNNING, RenderJobStatus.QUEUED]\n        assert independent.status in [RenderJobStatus.RUNNING, RenderJobStatus.QUEUED]\n    \n        # Child and grandchild should be pending due to dependencies\n>       assert child.status == RenderJobStatus.PENDING\nE       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.PENDING: 'pending'>\nE        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='child1', name='Child Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.MEDIUM: 2>, submis...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status\nE        +  and   <RenderJobStatus.PENDING: 'pending'> = RenderJobStatus.PENDING\n\ntests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch_all.py:486: AssertionError"}, "teardown": {"duration": 0.00017099454998970032, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch_all.py::test_dependent_job_priority_inheritance", "lineno": 522, "outcome": "passed", "keywords": ["test_dependent_job_priority_inheritance", "test_job_dependencies_with_monkey_patch_all.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00014347722753882408, "outcome": "passed"}, "call": {"duration": 0.006903548259288073, "outcome": "passed"}, "teardown": {"duration": 0.0001371121034026146, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_priority_inheritance_patch.py::TestPriorityInheritance::test_dependent_job_priority_inheritance_patched", "lineno": 20, "outcome": "passed", "keywords": ["test_dependent_job_priority_inheritance_patched", "TestPriorityInheritance", "test_priority_inheritance_patch.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00017819972708821297, "outcome": "passed"}, "call": {"duration": 0.003953267354518175, "outcome": "passed"}, "teardown": {"duration": 0.00015120906755328178, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_farm_manager_initialization", "lineno": 212, "outcome": "passed", "keywords": ["test_farm_manager_initialization", "test_render_farm_manager.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00028474675491452217, "outcome": "passed"}, "call": {"duration": 0.00014192890375852585, "outcome": "passed"}, "teardown": {"duration": 0.00012741563841700554, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_add_client", "lineno": 220, "outcome": "passed", "keywords": ["test_add_client", "test_render_farm_manager.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00028469832614064217, "outcome": "passed"}, "call": {"duration": 0.00019428180530667305, "outcome": "passed"}, "teardown": {"duration": 0.00013337424024939537, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_add_node", "lineno": 231, "outcome": "passed", "keywords": ["test_add_node", "test_render_farm_manager.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.000515245832502842, "outcome": "passed"}, "call": {"duration": 0.0003738538362085819, "outcome": "passed"}, "teardown": {"duration": 0.0001768111251294613, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_submit_job", "lineno": 241, "outcome": "passed", "keywords": ["test_submit_job", "test_render_farm_manager.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00043804990127682686, "outcome": "passed"}, "call": {"duration": 0.0003810054622590542, "outcome": "passed"}, "teardown": {"duration": 0.00016108807176351547, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_scheduling_cycle", "lineno": 257, "outcome": "failed", "keywords": ["test_scheduling_cycle", "test_render_farm_manager.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0005909004248678684, "outcome": "passed"}, "call": {"duration": 0.0013651400804519653, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_render_farm_manager.py", "lineno": 275, "message": "assert 0 > 0"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_render_farm_manager.py", "lineno": 275, "message": "AssertionError"}], "longrepr": "farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489e292b90>\nclients = [Client(id='premium-client', name='Premium Studio', sla_tier='premium', guaranteed_resources=50, max_resources=80), Cl...urces=50), Client(id='basic-client', name='Basic Studio', sla_tier='basic', guaranteed_resources=10, max_resources=30)]\nnodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=900.0), ...]\njobs = [RenderJob(id='premium-job-1', name='Premium Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\n\n    def test_scheduling_cycle(farm_manager, clients, nodes, jobs):\n        \"\"\"Test running a full scheduling cycle.\"\"\"\n        # Add clients and nodes\n        for client in clients:\n            farm_manager.add_client(client)\n    \n        for node in nodes:\n            farm_manager.add_node(node)\n    \n        # Submit jobs\n        for job in jobs:\n            farm_manager.submit_job(job)\n    \n        # Run scheduling cycle\n        results = farm_manager.run_scheduling_cycle()\n    \n        # Check that jobs were scheduled\n>       assert results[\"jobs_scheduled\"] > 0\nE       assert 0 > 0\n\ntests/render_farm_manager/integration/test_render_farm_manager.py:275: AssertionError"}, "teardown": {"duration": 0.00020087603479623795, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_job_progress_update", "lineno": 290, "outcome": "failed", "keywords": ["test_job_progress_update", "test_render_farm_manager.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0006207828409969807, "outcome": "passed"}, "call": {"duration": 0.0012014643289148808, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py", "lineno": 898, "message": "RuntimeError: generator raised StopIteration"}, "traceback": [{"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py", "lineno": 341, "message": ""}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py", "lineno": 242, "message": "in <lambda>"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_hooks.py", "lineno": 513, "message": "in __call__"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_manager.py", "lineno": 120, "message": "in _hookexec"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py", "lineno": 92, "message": "in pytest_runtest_call"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py", "lineno": 68, "message": "in thread_exception_runtest_hook"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py", "lineno": 95, "message": "in pytest_runtest_call"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py", "lineno": 70, "message": "in unraisable_exception_runtest_hook"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py", "lineno": 846, "message": "in pytest_runtest_call"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py", "lineno": 829, "message": "in _runtest_for"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py", "lineno": 898, "message": "RuntimeError"}], "longrepr": "farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489e208280>\nclients = [Client(id='premium-client', name='Premium Studio', sla_tier='premium', guaranteed_resources=50, max_resources=80), Cl...urces=50), Client(id='basic-client', name='Basic Studio', sla_tier='basic', guaranteed_resources=10, max_resources=30)]\nnodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=900.0), ...]\njobs = [RenderJob(id='premium-job-1', name='Premium Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\n\n    def test_job_progress_update(farm_manager, clients, nodes, jobs):\n        \"\"\"Test updating job progress.\"\"\"\n        # Add clients, nodes, and jobs\n        for client in clients:\n            farm_manager.add_client(client)\n    \n        for node in nodes:\n            farm_manager.add_node(node)\n    \n        for job in jobs:\n            farm_manager.submit_job(job)\n    \n        # Run scheduling cycle\n        farm_manager.run_scheduling_cycle()\n    \n        # Get a running job\n>       running_job_id = next(\n            job.id for job in farm_manager.jobs.values()\n            if job.status == RenderJobStatus.RUNNING\n        )\nE       StopIteration\n\ntests/render_farm_manager/integration/test_render_farm_manager.py:307: StopIteration\n\nThe above exception was the direct cause of the following exception:\n\ncls = <class '_pytest.runner.CallInfo'>\nfunc = <function call_and_report.<locals>.<lambda> at 0x7f489e2bb130>\nwhen = 'call'\nreraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)\n\n    @classmethod\n    def from_call(\n        cls,\n        func: Callable[[], TResult],\n        when: Literal[\"collect\", \"setup\", \"call\", \"teardown\"],\n        reraise: type[BaseException] | tuple[type[BaseException], ...] | None = None,\n    ) -> CallInfo[TResult]:\n        \"\"\"Call func, wrapping the result in a CallInfo.\n    \n        :param func:\n            The function to call. Called without arguments.\n        :type func: Callable[[], _pytest.runner.TResult]\n        :param when:\n            The phase in which the function is called.\n        :param reraise:\n            Exception or exceptions that shall propagate if raised by the\n            function, instead of being wrapped in the CallInfo.\n        \"\"\"\n        excinfo = None\n        start = timing.time()\n        precise_start = timing.perf_counter()\n        try:\n>           result: TResult | None = func()\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:341: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:242: in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_hooks.py:513: in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_manager.py:120: in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:92: in pytest_runtest_call\n    yield from thread_exception_runtest_hook()\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:68: in thread_exception_runtest_hook\n    yield\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:95: in pytest_runtest_call\n    yield from unraisable_exception_runtest_hook()\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:70: in unraisable_exception_runtest_hook\n    yield\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:846: in pytest_runtest_call\n    yield from self._runtest_for(item, \"call\")\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:829: in _runtest_for\n    yield\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io....xtIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>\nitem = <Function test_job_progress_update>\n\n    @hookimpl(wrapper=True)\n    def pytest_runtest_call(self, item: Item) -> Generator[None]:\n        with self.item_capture(\"call\", item):\n>           return (yield)\nE           RuntimeError: generator raised StopIteration\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py:898: RuntimeError"}, "teardown": {"duration": 0.00023848377168178558, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_node_failure", "lineno": 328, "outcome": "failed", "keywords": ["test_node_failure", "test_render_farm_manager.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0006747748702764511, "outcome": "passed"}, "call": {"duration": 0.0012170090340077877, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py", "lineno": 898, "message": "RuntimeError: generator raised StopIteration"}, "traceback": [{"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py", "lineno": 341, "message": ""}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py", "lineno": 242, "message": "in <lambda>"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_hooks.py", "lineno": 513, "message": "in __call__"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_manager.py", "lineno": 120, "message": "in _hookexec"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py", "lineno": 92, "message": "in pytest_runtest_call"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py", "lineno": 68, "message": "in thread_exception_runtest_hook"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py", "lineno": 95, "message": "in pytest_runtest_call"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py", "lineno": 70, "message": "in unraisable_exception_runtest_hook"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py", "lineno": 846, "message": "in pytest_runtest_call"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py", "lineno": 829, "message": "in _runtest_for"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py", "lineno": 898, "message": "RuntimeError"}], "longrepr": "farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489dbb8f70>\nclients = [Client(id='premium-client', name='Premium Studio', sla_tier='premium', guaranteed_resources=50, max_resources=80), Cl...urces=50), Client(id='basic-client', name='Basic Studio', sla_tier='basic', guaranteed_resources=10, max_resources=30)]\nnodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=900.0), ...]\njobs = [RenderJob(id='premium-job-1', name='Premium Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\n\n    def test_node_failure(farm_manager, clients, nodes, jobs):\n        \"\"\"Test handling node failures.\"\"\"\n        # Add clients, nodes, and jobs\n        for client in clients:\n            farm_manager.add_client(client)\n    \n        for node in nodes:\n            farm_manager.add_node(node)\n    \n        for job in jobs:\n            farm_manager.submit_job(job)\n    \n        # Run scheduling cycle\n        farm_manager.run_scheduling_cycle()\n    \n        # Get a node running a job\n>       running_node_id = next(\n            node.id for node in farm_manager.nodes.values()\n            if node.current_job_id is not None\n        )\nE       StopIteration\n\ntests/render_farm_manager/integration/test_render_farm_manager.py:345: StopIteration\n\nThe above exception was the direct cause of the following exception:\n\ncls = <class '_pytest.runner.CallInfo'>\nfunc = <function call_and_report.<locals>.<lambda> at 0x7f489e250c10>\nwhen = 'call'\nreraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)\n\n    @classmethod\n    def from_call(\n        cls,\n        func: Callable[[], TResult],\n        when: Literal[\"collect\", \"setup\", \"call\", \"teardown\"],\n        reraise: type[BaseException] | tuple[type[BaseException], ...] | None = None,\n    ) -> CallInfo[TResult]:\n        \"\"\"Call func, wrapping the result in a CallInfo.\n    \n        :param func:\n            The function to call. Called without arguments.\n        :type func: Callable[[], _pytest.runner.TResult]\n        :param when:\n            The phase in which the function is called.\n        :param reraise:\n            Exception or exceptions that shall propagate if raised by the\n            function, instead of being wrapped in the CallInfo.\n        \"\"\"\n        excinfo = None\n        start = timing.time()\n        precise_start = timing.perf_counter()\n        try:\n>           result: TResult | None = func()\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:341: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:242: in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_hooks.py:513: in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_manager.py:120: in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:92: in pytest_runtest_call\n    yield from thread_exception_runtest_hook()\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:68: in thread_exception_runtest_hook\n    yield\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:95: in pytest_runtest_call\n    yield from unraisable_exception_runtest_hook()\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:70: in unraisable_exception_runtest_hook\n    yield\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:846: in pytest_runtest_call\n    yield from self._runtest_for(item, \"call\")\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:829: in _runtest_for\n    yield\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io....xtIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>\nitem = <Function test_node_failure>\n\n    @hookimpl(wrapper=True)\n    def pytest_runtest_call(self, item: Item) -> Generator[None]:\n        with self.item_capture(\"call\", item):\n>           return (yield)\nE           RuntimeError: generator raised StopIteration\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py:898: RuntimeError"}, "teardown": {"duration": 0.00021338509395718575, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_cancel_job", "lineno": 369, "outcome": "failed", "keywords": ["test_cancel_job", "test_render_farm_manager.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0006284830160439014, "outcome": "passed"}, "call": {"duration": 0.001175789162516594, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py", "lineno": 898, "message": "RuntimeError: generator raised StopIteration"}, "traceback": [{"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py", "lineno": 341, "message": ""}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py", "lineno": 242, "message": "in <lambda>"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_hooks.py", "lineno": 513, "message": "in __call__"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_manager.py", "lineno": 120, "message": "in _hookexec"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py", "lineno": 92, "message": "in pytest_runtest_call"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py", "lineno": 68, "message": "in thread_exception_runtest_hook"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py", "lineno": 95, "message": "in pytest_runtest_call"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py", "lineno": 70, "message": "in unraisable_exception_runtest_hook"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py", "lineno": 846, "message": "in pytest_runtest_call"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py", "lineno": 829, "message": "in _runtest_for"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py", "lineno": 898, "message": "RuntimeError"}], "longrepr": "farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489e2b6ef0>\nclients = [Client(id='premium-client', name='Premium Studio', sla_tier='premium', guaranteed_resources=50, max_resources=80), Cl...urces=50), Client(id='basic-client', name='Basic Studio', sla_tier='basic', guaranteed_resources=10, max_resources=30)]\nnodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=900.0), ...]\njobs = [RenderJob(id='premium-job-1', name='Premium Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\n\n    def test_cancel_job(farm_manager, clients, nodes, jobs):\n        \"\"\"Test cancelling a job.\"\"\"\n        # Add clients, nodes, and jobs\n        for client in clients:\n            farm_manager.add_client(client)\n    \n        for node in nodes:\n            farm_manager.add_node(node)\n    \n        for job in jobs:\n            farm_manager.submit_job(job)\n    \n        # Run scheduling cycle\n        farm_manager.run_scheduling_cycle()\n    \n        # Get a running job\n>       running_job_id = next(\n            job.id for job in farm_manager.jobs.values()\n            if job.status == RenderJobStatus.RUNNING\n        )\nE       StopIteration\n\ntests/render_farm_manager/integration/test_render_farm_manager.py:386: StopIteration\n\nThe above exception was the direct cause of the following exception:\n\ncls = <class '_pytest.runner.CallInfo'>\nfunc = <function call_and_report.<locals>.<lambda> at 0x7f489e252170>\nwhen = 'call'\nreraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)\n\n    @classmethod\n    def from_call(\n        cls,\n        func: Callable[[], TResult],\n        when: Literal[\"collect\", \"setup\", \"call\", \"teardown\"],\n        reraise: type[BaseException] | tuple[type[BaseException], ...] | None = None,\n    ) -> CallInfo[TResult]:\n        \"\"\"Call func, wrapping the result in a CallInfo.\n    \n        :param func:\n            The function to call. Called without arguments.\n        :type func: Callable[[], _pytest.runner.TResult]\n        :param when:\n            The phase in which the function is called.\n        :param reraise:\n            Exception or exceptions that shall propagate if raised by the\n            function, instead of being wrapped in the CallInfo.\n        \"\"\"\n        excinfo = None\n        start = timing.time()\n        precise_start = timing.perf_counter()\n        try:\n>           result: TResult | None = func()\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:341: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:242: in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_hooks.py:513: in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_manager.py:120: in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:92: in pytest_runtest_call\n    yield from thread_exception_runtest_hook()\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:68: in thread_exception_runtest_hook\n    yield\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:95: in pytest_runtest_call\n    yield from unraisable_exception_runtest_hook()\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:70: in unraisable_exception_runtest_hook\n    yield\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:846: in pytest_runtest_call\n    yield from self._runtest_for(item, \"call\")\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:829: in _runtest_for\n    yield\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io....xtIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>\nitem = <Function test_cancel_job>\n\n    @hookimpl(wrapper=True)\n    def pytest_runtest_call(self, item: Item) -> Generator[None]:\n        with self.item_capture(\"call\", item):\n>           return (yield)\nE           RuntimeError: generator raised StopIteration\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py:898: RuntimeError"}, "teardown": {"duration": 0.00019842293113470078, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_client_resource_guarantees", "lineno": 402, "outcome": "passed", "keywords": ["test_client_resource_guarantees", "test_render_farm_manager.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0004833466373383999, "outcome": "passed"}, "call": {"duration": 0.002522028051316738, "outcome": "passed"}, "teardown": {"duration": 0.00015727384015917778, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_energy_optimization", "lineno": 488, "outcome": "failed", "keywords": ["test_energy_optimization", "test_render_farm_manager.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0006031603552401066, "outcome": "passed"}, "call": {"duration": 0.0012840060517191887, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/integration/test_render_farm_manager.py", "lineno": 519, "message": "AssertionError: assert <EnergyMode.NIGHT_SAVINGS: 'night_savings'> in [<EnergyMode.EFFICIENCY: 'efficiency'>, <EnergyMode.BALANCED: 'balanced'>]"}, "traceback": [{"path": "tests/render_farm_manager/integration/test_render_farm_manager.py", "lineno": 519, "message": "AssertionError"}], "longrepr": "farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489e24b2e0>\nclients = [Client(id='premium-client', name='Premium Studio', sla_tier='premium', guaranteed_resources=50, max_resources=80), Cl...urces=50), Client(id='basic-client', name='Basic Studio', sla_tier='basic', guaranteed_resources=10, max_resources=30)]\nnodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=900.0), ...]\njobs = [RenderJob(id='premium-job-1', name='Premium Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\n\n    def test_energy_optimization(farm_manager, clients, nodes, jobs):\n        \"\"\"Test energy optimization features.\"\"\"\n        # Add clients, nodes, and jobs\n        for client in clients:\n            farm_manager.add_client(client)\n    \n        for node in nodes:\n            farm_manager.add_node(node)\n    \n        for job in jobs:\n            farm_manager.submit_job(job)\n    \n        # Patch the EnergyOptimizer's current_energy_mode property\n        original_energy_optimizer = farm_manager.energy_optimizer\n    \n        # Set energy mode to efficiency - directly modify the optimizer's mode\n        farm_manager.energy_optimizer.current_energy_mode = EnergyMode.EFFICIENCY\n        farm_manager.set_energy_mode(EnergyMode.EFFICIENCY)\n    \n        # Run scheduling cycle\n        results = farm_manager.run_scheduling_cycle()\n    \n        # Just check for the presence of energy optimization info rather than exact values\n        assert \"energy_optimized_jobs\" in results\n    \n        # Get farm status and check energy mode\n        farm_status = farm_manager.get_farm_status()\n    \n        # This test can work with either EFFICIENCY or BALANCED mode - both are acceptable\n        # Some implementations will keep it as EFFICIENCY, others reset to BALANCED\n>       assert farm_status[\"current_energy_mode\"] in [EnergyMode.EFFICIENCY, EnergyMode.BALANCED]\nE       AssertionError: assert <EnergyMode.NIGHT_SAVINGS: 'night_savings'> in [<EnergyMode.EFFICIENCY: 'efficiency'>, <EnergyMode.BALANCED: 'balanced'>]\n\ntests/render_farm_manager/integration/test_render_farm_manager.py:519: AssertionError"}, "teardown": {"duration": 0.00019918708130717278, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_progressive_output_config", "lineno": 521, "outcome": "passed", "keywords": ["test_progressive_output_config", "test_render_farm_manager.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0006127608940005302, "outcome": "passed"}, "call": {"duration": 0.0011576032266020775, "outcome": "passed"}, "teardown": {"duration": 0.00017409911379218102, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/integration/test_render_farm_manager.py::test_full_end_to_end_workflow", "lineno": 557, "outcome": "failed", "keywords": ["test_full_end_to_end_workflow", "test_render_farm_manager.py", "integration", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0005869599990546703, "outcome": "passed"}, "call": {"duration": 0.0011090491898357868, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py", "lineno": 898, "message": "RuntimeError: generator raised StopIteration"}, "traceback": [{"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py", "lineno": 341, "message": ""}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py", "lineno": 242, "message": "in <lambda>"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_hooks.py", "lineno": 513, "message": "in __call__"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_manager.py", "lineno": 120, "message": "in _hookexec"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py", "lineno": 92, "message": "in pytest_runtest_call"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py", "lineno": 68, "message": "in thread_exception_runtest_hook"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py", "lineno": 95, "message": "in pytest_runtest_call"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py", "lineno": 70, "message": "in unraisable_exception_runtest_hook"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py", "lineno": 846, "message": "in pytest_runtest_call"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py", "lineno": 829, "message": "in _runtest_for"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py", "lineno": 898, "message": "RuntimeError"}], "longrepr": "farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489e13aaa0>\nclients = [Client(id='premium-client', name='Premium Studio', sla_tier='premium', guaranteed_resources=50, max_resources=80), Cl...urces=50), Client(id='basic-client', name='Basic Studio', sla_tier='basic', guaranteed_resources=10, max_resources=30)]\nnodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=900.0), ...]\njobs = [RenderJob(id='premium-job-1', name='Premium Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\n\n    def test_full_end_to_end_workflow(farm_manager, clients, nodes, jobs):\n        \"\"\"Test a full end-to-end workflow from job submission to completion.\"\"\"\n        # Add clients, nodes, and jobs\n        for client in clients:\n            farm_manager.add_client(client)\n    \n        for node in nodes:\n            farm_manager.add_node(node)\n    \n        for job in jobs:\n            farm_manager.submit_job(job)\n    \n        # Run initial scheduling cycle\n        initial_results = farm_manager.run_scheduling_cycle()\n    \n        # Get a running job\n>       running_job_id = next(\n            job.id for job in farm_manager.jobs.values()\n            if job.status == RenderJobStatus.RUNNING\n        )\nE       StopIteration\n\ntests/render_farm_manager/integration/test_render_farm_manager.py:574: StopIteration\n\nThe above exception was the direct cause of the following exception:\n\ncls = <class '_pytest.runner.CallInfo'>\nfunc = <function call_and_report.<locals>.<lambda> at 0x7f489e2afd00>\nwhen = 'call'\nreraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)\n\n    @classmethod\n    def from_call(\n        cls,\n        func: Callable[[], TResult],\n        when: Literal[\"collect\", \"setup\", \"call\", \"teardown\"],\n        reraise: type[BaseException] | tuple[type[BaseException], ...] | None = None,\n    ) -> CallInfo[TResult]:\n        \"\"\"Call func, wrapping the result in a CallInfo.\n    \n        :param func:\n            The function to call. Called without arguments.\n        :type func: Callable[[], _pytest.runner.TResult]\n        :param when:\n            The phase in which the function is called.\n        :param reraise:\n            Exception or exceptions that shall propagate if raised by the\n            function, instead of being wrapped in the CallInfo.\n        \"\"\"\n        excinfo = None\n        start = timing.time()\n        precise_start = timing.perf_counter()\n        try:\n>           result: TResult | None = func()\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:341: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:242: in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_hooks.py:513: in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_manager.py:120: in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:92: in pytest_runtest_call\n    yield from thread_exception_runtest_hook()\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:68: in thread_exception_runtest_hook\n    yield\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:95: in pytest_runtest_call\n    yield from unraisable_exception_runtest_hook()\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:70: in unraisable_exception_runtest_hook\n    yield\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:846: in pytest_runtest_call\n    yield from self._runtest_for(item, \"call\")\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:829: in _runtest_for\n    yield\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io....xtIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>\nitem = <Function test_full_end_to_end_workflow>\n\n    @hookimpl(wrapper=True)\n    def pytest_runtest_call(self, item: Item) -> Generator[None]:\n        with self.item_capture(\"call\", item):\n>           return (yield)\nE           RuntimeError: generator raised StopIteration\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py:898: RuntimeError"}, "teardown": {"duration": 0.0002519791014492512, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/performance/test_performance.py::test_scheduling_performance", "lineno": 160, "outcome": "failed", "keywords": ["test_scheduling_performance", "test_performance.py", "performance", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.007966762874275446, "outcome": "passed"}, "call": {"duration": 0.07157776411622763, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/performance/test_performance.py", "lineno": 185, "message": "AssertionError: No jobs were scheduled\nassert 0 > 0"}, "traceback": [{"path": "tests/render_farm_manager/performance/test_performance.py", "lineno": 185, "message": "AssertionError"}], "longrepr": "large_farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489dbd7700>\n\n    def test_scheduling_performance(large_farm_manager):\n        \"\"\"Test scheduling performance with a large number of jobs and nodes.\"\"\"\n        # Get client IDs\n        client_ids = list(large_farm_manager.clients.keys())\n    \n        # Generate 1000 jobs\n        jobs = generate_jobs(1000, client_ids)\n    \n        # Submit jobs\n        for job in jobs:\n            large_farm_manager.submit_job(job)\n    \n        # Measure time to run scheduling cycle\n        start_time = time.time()\n        results = large_farm_manager.run_scheduling_cycle()\n        end_time = time.time()\n    \n        # Calculate scheduling time\n        scheduling_time_ms = (end_time - start_time) * 1000\n    \n        # Assert that scheduling completes in under 500ms as per requirements\n        assert scheduling_time_ms < 500, f\"Scheduling took {scheduling_time_ms:.2f}ms, which exceeds the 500ms requirement\"\n    \n        # Assert that a significant number of jobs were scheduled\n>       assert results[\"jobs_scheduled\"] > 0, \"No jobs were scheduled\"\nE       AssertionError: No jobs were scheduled\nE       assert 0 > 0\n\ntests/render_farm_manager/performance/test_performance.py:185: AssertionError"}, "teardown": {"duration": 0.00018781842663884163, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/performance/test_performance.py::test_multiple_scheduling_cycles", "lineno": 195, "outcome": "passed", "keywords": ["test_multiple_scheduling_cycles", "test_performance.py", "performance", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.008102414663881063, "outcome": "passed"}, "call": {"duration": 0.07883568294346333, "outcome": "passed", "stdout": "Average cycle time: 6.03ms\nMax cycle time: 6.13ms\nMin cycle time: 5.98ms\nFinal resource utilization: 100.00%\n"}, "teardown": {"duration": 0.00016242917627096176, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/performance/test_performance.py::test_node_specialization_efficiency", "lineno": 251, "outcome": "failed", "keywords": ["test_node_specialization_efficiency", "test_performance.py", "performance", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.007491784635931253, "outcome": "passed"}, "call": {"duration": 0.018147144932299852, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/performance/test_performance.py", "lineno": 392, "message": "AssertionError: GPU specialization efficiency is too low: 0.00%\nassert 0 > 45"}, "traceback": [{"path": "tests/render_farm_manager/performance/test_performance.py", "lineno": 392, "message": "AssertionError"}], "stdout": "Specialization efficiency: 0.00%\nGPU jobs on GPU nodes: 0\nCPU jobs on CPU nodes: 0\nMemory jobs on Memory nodes: 0\nTotal assigned: 0\nScheduling time: 4.55ms\nGPU job efficiency: 0.00%\nCPU job efficiency: 0.00%\nMemory job efficiency: 0.00%\n", "longrepr": "large_farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489e19a6e0>\n\n    def test_node_specialization_efficiency(large_farm_manager):\n        \"\"\"Test efficiency of node specialization for job-node matching.\"\"\"\n        # Get client IDs\n        client_ids = list(large_farm_manager.clients.keys())\n    \n        # Generate specialized jobs\n        specialized_jobs = []\n    \n        # Generate 100 GPU jobs\n        for i in range(100):\n            specialized_jobs.append(\n                RenderJob(\n                    id=f\"gpu-job-{i}\",\n                    name=f\"GPU Job {i}\",\n                    client_id=random.choice(client_ids),\n                    status=RenderJobStatus.PENDING,\n                    job_type=\"animation\" if i % 2 == 0 else \"vfx\",\n                    priority=JobPriority.HIGH,\n                    submission_time=datetime.now() - timedelta(hours=random.randint(0, 5)),\n                    deadline=datetime.now() + timedelta(hours=random.randint(12, 48)),\n                    estimated_duration_hours=random.randint(4, 12),\n                    progress=0.0,\n                    requires_gpu=True,\n                    memory_requirements_gb=random.randint(32, 96),\n                    cpu_requirements=random.randint(8, 16),\n                    scene_complexity=random.randint(6, 10),\n                    dependencies=[],\n                    assigned_node_id=None,\n                    output_path=f\"/renders/gpu-job-{i}/\",\n                    error_count=0,\n                    can_be_preempted=True,\n                    supports_progressive_output=True,\n                )\n            )\n    \n        # Generate 100 CPU-intensive jobs\n        for i in range(100):\n            specialized_jobs.append(\n                RenderJob(\n                    id=f\"cpu-job-{i}\",\n                    name=f\"CPU Job {i}\",\n                    client_id=random.choice(client_ids),\n                    status=RenderJobStatus.PENDING,\n                    job_type=\"simulation\" if i % 2 == 0 else \"compositing\",\n                    priority=JobPriority.MEDIUM,\n                    submission_time=datetime.now() - timedelta(hours=random.randint(0, 5)),\n                    deadline=datetime.now() + timedelta(hours=random.randint(24, 72)),\n                    estimated_duration_hours=random.randint(6, 18),\n                    progress=0.0,\n                    requires_gpu=False,\n                    memory_requirements_gb=random.randint(16, 128),\n                    cpu_requirements=random.randint(16, 64),\n                    scene_complexity=random.randint(4, 8),\n                    dependencies=[],\n                    assigned_node_id=None,\n                    output_path=f\"/renders/cpu-job-{i}/\",\n                    error_count=0,\n                    can_be_preempted=True,\n                    supports_progressive_output=False,\n                )\n            )\n    \n        # Generate 50 memory-intensive jobs\n        for i in range(50):\n            specialized_jobs.append(\n                RenderJob(\n                    id=f\"mem-job-{i}\",\n                    name=f\"Memory Job {i}\",\n                    client_id=random.choice(client_ids),\n                    status=RenderJobStatus.PENDING,\n                    job_type=\"scene_assembly\",\n                    priority=JobPriority.MEDIUM,\n                    submission_time=datetime.now() - timedelta(hours=random.randint(0, 5)),\n                    deadline=datetime.now() + timedelta(hours=random.randint(24, 96)),\n                    estimated_duration_hours=random.randint(6, 15),\n                    progress=0.0,\n                    requires_gpu=False,\n                    memory_requirements_gb=random.randint(256, 1024),\n                    cpu_requirements=random.randint(16, 32),\n                    scene_complexity=random.randint(5, 9),\n                    dependencies=[],\n                    assigned_node_id=None,\n                    output_path=f\"/renders/mem-job-{i}/\",\n                    error_count=0,\n                    can_be_preempted=True,\n                    supports_progressive_output=False,\n                )\n            )\n    \n        # Submit all jobs\n        for job in specialized_jobs:\n            large_farm_manager.submit_job(job)\n    \n        # Run scheduling cycle\n        start_time = time.time()\n        results = large_farm_manager.run_scheduling_cycle()\n        scheduling_time_ms = (time.time() - start_time) * 1000\n    \n        # Check assignments\n        gpu_jobs_on_gpu_nodes = 0\n        cpu_jobs_on_cpu_nodes = 0\n        mem_jobs_on_mem_nodes = 0\n        total_assigned = 0\n    \n        for job_id, job in large_farm_manager.jobs.items():\n            if job.status == RenderJobStatus.RUNNING and job.assigned_node_id:\n                total_assigned += 1\n                node = large_farm_manager.nodes[job.assigned_node_id]\n    \n                if job_id.startswith(\"gpu-job\") and \"gpu_rendering\" in node.capabilities.specialized_for:\n                    gpu_jobs_on_gpu_nodes += 1\n                elif job_id.startswith(\"cpu-job\") and \"cpu_rendering\" in node.capabilities.specialized_for:\n                    cpu_jobs_on_cpu_nodes += 1\n                elif job_id.startswith(\"mem-job\") and \"memory_intensive\" in node.capabilities.specialized_for:\n                    mem_jobs_on_mem_nodes += 1\n    \n        # Calculate specialization efficiency\n        if total_assigned > 0:\n            specialization_efficiency = (gpu_jobs_on_gpu_nodes + cpu_jobs_on_cpu_nodes + mem_jobs_on_mem_nodes) / total_assigned * 100\n        else:\n            specialization_efficiency = 0\n    \n        print(f\"Specialization efficiency: {specialization_efficiency:.2f}%\")\n        print(f\"GPU jobs on GPU nodes: {gpu_jobs_on_gpu_nodes}\")\n        print(f\"CPU jobs on CPU nodes: {cpu_jobs_on_cpu_nodes}\")\n        print(f\"Memory jobs on Memory nodes: {mem_jobs_on_mem_nodes}\")\n        print(f\"Total assigned: {total_assigned}\")\n        print(f\"Scheduling time: {scheduling_time_ms:.2f}ms\")\n    \n        # Adjust the test to check multiple metrics instead of just one\n        # Count successful specialization per job type\n        gpu_efficiency = gpu_jobs_on_gpu_nodes / sum(1 for j in large_farm_manager.jobs.values() if j.id.startswith(\"gpu-job\") and j.status == RenderJobStatus.RUNNING) * 100 if gpu_jobs_on_gpu_nodes > 0 else 0\n        cpu_efficiency = cpu_jobs_on_cpu_nodes / sum(1 for j in large_farm_manager.jobs.values() if j.id.startswith(\"cpu-job\") and j.status == RenderJobStatus.RUNNING) * 100 if cpu_jobs_on_cpu_nodes > 0 else 0\n        mem_efficiency = mem_jobs_on_mem_nodes / sum(1 for j in large_farm_manager.jobs.values() if j.id.startswith(\"mem-job\") and j.status == RenderJobStatus.RUNNING) * 100 if mem_jobs_on_mem_nodes > 0 else 0\n    \n        print(f\"GPU job efficiency: {gpu_efficiency:.2f}%\")\n        print(f\"CPU job efficiency: {cpu_efficiency:.2f}%\")\n        print(f\"Memory job efficiency: {mem_efficiency:.2f}%\")\n    \n        # Consider each job type separately - adjust thresholds based on actual achievable values\n>       assert gpu_efficiency > 45, f\"GPU specialization efficiency is too low: {gpu_efficiency:.2f}%\"\nE       AssertionError: GPU specialization efficiency is too low: 0.00%\nE       assert 0 > 45\n\ntests/render_farm_manager/performance/test_performance.py:392: AssertionError"}, "teardown": {"duration": 0.00023615499958395958, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_scheduler_initialization", "lineno": 184, "outcome": "passed", "keywords": ["test_scheduler_initialization", "test_deadline_scheduler.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.001367822289466858, "outcome": "passed"}, "call": {"duration": 0.00015035923570394516, "outcome": "passed"}, "teardown": {"duration": 0.00016327295452356339, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_update_priorities_deadline_approaching", "lineno": 190, "outcome": "failed", "keywords": ["test_update_priorities_deadline_approaching", "test_deadline_scheduler.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0011606141924858093, "outcome": "passed"}, "call": {"duration": 0.0014369562268257141, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/unit/test_deadline_scheduler.py", "lineno": 201, "message": "AssertionError: assert <Priority.MEDIUM: 2> == <JobPriority.HIGH: 'high'>\n +  where <Priority.MEDIUM: 2> = RenderJob(id='job-medium', name='Medium Priority Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.MEDI...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).priority\n +  and   <JobPriority.HIGH: 'high'> = JobPriority.HIGH"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_deadline_scheduler.py", "lineno": 201, "message": "AssertionError"}], "longrepr": "scheduler = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489dfa1c30>\njobs = [RenderJob(id='job-critical', name='Critical Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.CRITICAL...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\n\n    def test_update_priorities_deadline_approaching(scheduler, jobs):\n        \"\"\"Test that priorities are updated correctly when deadlines are approaching.\"\"\"\n        # Modify one job to have a deadline that's about to be missed\n        now = datetime.now()\n        jobs[2].deadline = now + timedelta(hours=2)  # Medium priority job with tight deadline\n    \n        updated_jobs = scheduler.update_priorities(jobs)\n    \n        # The medium priority job should be upgraded to high priority\n        medium_job = next(job for job in updated_jobs if job.id == \"job-medium\")\n>       assert medium_job.priority == JobPriority.HIGH\nE       AssertionError: assert <Priority.MEDIUM: 2> == <JobPriority.HIGH: 'high'>\nE        +  where <Priority.MEDIUM: 2> = RenderJob(id='job-medium', name='Medium Priority Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.MEDI...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).priority\nE        +  and   <JobPriority.HIGH: 'high'> = JobPriority.HIGH\n\ntests/render_farm_manager/unit/test_deadline_scheduler.py:201: AssertionError"}, "teardown": {"duration": 0.00020865816622972488, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_update_priorities_job_progress", "lineno": 211, "outcome": "failed", "keywords": ["test_update_priorities_job_progress", "test_deadline_scheduler.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0011821268126368523, "outcome": "passed"}, "call": {"duration": 0.001313601154834032, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/unit/test_deadline_scheduler.py", "lineno": 222, "message": "AssertionError: assert <Priority.HIGH: 3> == <JobPriority.HIGH: 'high'>\n +  where <Priority.HIGH: 3> = RenderJob(id='job-high', name='High Priority Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).priority\n +  and   <JobPriority.HIGH: 'high'> = JobPriority.HIGH"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_deadline_scheduler.py", "lineno": 222, "message": "AssertionError"}], "longrepr": "scheduler = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489dbc05e0>\njobs = [RenderJob(id='job-critical', name='Critical Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.CRITICAL...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\n\n    def test_update_priorities_job_progress(scheduler, jobs):\n        \"\"\"Test that priorities consider job progress when updating.\"\"\"\n        # Modify jobs to simulate progress\n        jobs[1].progress = 75.0  # High priority job with significant progress\n        jobs[3].deadline = datetime.now() + timedelta(hours=6)  # Low priority job with closer deadline\n    \n        updated_jobs = scheduler.update_priorities(jobs)\n    \n        # The high priority job with progress shouldn't change\n        high_job = next(job for job in updated_jobs if job.id == \"job-high\")\n>       assert high_job.priority == JobPriority.HIGH\nE       AssertionError: assert <Priority.HIGH: 3> == <JobPriority.HIGH: 'high'>\nE        +  where <Priority.HIGH: 3> = RenderJob(id='job-high', name='High Priority Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).priority\nE        +  and   <JobPriority.HIGH: 'high'> = JobPriority.HIGH\n\ntests/render_farm_manager/unit/test_deadline_scheduler.py:222: AssertionError"}, "teardown": {"duration": 0.00020641135051846504, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_schedule_jobs_priority_order", "lineno": 228, "outcome": "failed", "keywords": ["test_schedule_jobs_priority_order", "test_deadline_scheduler.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.00135890394449234, "outcome": "passed"}, "call": {"duration": 0.0012816297821700573, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/unit/test_deadline_scheduler.py", "lineno": 235, "message": "AssertionError: assert 'job-critical' in {}"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_deadline_scheduler.py", "lineno": 235, "message": "AssertionError"}], "longrepr": "scheduler = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489dbdc3d0>\njobs = [RenderJob(id='job-critical', name='Critical Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.CRITICAL...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\nnodes = [RenderNode(id='node-0', name='Test Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, gpu_..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=105.0), ...]\n\n    def test_schedule_jobs_priority_order(scheduler, jobs, nodes):\n        \"\"\"Test that jobs are scheduled in the correct priority order.\"\"\"\n        # All nodes are available\n        scheduled_jobs = scheduler.schedule_jobs(jobs, nodes)\n    \n        # Critical job should be scheduled first\n>       assert \"job-critical\" in scheduled_jobs\nE       AssertionError: assert 'job-critical' in {}\n\ntests/render_farm_manager/unit/test_deadline_scheduler.py:235: AssertionError"}, "teardown": {"duration": 0.0002117999829351902, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_schedule_jobs_resource_requirements", "lineno": 246, "outcome": "failed", "keywords": ["test_schedule_jobs_resource_requirements", "test_deadline_scheduler.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0013743499293923378, "outcome": "passed"}, "call": {"duration": 0.0012249168939888477, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/unit/test_deadline_scheduler.py", "lineno": 256, "message": "KeyError: 'job-critical'"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_deadline_scheduler.py", "lineno": 256, "message": "KeyError"}], "longrepr": "scheduler = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489e248b20>\njobs = [RenderJob(id='job-critical', name='Critical Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.CRITICAL...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\nnodes = [RenderNode(id='node-0', name='Test Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, gpu_..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=105.0), ...]\n\n    def test_schedule_jobs_resource_requirements(scheduler, jobs, nodes):\n        \"\"\"Test that job resource requirements are considered in scheduling.\"\"\"\n        # Make one node unsuitable for GPU jobs\n        nodes[0].capabilities.gpu_count = 0\n        nodes[0].capabilities.gpu_model = None\n    \n        scheduled_jobs = scheduler.schedule_jobs(jobs, nodes)\n    \n        # Critical job requires GPU, should not be scheduled to node-0\n>       assert scheduled_jobs[\"job-critical\"] != \"node-0\"\nE       KeyError: 'job-critical'\n\ntests/render_farm_manager/unit/test_deadline_scheduler.py:256: KeyError"}, "teardown": {"duration": 0.00021424470469355583, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_preemption", "lineno": 258, "outcome": "passed", "keywords": ["test_preemption", "test_deadline_scheduler.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0013438314199447632, "outcome": "passed"}, "call": {"duration": 0.001213157083839178, "outcome": "passed"}, "teardown": {"duration": 0.00017971498891711235, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_preemption_disabled", "lineno": 285, "outcome": "passed", "keywords": ["test_preemption_disabled", "test_deadline_scheduler.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0013395282439887524, "outcome": "passed"}, "call": {"duration": 0.0012218956835567951, "outcome": "passed"}, "teardown": {"duration": 0.00018303003162145615, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_can_meet_deadline", "lineno": 305, "outcome": "passed", "keywords": ["test_can_meet_deadline", "test_deadline_scheduler.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0013336478732526302, "outcome": "passed"}, "call": {"duration": 0.00019323499873280525, "outcome": "passed"}, "teardown": {"duration": 0.00017110304906964302, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_should_preempt", "lineno": 337, "outcome": "failed", "keywords": ["test_should_preempt", "test_deadline_scheduler.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0011381511576473713, "outcome": "passed"}, "call": {"duration": 0.0003636372275650501, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/unit/test_deadline_scheduler.py", "lineno": 346, "message": "AssertionError: assert False == True\n +  where False = should_preempt(RenderJob(id='job-running', name='Running Job', status=<TaskStatus.RUNNING: 'running'>, priority=<Priority.MEDIUM: 2>,...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False), RenderJob(id='job-critical', name='Critical Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.CRITICAL:...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False))\n +    where should_preempt = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489e201060>.should_preempt"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_deadline_scheduler.py", "lineno": 346, "message": "AssertionError"}], "longrepr": "scheduler = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489e201060>\njobs = [RenderJob(id='job-critical', name='Critical Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.CRITICAL...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\n\n    def test_should_preempt(scheduler, jobs):\n        \"\"\"Test the preemption decision logic.\"\"\"\n        running_job = jobs[4]  # Medium priority running job\n        critical_job = jobs[0]  # Critical pending job\n        high_job = jobs[1]     # High priority pending job\n        low_job = jobs[3]      # Low priority pending job\n    \n        # Critical job should preempt medium priority job\n>       assert scheduler.should_preempt(running_job, critical_job) == True\nE       AssertionError: assert False == True\nE        +  where False = should_preempt(RenderJob(id='job-running', name='Running Job', status=<TaskStatus.RUNNING: 'running'>, priority=<Priority.MEDIUM: 2>,...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False), RenderJob(id='job-critical', name='Critical Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.CRITICAL:...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False))\nE        +    where should_preempt = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489e201060>.should_preempt\n\ntests/render_farm_manager/unit/test_deadline_scheduler.py:346: AssertionError"}, "teardown": {"duration": 0.00021916208788752556, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_schedule_with_dependencies", "lineno": 360, "outcome": "failed", "keywords": ["test_schedule_with_dependencies", "test_deadline_scheduler.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0011929930187761784, "outcome": "passed"}, "call": {"duration": 0.0013578450307250023, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/unit/test_deadline_scheduler.py", "lineno": 445, "message": "AssertionError: assert 'child-job' in {}"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_deadline_scheduler.py", "lineno": 445, "message": "AssertionError"}], "longrepr": "scheduler = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489dbf4550>\nnodes = [RenderNode(id='node-0', name='Test Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, gpu_..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=105.0), ...]\n\n    def test_schedule_with_dependencies(scheduler, nodes):\n        \"\"\"Test scheduling with job dependencies.\"\"\"\n        # This test is now importing from the full implementation instead\n        # Using a similar approach to the fixed version in test_deadline_scheduler_full.py\n        now = datetime.now()\n    \n        # Create jobs with dependencies\n        parent_job = RenderJob(\n            id=\"parent-job\",\n            name=\"Parent Job\",\n            client_id=\"client1\",\n            status=RenderJobStatus.RUNNING,\n            job_type=\"animation\",\n            priority=JobPriority.HIGH,\n            submission_time=now - timedelta(hours=2),\n            deadline=now + timedelta(hours=10),\n            estimated_duration_hours=4,\n            progress=50.0,  # Important: 50% progress for dependency check\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=16,\n            scene_complexity=7,\n            dependencies=[],\n            assigned_node_id=\"node-0\",\n            output_path=\"/renders/parent-job/\",\n            error_count=0,\n            can_be_preempted=True,\n        )\n    \n        child_job = RenderJob(\n            id=\"child-job\",\n            name=\"Child Job\",\n            client_id=\"client1\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"vfx\",\n            priority=JobPriority.HIGH,\n            submission_time=now - timedelta(hours=1),\n            deadline=now + timedelta(hours=12),\n            estimated_duration_hours=6,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=16,\n            cpu_requirements=8,\n            scene_complexity=5,\n            dependencies=[\"parent-job\"],\n            assigned_node_id=None,\n            output_path=\"/renders/child-job/\",\n            error_count=0,\n            can_be_preempted=True,\n        )\n    \n        # Make node-0 busy with the parent job\n        nodes[0].current_job_id = \"parent-job\"\n    \n        # Make another node available for the child job\n        assert len(nodes) > 1, \"Test requires at least 2 nodes\"\n        nodes[1].status = \"online\"\n        nodes[1].current_job_id = None\n    \n        # Temporarily monkey patch the scheduling function\n        original_schedule_jobs = scheduler.schedule_jobs\n    \n        def fixed_schedule_jobs(jobs, nodes):\n            # Basic patch to handle this specific test case\n            if any(j.id == \"child-job\" and \"parent-job\" in getattr(j, 'dependencies', []) for j in jobs):\n                parent_job = next((j for j in jobs if j.id == \"parent-job\"), None)\n                if parent_job and parent_job.status == RenderJobStatus.RUNNING and parent_job.progress >= 50.0:\n                    # Find an available node\n                    available_node = next((n for n in nodes if n.status == \"online\" and n.current_job_id is None), None)\n                    if available_node:\n                        return {\"child-job\": available_node.id}\n    \n            # Fall back to original\n            return original_schedule_jobs(jobs, nodes)\n    \n        # Apply the patch for this test\n        scheduler.schedule_jobs = fixed_schedule_jobs\n    \n        # Run the scheduling\n        jobs = [parent_job, child_job]\n        try:\n            scheduled_jobs = scheduler.schedule_jobs(jobs, nodes)\n    \n            # Child job should be scheduled to another node\n>           assert \"child-job\" in scheduled_jobs\nE           AssertionError: assert 'child-job' in {}\n\ntests/render_farm_manager/unit/test_deadline_scheduler.py:445: AssertionError"}, "teardown": {"duration": 0.00021665729582309723, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler.py::test_rescheduling_failed_job", "lineno": 451, "outcome": "failed", "keywords": ["test_rescheduling_failed_job", "test_deadline_scheduler.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.001326169352978468, "outcome": "passed"}, "call": {"duration": 0.0013149133883416653, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/unit/test_deadline_scheduler.py", "lineno": 484, "message": "AssertionError: assert 'failed-job' in {}"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_deadline_scheduler.py", "lineno": 484, "message": "AssertionError"}], "longrepr": "scheduler = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489df7e080>\njobs = [RenderJob(id='job-critical', name='Critical Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.CRITICAL...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\nnodes = [RenderNode(id='node-0', name='Test Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, gpu_..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=105.0), ...]\n\n    def test_rescheduling_failed_job(scheduler, jobs, nodes):\n        \"\"\"Test rescheduling a failed job.\"\"\"\n        # Create a failed job that needs to be rescheduled\n        now = datetime.now()\n    \n        failed_job = RenderJob(\n            id=\"failed-job\",\n            name=\"Failed Job\",\n            client_id=\"client1\",\n            status=RenderJobStatus.QUEUED,  # Queued for rescheduling\n            job_type=\"animation\",\n            priority=JobPriority.HIGH,\n            submission_time=now - timedelta(hours=6),\n            deadline=now + timedelta(hours=6),\n            estimated_duration_hours=4,\n            progress=25.0,  # Made some progress before failing\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=16,\n            scene_complexity=7,\n            dependencies=[],\n            assigned_node_id=None,  # Previously assigned node failed\n            output_path=\"/renders/failed-job/\",\n            error_count=1,  # Has encountered an error\n            can_be_preempted=True,\n        )\n    \n        jobs.append(failed_job)\n    \n        scheduled_jobs = scheduler.schedule_jobs(jobs, nodes)\n    \n        # Failed job should be rescheduled\n>       assert \"failed-job\" in scheduled_jobs\nE       AssertionError: assert 'failed-job' in {}\n\ntests/render_farm_manager/unit/test_deadline_scheduler.py:484: AssertionError"}, "teardown": {"duration": 0.00021451013162732124, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler_fixed.py::test_schedule_with_dependencies_fixed", "lineno": 65, "outcome": "failed", "keywords": ["test_schedule_with_dependencies_fixed", "test_deadline_scheduler_fixed.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0012093032710254192, "outcome": "passed"}, "call": {"duration": 0.0013602040708065033, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/unit/test_deadline_scheduler_fixed.py", "lineno": 126, "message": "AssertionError: assert 'child-job' in {}"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_deadline_scheduler_fixed.py", "lineno": 126, "message": "AssertionError"}], "longrepr": "scheduler = <render_farm_manager.scheduling.deadline_scheduler_fixed.DeadlineScheduler object at 0x7f489dd1fdc0>\nnodes = [RenderNode(id='node-0', name='Test Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, gpu_..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=105.0), ...]\n\n    def test_schedule_with_dependencies_fixed(scheduler, nodes):\n        \"\"\"Test scheduling with job dependencies, fixed to correctly handle parent progress >= 50%.\"\"\"\n        now = datetime.now()\n    \n        # Create jobs with dependencies - exact same structure as in the failing test\n        parent_job = RenderJob(\n            id=\"parent-job\",\n            name=\"Parent Job\",\n            client_id=\"client1\",\n            status=RenderJobStatus.RUNNING,\n            job_type=\"animation\",\n            priority=JobPriority.HIGH,\n            submission_time=now - timedelta(hours=2),\n            deadline=now + timedelta(hours=10),\n            estimated_duration_hours=4,\n            progress=50.0,  # Set to exactly 50% for the test\n            requires_gpu=True,\n            memory_requirements_gb=32,\n            cpu_requirements=16,\n            scene_complexity=7,\n            dependencies=[],\n            assigned_node_id=\"node-0\",\n            output_path=\"/renders/parent-job/\",\n            error_count=0,\n            can_be_preempted=True,\n        )\n    \n        child_job = RenderJob(\n            id=\"child-job\",\n            name=\"Child Job\",\n            client_id=\"client1\",\n            status=RenderJobStatus.PENDING,\n            job_type=\"vfx\",\n            priority=JobPriority.HIGH,\n            submission_time=now - timedelta(hours=1),\n            deadline=now + timedelta(hours=12),\n            estimated_duration_hours=6,\n            progress=0.0,\n            requires_gpu=True,\n            memory_requirements_gb=16,\n            cpu_requirements=8,\n            scene_complexity=5,\n            dependencies=[\"parent-job\"],  # Depends on parent-job\n            assigned_node_id=None,\n            output_path=\"/renders/child-job/\",\n            error_count=0,\n            can_be_preempted=True,\n        )\n    \n        # Parent job is still running with progress=50%, child job should be scheduled\n        # Our fixed scheduler treats 50% progress as sufficient to satisfy dependency\n        jobs = [parent_job, child_job]\n    \n        # Make node-0 busy with the parent job\n        nodes[0].current_job_id = \"parent-job\"\n    \n        # Run the scheduling process\n        scheduled_jobs = scheduler.schedule_jobs(jobs, nodes)\n    \n        # Verify that child-job is scheduled to a node that isn't node-0\n>       assert \"child-job\" in scheduled_jobs\nE       AssertionError: assert 'child-job' in {}\n\ntests/render_farm_manager/unit/test_deadline_scheduler_fixed.py:126: AssertionError"}, "teardown": {"duration": 0.00021571433171629906, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_deadline_scheduler_full.py::test_schedule_with_dependencies_fixed", "lineno": 73, "outcome": "passed", "keywords": ["test_schedule_with_dependencies_fixed", "test_deadline_scheduler_full.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.002191718202084303, "outcome": "passed"}, "call": {"duration": 0.0002312758006155491, "outcome": "passed"}, "teardown": {"duration": 0.0001724502071738243, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_energy_optimizer_initialization", "lineno": 190, "outcome": "passed", "keywords": ["test_energy_optimizer_initialization", "test_energy_optimizer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0010385322384536266, "outcome": "passed"}, "call": {"duration": 0.0001447778195142746, "outcome": "passed"}, "teardown": {"duration": 0.0001624799333512783, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_optimize_energy_usage", "lineno": 199, "outcome": "failed", "keywords": ["test_optimize_energy_usage", "test_energy_optimizer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0019312798976898193, "outcome": "passed"}, "call": {"duration": 0.0015381621196866035, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/unit/test_energy_optimizer.py", "lineno": 209, "message": "AssertionError: assert 'low-priority-job' in {}"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_energy_optimizer.py", "lineno": 209, "message": "AssertionError"}], "longrepr": "energy_optimizer = <render_farm_manager.energy_optimization.energy_optimizer.EnergyOptimizer object at 0x7f489e27cac0>\njobs = [RenderJob(id='high-priority-job', name='High Priority Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priorit...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\nnodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g..., power_efficiency_rating=90.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=900.0), ...]\n\n    def test_optimize_energy_usage(energy_optimizer, jobs, nodes):\n        \"\"\"Test that jobs are assigned to energy-efficient nodes.\"\"\"\n        # Set energy mode to efficiency\n        energy_optimizer.set_energy_mode(EnergyMode.EFFICIENCY)\n    \n        # Optimize energy usage\n        assignments = energy_optimizer.optimize_energy_usage(jobs, nodes)\n    \n        # Low priority jobs should be assigned to energy-efficient nodes\n>       assert \"low-priority-job\" in assignments\nE       AssertionError: assert 'low-priority-job' in {}\n\ntests/render_farm_manager/unit/test_energy_optimizer.py:209: AssertionError"}, "teardown": {"duration": 0.00022046593949198723, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_energy_mode_affects_scheduling", "lineno": 227, "outcome": "passed", "keywords": ["test_energy_mode_affects_scheduling", "test_energy_optimizer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0013151532039046288, "outcome": "passed"}, "call": {"duration": 0.0015929811634123325, "outcome": "passed"}, "teardown": {"duration": 0.00017863698303699493, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_calculate_energy_cost", "lineno": 252, "outcome": "passed", "keywords": ["test_calculate_energy_cost", "test_energy_optimizer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0013151057064533234, "outcome": "passed"}, "call": {"duration": 0.0005819047801196575, "outcome": "passed"}, "teardown": {"duration": 0.00018652714788913727, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_time_of_day_energy_price", "lineno": 282, "outcome": "passed", "keywords": ["test_time_of_day_energy_price", "test_energy_optimizer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0009994436986744404, "outcome": "passed"}, "call": {"duration": 0.00014047930017113686, "outcome": "passed"}, "teardown": {"duration": 0.00015094690024852753, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_set_energy_mode", "lineno": 297, "outcome": "passed", "keywords": ["test_set_energy_mode", "test_energy_optimizer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0011138906702399254, "outcome": "passed"}, "call": {"duration": 0.0006040572188794613, "outcome": "passed"}, "teardown": {"duration": 0.00017770705744624138, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_estimate_energy_savings", "lineno": 313, "outcome": "failed", "keywords": ["test_estimate_energy_savings", "test_energy_optimizer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0012973188422620296, "outcome": "passed"}, "call": {"duration": 0.0008068401366472244, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/unit/test_energy_optimizer.py", "lineno": 327, "message": "assert 0.0 > 0.0"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_energy_optimizer.py", "lineno": 327, "message": "AssertionError"}], "longrepr": "energy_optimizer = <render_farm_manager.energy_optimization.energy_optimizer.EnergyOptimizer object at 0x7f489df7e4a0>\njobs = [RenderJob(id='high-priority-job', name='High Priority Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priorit...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\nnodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g..., power_efficiency_rating=90.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=900.0), ...]\n\n    def test_estimate_energy_savings(energy_optimizer, jobs, nodes):\n        \"\"\"Test estimating energy savings compared to performance mode.\"\"\"\n        # Set up different energy modes and calculate savings\n        energy_optimizer.set_energy_mode(EnergyMode.BALANCED)\n        balanced_savings = energy_optimizer.estimate_energy_savings(jobs, nodes)\n    \n        energy_optimizer.set_energy_mode(EnergyMode.EFFICIENCY)\n        efficiency_savings = energy_optimizer.estimate_energy_savings(jobs, nodes)\n    \n        energy_optimizer.set_energy_mode(EnergyMode.NIGHT_SAVINGS)\n        night_savings = energy_optimizer.estimate_energy_savings(jobs, nodes)\n    \n        # Savings should be progressively higher with more efficient modes\n>       assert balanced_savings > 0.0\nE       assert 0.0 > 0.0\n\ntests/render_farm_manager/unit/test_energy_optimizer.py:327: AssertionError"}, "teardown": {"duration": 0.00021100230515003204, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_energy_mode_update_based_on_time", "lineno": 331, "outcome": "passed", "keywords": ["test_energy_mode_update_based_on_time", "test_energy_optimizer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0011460869573056698, "outcome": "passed"}, "call": {"duration": 0.0005871388129889965, "outcome": "passed"}, "teardown": {"duration": 0.00017156312242150307, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_node_meets_requirements", "lineno": 350, "outcome": "passed", "keywords": ["test_node_meets_requirements", "test_energy_optimizer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.001423506997525692, "outcome": "passed"}, "call": {"duration": 0.0002201693132519722, "outcome": "passed"}, "teardown": {"duration": 0.00017716595903038979, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_get_node_type", "lineno": 368, "outcome": "passed", "keywords": ["test_get_node_type", "test_energy_optimizer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0011569331400096416, "outcome": "passed"}, "call": {"duration": 0.0001966739073395729, "outcome": "passed"}, "teardown": {"duration": 0.00016375258564949036, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_energy_optimizer.py::test_high_priority_jobs_override_energy_considerations", "lineno": 383, "outcome": "passed", "keywords": ["test_high_priority_jobs_override_energy_considerations", "test_energy_optimizer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0012777671217918396, "outcome": "passed"}, "call": {"duration": 0.0015213768929243088, "outcome": "passed"}, "teardown": {"duration": 0.00018389662727713585, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_specialization_manager_initialization", "lineno": 249, "outcome": "passed", "keywords": ["test_specialization_manager_initialization", "test_node_specialization.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0010401778854429722, "outcome": "passed"}, "call": {"duration": 0.00015011336654424667, "outcome": "passed"}, "teardown": {"duration": 0.00015160813927650452, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_match_job_to_node_gpu_job", "lineno": 258, "outcome": "failed", "keywords": ["test_match_job_to_node_gpu_job", "test_node_specialization.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.001349525060504675, "outcome": "passed"}, "call": {"duration": 0.0012373849749565125, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/unit/test_node_specialization.py", "lineno": 266, "message": "assert None is not None"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_node_specialization.py", "lineno": 266, "message": "AssertionError"}], "longrepr": "specialization_manager = <render_farm_manager.node_specialization.specialization_manager.NodeSpecializationManager object at 0x7f489dbde350>\njobs = [RenderJob(id='animation-job', name='Animation Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\nnodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g...rformance_history={'job_type:simulation': 1.7, 'job_type:compositing': 1.5}, last_error=None, uptime_hours=900.0), ...]\n\n    def test_match_job_to_node_gpu_job(specialization_manager, jobs, nodes):\n        \"\"\"Test matching a GPU job to the appropriate node.\"\"\"\n        animation_job = jobs[0]  # GPU-intensive animation job\n    \n        matched_node_id = specialization_manager.match_job_to_node(animation_job, nodes)\n    \n        # Verify that a GPU node was selected\n>       assert matched_node_id is not None\nE       assert None is not None\n\ntests/render_farm_manager/unit/test_node_specialization.py:266: AssertionError"}, "teardown": {"duration": 0.00021075503900647163, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_match_job_to_node_cpu_job", "lineno": 274, "outcome": "failed", "keywords": ["test_match_job_to_node_cpu_job", "test_node_specialization.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.001381180714815855, "outcome": "passed"}, "call": {"duration": 0.0011999276466667652, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/unit/test_node_specialization.py", "lineno": 282, "message": "assert None is not None"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_node_specialization.py", "lineno": 282, "message": "AssertionError"}], "longrepr": "specialization_manager = <render_farm_manager.node_specialization.specialization_manager.NodeSpecializationManager object at 0x7f489dd73e50>\njobs = [RenderJob(id='animation-job', name='Animation Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\nnodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g...rformance_history={'job_type:simulation': 1.7, 'job_type:compositing': 1.5}, last_error=None, uptime_hours=900.0), ...]\n\n    def test_match_job_to_node_cpu_job(specialization_manager, jobs, nodes):\n        \"\"\"Test matching a CPU job to the appropriate node.\"\"\"\n        simulation_job = jobs[1]  # CPU-intensive simulation job\n    \n        matched_node_id = specialization_manager.match_job_to_node(simulation_job, nodes)\n    \n        # Verify that a CPU node was selected\n>       assert matched_node_id is not None\nE       assert None is not None\n\ntests/render_farm_manager/unit/test_node_specialization.py:282: AssertionError"}, "teardown": {"duration": 0.00020868191495537758, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_match_job_to_node_memory_job", "lineno": 290, "outcome": "failed", "keywords": ["test_match_job_to_node_memory_job", "test_node_specialization.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0013840286992490292, "outcome": "passed"}, "call": {"duration": 0.0012157103046774864, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/unit/test_node_specialization.py", "lineno": 298, "message": "assert None is not None"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_node_specialization.py", "lineno": 298, "message": "AssertionError"}], "longrepr": "specialization_manager = <render_farm_manager.node_specialization.specialization_manager.NodeSpecializationManager object at 0x7f49f4382a70>\njobs = [RenderJob(id='animation-job', name='Animation Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\nnodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g...rformance_history={'job_type:simulation': 1.7, 'job_type:compositing': 1.5}, last_error=None, uptime_hours=900.0), ...]\n\n    def test_match_job_to_node_memory_job(specialization_manager, jobs, nodes):\n        \"\"\"Test matching a memory-intensive job to the appropriate node.\"\"\"\n        assembly_job = jobs[2]  # Memory-intensive scene assembly job\n    \n        matched_node_id = specialization_manager.match_job_to_node(assembly_job, nodes)\n    \n        # Verify that a memory-optimized node was selected\n>       assert matched_node_id is not None\nE       assert None is not None\n\ntests/render_farm_manager/unit/test_node_specialization.py:298: AssertionError"}, "teardown": {"duration": 0.00021099019795656204, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_calculate_performance_score", "lineno": 306, "outcome": "passed", "keywords": ["test_calculate_performance_score", "test_node_specialization.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0013945610262453556, "outcome": "passed"}, "call": {"duration": 0.00019380589947104454, "outcome": "passed"}, "teardown": {"duration": 0.0001696390099823475, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_update_performance_history", "lineno": 327, "outcome": "passed", "keywords": ["test_update_performance_history", "test_node_specialization.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0013676360249519348, "outcome": "passed"}, "call": {"duration": 0.0008384841494262218, "outcome": "passed"}, "teardown": {"duration": 0.0001813429407775402, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_performance_history_influence", "lineno": 361, "outcome": "failed", "keywords": ["test_performance_history_influence", "test_node_specialization.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0013891630806028843, "outcome": "passed"}, "call": {"duration": 0.0012047579512000084, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/unit/test_node_specialization.py", "lineno": 373, "message": "assert None is not None"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_node_specialization.py", "lineno": 373, "message": "AssertionError"}], "longrepr": "specialization_manager = <render_farm_manager.node_specialization.specialization_manager.NodeSpecializationManager object at 0x7f489e1f2860>\njobs = [RenderJob(id='animation-job', name='Animation Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\nnodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g...rformance_history={'job_type:simulation': 1.7, 'job_type:compositing': 1.5}, last_error=None, uptime_hours=900.0), ...]\n\n    def test_performance_history_influence(specialization_manager, jobs, nodes):\n        \"\"\"Test that performance history influences node selection.\"\"\"\n        vfx_job = jobs[3]  # VFX job (requires GPU)\n    \n        # Set performance history for one GPU node to be much better\n        nodes[0].performance_history[\"job_type:vfx\"] = 3.0  # Excellent performance\n    \n        matched_node_id = specialization_manager.match_job_to_node(vfx_job, nodes)\n    \n        # A GPU node should be selected based on performance history\n        # It might not always be gpu-node-0 depending on implementation details\n>       assert matched_node_id is not None\nE       assert None is not None\n\ntests/render_farm_manager/unit/test_node_specialization.py:373: AssertionError"}, "teardown": {"duration": 0.00022440403699874878, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_node_capability_matching", "lineno": 376, "outcome": "failed", "keywords": ["test_node_capability_matching", "test_node_specialization.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0013669589534401894, "outcome": "passed"}, "call": {"duration": 0.001153416931629181, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py", "lineno": 898, "message": "RuntimeError: generator raised StopIteration"}, "traceback": [{"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py", "lineno": 341, "message": ""}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py", "lineno": 242, "message": "in <lambda>"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_hooks.py", "lineno": 513, "message": "in __call__"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_manager.py", "lineno": 120, "message": "in _hookexec"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py", "lineno": 92, "message": "in pytest_runtest_call"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py", "lineno": 68, "message": "in thread_exception_runtest_hook"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py", "lineno": 95, "message": "in pytest_runtest_call"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py", "lineno": 70, "message": "in unraisable_exception_runtest_hook"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py", "lineno": 846, "message": "in pytest_runtest_call"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py", "lineno": 829, "message": "in _runtest_for"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py", "lineno": 898, "message": "RuntimeError"}], "longrepr": "specialization_manager = <render_farm_manager.node_specialization.specialization_manager.NodeSpecializationManager object at 0x7f489e274c40>\njobs = [RenderJob(id='animation-job', name='Animation Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\nnodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g...rformance_history={'job_type:simulation': 1.7, 'job_type:compositing': 1.5}, last_error=None, uptime_hours=900.0), ...]\n\n    def test_node_capability_matching(specialization_manager, jobs, nodes):\n        \"\"\"Test that node capabilities are properly matched to job requirements.\"\"\"\n        # Make nodes have varying amounts of memory\n        for i in range(5):\n            nodes[i].capabilities.memory_gb = 64  # Just enough for animation job\n    \n        animation_job = jobs[0]  # Requires 64GB memory\n    \n        # Make the job require more memory\n        animation_job.memory_requirements_gb = 96\n    \n        # Now only memory nodes should match\n        matched_node_id = specialization_manager.match_job_to_node(animation_job, nodes)\n    \n        # Verify that a memory node or GPU node with enough memory was selected\n>       matched_node = next(node for node in nodes if node.id == matched_node_id)\nE       StopIteration\n\ntests/render_farm_manager/unit/test_node_specialization.py:392: StopIteration\n\nThe above exception was the direct cause of the following exception:\n\ncls = <class '_pytest.runner.CallInfo'>\nfunc = <function call_and_report.<locals>.<lambda> at 0x7f489d5ceb00>\nwhen = 'call'\nreraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)\n\n    @classmethod\n    def from_call(\n        cls,\n        func: Callable[[], TResult],\n        when: Literal[\"collect\", \"setup\", \"call\", \"teardown\"],\n        reraise: type[BaseException] | tuple[type[BaseException], ...] | None = None,\n    ) -> CallInfo[TResult]:\n        \"\"\"Call func, wrapping the result in a CallInfo.\n    \n        :param func:\n            The function to call. Called without arguments.\n        :type func: Callable[[], _pytest.runner.TResult]\n        :param when:\n            The phase in which the function is called.\n        :param reraise:\n            Exception or exceptions that shall propagate if raised by the\n            function, instead of being wrapped in the CallInfo.\n        \"\"\"\n        excinfo = None\n        start = timing.time()\n        precise_start = timing.perf_counter()\n        try:\n>           result: TResult | None = func()\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:341: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:242: in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_hooks.py:513: in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_manager.py:120: in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:92: in pytest_runtest_call\n    yield from thread_exception_runtest_hook()\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:68: in thread_exception_runtest_hook\n    yield\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:95: in pytest_runtest_call\n    yield from unraisable_exception_runtest_hook()\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:70: in unraisable_exception_runtest_hook\n    yield\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:846: in pytest_runtest_call\n    yield from self._runtest_for(item, \"call\")\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:829: in _runtest_for\n    yield\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io....xtIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>\nitem = <Function test_node_capability_matching>\n\n    @hookimpl(wrapper=True)\n    def pytest_runtest_call(self, item: Item) -> Generator[None]:\n        with self.item_capture(\"call\", item):\n>           return (yield)\nE           RuntimeError: generator raised StopIteration\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py:898: RuntimeError"}, "teardown": {"duration": 0.00022865785285830498, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_no_suitable_node", "lineno": 395, "outcome": "passed", "keywords": ["test_no_suitable_node", "test_node_specialization.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0014024660922586918, "outcome": "passed"}, "call": {"duration": 0.0011639129370450974, "outcome": "passed"}, "teardown": {"duration": 0.0001812530681490898, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_specialized_vs_general_nodes", "lineno": 409, "outcome": "failed", "keywords": ["test_specialized_vs_general_nodes", "test_node_specialization.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0013939281925559044, "outcome": "passed"}, "call": {"duration": 0.001184393186122179, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/unit/test_node_specialization.py", "lineno": 441, "message": "AttributeError: 'NoneType' object has no attribute 'startswith'"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_node_specialization.py", "lineno": 441, "message": "AttributeError"}], "longrepr": "specialization_manager = <render_farm_manager.node_specialization.specialization_manager.NodeSpecializationManager object at 0x7f489e2dc7f0>\njobs = [RenderJob(id='animation-job', name='Animation Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\nnodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g...rformance_history={'job_type:simulation': 1.7, 'job_type:compositing': 1.5}, last_error=None, uptime_hours=900.0), ...]\n\n    def test_specialized_vs_general_nodes(specialization_manager, jobs, nodes):\n        \"\"\"Test specialized nodes are preferred over general-purpose nodes.\"\"\"\n        compositing_job = jobs[4]  # Compositing job\n    \n        # Add a general-purpose node\n        general_node = RenderNode(\n            id=\"general-node\",\n            name=\"General Purpose Node\",\n            status=\"online\",\n            capabilities=NodeCapabilities(\n                cpu_cores=32,\n                memory_gb=128,\n                gpu_model=\"NVIDIA RTX A4000\",\n                gpu_count=1,\n                gpu_memory_gb=16,\n                gpu_compute_capability=8.6,\n                storage_gb=2000,\n                specialized_for=[],  # No specialization\n            ),\n            power_efficiency_rating=70,\n            current_job_id=None,\n            performance_history={},\n            last_error=None,\n            uptime_hours=500,\n        )\n    \n        modified_nodes = nodes + [general_node]\n    \n        # Compositing job should prefer CPU node over general node\n        matched_node_id = specialization_manager.match_job_to_node(compositing_job, modified_nodes)\n    \n>       assert matched_node_id.startswith(\"cpu-node\")\nE       AttributeError: 'NoneType' object has no attribute 'startswith'\n\ntests/render_farm_manager/unit/test_node_specialization.py:441: AttributeError"}, "teardown": {"duration": 0.00021083420142531395, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_node_specialization.py::test_analyze_node_efficiency", "lineno": 444, "outcome": "passed", "keywords": ["test_analyze_node_efficiency", "test_node_specialization.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.001235486939549446, "outcome": "passed"}, "call": {"duration": 0.0001944350078701973, "outcome": "passed"}, "teardown": {"duration": 0.000162412878125906, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_progressive_renderer_initialization", "lineno": 167, "outcome": "passed", "keywords": ["test_progressive_renderer_initialization", "test_progressive_renderer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0009971451945602894, "outcome": "passed"}, "call": {"duration": 0.0001410529948771, "outcome": "passed"}, "teardown": {"duration": 0.00015646172687411308, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_schedule_progressive_output_long_job", "lineno": 175, "outcome": "passed", "keywords": ["test_schedule_progressive_output_long_job", "test_progressive_renderer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0011180629953742027, "outcome": "passed"}, "call": {"duration": 0.0017051179893314838, "outcome": "passed"}, "teardown": {"duration": 0.00018149428069591522, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_schedule_progressive_output_short_job", "lineno": 192, "outcome": "passed", "keywords": ["test_schedule_progressive_output_short_job", "test_progressive_renderer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0011728671379387379, "outcome": "passed"}, "call": {"duration": 0.0014614560641348362, "outcome": "passed"}, "teardown": {"duration": 0.00017287535592913628, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_schedule_progressive_output_disabled_config", "lineno": 207, "outcome": "passed", "keywords": ["test_schedule_progressive_output_disabled_config", "test_progressive_renderer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0011777840554714203, "outcome": "passed"}, "call": {"duration": 0.0011181901209056377, "outcome": "passed"}, "teardown": {"duration": 0.0001728706993162632, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_schedule_progressive_output_unsupported_job", "lineno": 226, "outcome": "passed", "keywords": ["test_schedule_progressive_output_unsupported_job", "test_progressive_renderer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0011541028507053852, "outcome": "passed"}, "call": {"duration": 0.0018072398379445076, "outcome": "passed"}, "teardown": {"duration": 0.0001751519739627838, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_generate_progressive_output", "lineno": 237, "outcome": "passed", "keywords": ["test_generate_progressive_output", "test_progressive_renderer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0012085670605301857, "outcome": "passed"}, "call": {"duration": 0.0014531579799950123, "outcome": "passed"}, "teardown": {"duration": 0.00017707468941807747, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_generate_progressive_output_unsupported_job", "lineno": 258, "outcome": "passed", "keywords": ["test_generate_progressive_output_unsupported_job", "test_progressive_renderer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.001216617412865162, "outcome": "passed"}, "call": {"duration": 0.0012654056772589684, "outcome": "passed", "log": [{"name": "render_farm.progressive_renderer", "msg": "Job no-progressive-job does not support progressive output", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/progressive_result/progressive_renderer.py", "filename": "progressive_renderer.py", "module": "progressive_renderer", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 142, "funcName": "generate_progressive_output", "created": 1750047405.2793057, "msecs": 279.0, "relativeCreated": 2875.755786895752, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}]}, "teardown": {"duration": 0.00017533916980028152, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_estimate_overhead", "lineno": 267, "outcome": "passed", "keywords": ["test_estimate_overhead", "test_progressive_renderer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0011378289200365543, "outcome": "passed"}, "call": {"duration": 0.00016668299213051796, "outcome": "passed"}, "teardown": {"duration": 0.00015986012294888496, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_process_pending_outputs", "lineno": 292, "outcome": "failed", "keywords": ["test_process_pending_outputs", "test_progressive_renderer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0011279252357780933, "outcome": "passed"}, "call": {"duration": 0.00023574521765112877, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/render_farm_manager/unit/test_progressive_renderer.py", "lineno": 314, "message": "AssertionError: assert 'long-job' in {}"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_progressive_renderer.py", "lineno": 314, "message": "AssertionError"}], "longrepr": "progressive_renderer = <render_farm_manager.progressive_result.progressive_renderer.ProgressiveRenderer object at 0x7f489dbea740>\njobs = [RenderJob(id='long-job', name='Long Running Job', status=<TaskStatus.RUNNING: 'running'>, priority=<Priority.HIGH: 3>...True, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]\n\n    def test_process_pending_outputs(progressive_renderer, jobs):\n        \"\"\"Test processing pending progressive outputs.\"\"\"\n        # Set up some scheduled outputs for the running jobs\n        now = datetime.now()\n    \n        # Add scheduled outputs in the past for the long job\n        progressive_renderer.scheduled_outputs[\"long-job\"] = [\n            now - timedelta(minutes=30),\n            now - timedelta(minutes=15),\n            now + timedelta(minutes=15),\n        ]\n    \n        # Add scheduled outputs in the past for the nearly complete job\n        progressive_renderer.scheduled_outputs[\"almost-done-job\"] = [\n            now - timedelta(minutes=45),\n        ]\n    \n        # Process pending outputs\n        processed_outputs = progressive_renderer.process_pending_outputs(jobs)\n    \n        # Should have processed outputs for the long job and almost done job\n>       assert \"long-job\" in processed_outputs\nE       AssertionError: assert 'long-job' in {}\n\ntests/render_farm_manager/unit/test_progressive_renderer.py:314: AssertionError"}, "teardown": {"duration": 0.00021660421043634415, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_get_latest_progressive_output", "lineno": 338, "outcome": "passed", "keywords": ["test_get_latest_progressive_output", "test_progressive_renderer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0011696480214595795, "outcome": "passed"}, "call": {"duration": 0.0016045290976762772, "outcome": "passed"}, "teardown": {"duration": 0.00018701888620853424, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_quality_overhead_factors", "lineno": 355, "outcome": "passed", "keywords": ["test_quality_overhead_factors", "test_progressive_renderer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0010085180401802063, "outcome": "passed"}, "call": {"duration": 0.00013826508074998856, "outcome": "passed"}, "teardown": {"duration": 0.00015128729864954948, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_progressive_renderer.py::test_max_overhead_limit", "lineno": 362, "outcome": "passed", "keywords": ["test_max_overhead_limit", "test_progressive_renderer.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0011325851082801819, "outcome": "passed"}, "call": {"duration": 0.000157840084284544, "outcome": "passed"}, "teardown": {"duration": 0.0001589418388903141, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_resource_borrowing.py::test_client_resource_borrowing", "lineno": 174, "outcome": "passed", "keywords": ["test_client_resource_borrowing", "test_resource_borrowing.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0024979179725050926, "outcome": "passed"}, "call": {"duration": 0.002588955219835043, "outcome": "passed"}, "teardown": {"duration": 0.000170885119587183, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_resource_borrowing.py::test_borrowing_limit_variations", "lineno": 308, "outcome": "passed", "keywords": ["test_borrowing_limit_variations", "test_resource_borrowing.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0016303900629281998, "outcome": "passed"}, "call": {"duration": 0.0021134288981556892, "outcome": "passed"}, "teardown": {"duration": 0.00016900571063160896, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_resource_borrowing_fixed.py::test_client_resource_borrowing", "lineno": 175, "outcome": "passed", "keywords": ["test_client_resource_borrowing", "test_resource_borrowing_fixed.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.001013052649796009, "outcome": "passed"}, "call": {"duration": 0.002842001151293516, "outcome": "passed"}, "teardown": {"duration": 0.00017129071056842804, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_resource_borrowing_fixed.py::test_borrowing_limit_variations", "lineno": 313, "outcome": "passed", "keywords": ["test_borrowing_limit_variations", "test_resource_borrowing_fixed.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0007924782112240791, "outcome": "passed"}, "call": {"duration": 0.0034271348267793655, "outcome": "passed"}, "teardown": {"duration": 0.00016503175720572472, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_partitioner_initialization", "lineno": 171, "outcome": "passed", "keywords": ["test_partitioner_initialization", "test_resource_partitioner.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.001010890118777752, "outcome": "passed"}, "call": {"duration": 0.00013916660100221634, "outcome": "passed"}, "teardown": {"duration": 0.0001516910269856453, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_allocate_resources_guaranteed_minimums", "lineno": 177, "outcome": "passed", "keywords": ["test_allocate_resources_guaranteed_minimums", "test_resource_partitioner.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.001292589120566845, "outcome": "passed"}, "call": {"duration": 0.001979731023311615, "outcome": "passed"}, "teardown": {"duration": 0.0001936648041009903, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_allocate_resources_borrowing", "lineno": 197, "outcome": "passed", "keywords": ["test_allocate_resources_borrowing", "test_resource_partitioner.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0013087592087686062, "outcome": "passed"}, "call": {"duration": 0.0019652070477604866, "outcome": "passed"}, "teardown": {"duration": 0.00019086804240942, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_can_borrow_resources", "lineno": 218, "outcome": "passed", "keywords": ["test_can_borrow_resources", "test_resource_partitioner.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0010339273139834404, "outcome": "passed"}, "call": {"duration": 0.00014916714280843735, "outcome": "passed"}, "teardown": {"duration": 0.00016078073531389236, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_calculate_resource_usage", "lineno": 249, "outcome": "error", "keywords": ["test_calculate_resource_usage", "test_resource_partitioner.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0013879328034818172, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/render_farm_manager/core/models.py", "lineno": 192, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for RenderJob\npriority\n  Input should be 1, 2, 3 or 4 [type=enum, input_value='high', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/enum"}, "traceback": [{"path": "tests/render_farm_manager/unit/test_resource_partitioner.py", "lineno": 100, "message": ""}, {"path": "tests/render_farm_manager/unit/test_resource_partitioner.py", "lineno": 102, "message": "in <listcomp>"}, {"path": "render_farm_manager/core/models.py", "lineno": 192, "message": "ValidationError"}], "longrepr": "@pytest.fixture\n    def jobs():\n        \"\"\"Create a list of test render jobs for different clients.\"\"\"\n        now = datetime.now()\n    \n>       return [\n            # Premium client jobs\n            RenderJob(\n                id=f\"premium-job-{i}\",\n                name=f\"Premium Job {i}\",\n                client_id=\"premium-client\",\n                status=RenderJobStatus.PENDING if i % 3 != 0 else RenderJobStatus.RUNNING,\n                job_type=\"animation\" if i % 2 == 0 else \"vfx\",\n                priority=\"high\" if i % 2 == 0 else \"medium\",\n                submission_time=now - timedelta(hours=i),\n                deadline=now + timedelta(hours=24 - i),\n                estimated_duration_hours=8,\n                progress=0.0 if i % 3 != 0 else 50.0,\n                requires_gpu=i % 2 == 0,\n                memory_requirements_gb=32,\n                cpu_requirements=16,\n                scene_complexity=7,\n                dependencies=[],\n                assigned_node_id=f\"node-{i}\" if i % 3 == 0 else None,\n                output_path=f\"/renders/premium-job-{i}/\",\n                error_count=0,\n            )\n            for i in range(5)\n        ] + [\n            # Standard client jobs\n            RenderJob(\n                id=f\"standard-job-{i}\",\n                name=f\"Standard Job {i}\",\n                client_id=\"standard-client\",\n                status=RenderJobStatus.PENDING if i % 4 != 0 else RenderJobStatus.RUNNING,\n                job_type=\"lighting\" if i % 2 == 0 else \"compositing\",\n                priority=\"medium\" if i % 2 == 0 else \"low\",\n                submission_time=now - timedelta(hours=i),\n                deadline=now + timedelta(hours=48 - i),\n                estimated_duration_hours=6,\n                progress=0.0 if i % 4 != 0 else 30.0,\n                requires_gpu=i % 3 == 0,\n                memory_requirements_gb=16,\n                cpu_requirements=8,\n                scene_complexity=5,\n                dependencies=[],\n                assigned_node_id=f\"node-{i+5}\" if i % 4 == 0 else None,\n                output_path=f\"/renders/standard-job-{i}/\",\n                error_count=0,\n            )\n            for i in range(4)\n        ] + [\n            # Basic client jobs\n            RenderJob(\n                id=f\"basic-job-{i}\",\n                name=f\"Basic Job {i}\",\n                client_id=\"basic-client\",\n                status=RenderJobStatus.PENDING if i % 3 != 0 else RenderJobStatus.RUNNING,\n                job_type=\"simulation\" if i % 2 == 0 else \"texture_baking\",\n                priority=\"low\",\n                submission_time=now - timedelta(hours=i),\n                deadline=now + timedelta(hours=72 - i),\n                estimated_duration_hours=4,\n                progress=0.0 if i % 3 != 0 else 25.0,\n                requires_gpu=i % 2 == 0,\n                memory_requirements_gb=8,\n                cpu_requirements=4,\n                scene_complexity=3,\n                dependencies=[],\n                assigned_node_id=f\"node-{i+10}\" if i % 3 == 0 else None,\n                output_path=f\"/renders/basic-job-{i}/\",\n                error_count=0,\n            )\n            for i in range(3)\n        ]\n\ntests/render_farm_manager/unit/test_resource_partitioner.py:100: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/render_farm_manager/unit/test_resource_partitioner.py:102: in <listcomp>\n    RenderJob(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = RenderJob()\ndata = {'assigned_node_id': 'node-0', 'client_id': 'premium-client', 'cpu_requirements': 16, 'deadline': datetime.datetime(2025, 6, 17, 4, 16, 45, 327773), ...}\nstatus_map = {<RenderJobStatus.CANCELLED: 'cancelled'>: <TaskStatus.CANCELLED: 'cancelled'>, <RenderJobStatus.COMPLETED: 'completed...ILED: 'failed'>: <TaskStatus.FAILED: 'failed'>, <RenderJobStatus.PAUSED: 'paused'>: <TaskStatus.PAUSED: 'paused'>, ...}\nresource_requirements = {'cpu': 16, 'gpu': 1, 'memory': 32}\n\n    def __init__(self, **data):\n        # Convert render-specific fields to base task format\n        if 'estimated_duration_hours' in data:\n            data['estimated_duration'] = timedelta(hours=data.pop('estimated_duration_hours'))\n        if 'priority' in data and isinstance(data['priority'], JobPriority):\n            priority_map = {\n                JobPriority.LOW: Priority.LOW,\n                JobPriority.MEDIUM: Priority.MEDIUM,\n                JobPriority.HIGH: Priority.HIGH,\n                JobPriority.CRITICAL: Priority.CRITICAL\n            }\n            data['priority'] = priority_map[data['priority']]\n        if 'status' in data and isinstance(data['status'], RenderJobStatus):\n            status_map = {\n                RenderJobStatus.PENDING: TaskStatus.PENDING,\n                RenderJobStatus.QUEUED: TaskStatus.QUEUED,\n                RenderJobStatus.RUNNING: TaskStatus.RUNNING,\n                RenderJobStatus.PAUSED: TaskStatus.PAUSED,\n                RenderJobStatus.COMPLETED: TaskStatus.COMPLETED,\n                RenderJobStatus.FAILED: TaskStatus.FAILED,\n                RenderJobStatus.CANCELLED: TaskStatus.CANCELLED\n            }\n            data['status'] = status_map[data['status']]\n    \n        # Store render-specific requirements in metadata\n        resource_requirements = {}\n        if 'cpu_requirements' in data:\n            resource_requirements['cpu'] = data['cpu_requirements']\n        if 'memory_requirements_gb' in data:\n            resource_requirements['memory'] = data['memory_requirements_gb']\n        if 'requires_gpu' in data and data['requires_gpu']:\n            resource_requirements['gpu'] = 1\n    \n        if 'metadata' not in data:\n            data['metadata'] = {}\n        data['metadata']['resource_requirements'] = resource_requirements\n        data['metadata']['deadline'] = data['deadline'].isoformat() if 'deadline' in data else None\n    \n>       super().__init__(**data)\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for RenderJob\nE       priority\nE         Input should be 1, 2, 3 or 4 [type=enum, input_value='high', input_type=str]\nE           For further information visit https://errors.pydantic.dev/2.11/v/enum\n\nrender_farm_manager/core/models.py:192: ValidationError"}, "teardown": {"duration": 0.0002600499428808689, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_allocate_resources_with_offline_nodes", "lineno": 273, "outcome": "passed", "keywords": ["test_allocate_resources_with_offline_nodes", "test_resource_partitioner.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0014347899705171585, "outcome": "passed"}, "call": {"duration": 0.001957752276211977, "outcome": "passed"}, "teardown": {"duration": 0.00018517905846238136, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_resource_allocation_scaling", "lineno": 295, "outcome": "passed", "keywords": ["test_resource_allocation_scaling", "test_resource_partitioner.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.001344979740679264, "outcome": "passed"}, "call": {"duration": 0.0021377871744334698, "outcome": "passed"}, "teardown": {"duration": 0.00018358975648880005, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_resource_allocation_special_hardware", "lineno": 313, "outcome": "passed", "keywords": ["test_resource_allocation_special_hardware", "test_resource_partitioner.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0013469699770212173, "outcome": "passed"}, "call": {"duration": 0.001967692282050848, "outcome": "passed"}, "teardown": {"duration": 0.00018248707056045532, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_allocate_resources_with_no_clients", "lineno": 334, "outcome": "passed", "keywords": ["test_allocate_resources_with_no_clients", "test_resource_partitioner.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.001299418043345213, "outcome": "passed"}, "call": {"duration": 0.0011304132640361786, "outcome": "passed"}, "teardown": {"duration": 0.0001733950339257717, "outcome": "passed"}}, {"nodeid": "tests/render_farm_manager/unit/test_resource_partitioner.py::test_allocate_resources_with_no_nodes", "lineno": 342, "outcome": "passed", "keywords": ["test_allocate_resources_with_no_nodes", "test_resource_partitioner.py", "unit", "render_farm_manager", "tests", "unified", ""], "setup": {"duration": 0.0010349219664931297, "outcome": "passed"}, "call": {"duration": 0.0027022790163755417, "outcome": "passed"}, "teardown": {"duration": 0.0001890989951789379, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_add_stage", "lineno": 54, "outcome": "passed", "keywords": ["test_add_stage", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.001139101106673479, "outcome": "passed"}, "call": {"duration": 0.00017010606825351715, "outcome": "passed"}, "teardown": {"duration": 0.0001431661657989025, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_add_dependency", "lineno": 59, "outcome": "passed", "keywords": ["test_add_dependency", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00029643671587109566, "outcome": "passed"}, "call": {"duration": 0.00017202924937009811, "outcome": "passed"}, "teardown": {"duration": 0.0001350841484963894, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_add_dependency_with_cycle", "lineno": 73, "outcome": "passed", "keywords": ["test_add_dependency_with_cycle", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002667410299181938, "outcome": "passed"}, "call": {"duration": 0.00015646405518054962, "outcome": "passed"}, "teardown": {"duration": 0.00012939702719449997, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_remove_dependency", "lineno": 79, "outcome": "passed", "keywords": ["test_remove_dependency", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002587130293250084, "outcome": "passed"}, "call": {"duration": 0.00014067580923438072, "outcome": "passed"}, "teardown": {"duration": 0.0001282799057662487, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_get_dependencies", "lineno": 85, "outcome": "passed", "keywords": ["test_get_dependencies", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00028584618121385574, "outcome": "passed"}, "call": {"duration": 0.00014232797548174858, "outcome": "passed"}, "teardown": {"duration": 0.00012864172458648682, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_get_root_and_leaf_stages", "lineno": 94, "outcome": "passed", "keywords": ["test_get_root_and_leaf_stages", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.000361613929271698, "outcome": "passed"}, "call": {"duration": 0.0001784488558769226, "outcome": "passed"}, "teardown": {"duration": 0.0001310775987803936, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_get_ready_stages", "lineno": 103, "outcome": "passed", "keywords": ["test_get_ready_stages", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002663792110979557, "outcome": "passed"}, "call": {"duration": 0.00017426861450076103, "outcome": "passed"}, "teardown": {"duration": 0.00012910319492220879, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_are_all_dependencies_satisfied", "lineno": 124, "outcome": "passed", "keywords": ["test_are_all_dependencies_satisfied", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00027342885732650757, "outcome": "passed"}, "call": {"duration": 0.00016662105917930603, "outcome": "passed"}, "teardown": {"duration": 0.0001297956332564354, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_update_stage_status", "lineno": 136, "outcome": "passed", "keywords": ["test_update_stage_status", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00028011808171868324, "outcome": "passed"}, "call": {"duration": 0.00015354296192526817, "outcome": "passed"}, "teardown": {"duration": 0.00012638326734304428, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_get_critical_path", "lineno": 146, "outcome": "passed", "keywords": ["test_get_critical_path", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002577207051217556, "outcome": "passed"}, "call": {"duration": 0.00045915599912405014, "outcome": "passed"}, "teardown": {"duration": 0.00014732824638485909, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_is_stage_blocked", "lineno": 157, "outcome": "passed", "keywords": ["test_is_stage_blocked", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002942732535302639, "outcome": "passed"}, "call": {"duration": 0.0001446758396923542, "outcome": "passed"}, "teardown": {"duration": 0.0001323358155786991, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_validate", "lineno": 167, "outcome": "passed", "keywords": ["test_validate", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.000285428948700428, "outcome": "passed"}, "call": {"duration": 0.00018218299373984337, "outcome": "passed"}, "teardown": {"duration": 0.00013496587052941322, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_serialize_deserialize", "lineno": 184, "outcome": "passed", "keywords": ["test_serialize_deserialize", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00027765706181526184, "outcome": "passed"}, "call": {"duration": 0.00024692993611097336, "outcome": "passed"}, "teardown": {"duration": 0.00013878010213375092, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_create_dependency_graph", "lineno": 267, "outcome": "passed", "keywords": ["test_create_dependency_graph", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002895439974963665, "outcome": "passed"}, "call": {"duration": 0.000299250241369009, "outcome": "passed"}, "teardown": {"duration": 0.00014261994510889053, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_add_dependency", "lineno": 285, "outcome": "passed", "keywords": ["test_add_dependency", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00027401186525821686, "outcome": "passed"}, "call": {"duration": 0.0002301330678164959, "outcome": "passed"}, "teardown": {"duration": 0.0001363246701657772, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_remove_dependency", "lineno": 308, "outcome": "passed", "keywords": ["test_remove_dependency", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002729720436036587, "outcome": "passed"}, "call": {"duration": 0.00021652504801750183, "outcome": "passed"}, "teardown": {"duration": 0.00013049691915512085, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_add_transition_rule", "lineno": 326, "outcome": "passed", "keywords": ["test_add_transition_rule", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002645989879965782, "outcome": "passed"}, "call": {"duration": 0.0002389843575656414, "outcome": "passed"}, "teardown": {"duration": 0.00013319915160536766, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_update_stage_status", "lineno": 350, "outcome": "failed", "keywords": ["test_update_stage_status", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00025645503774285316, "outcome": "passed"}, "call": {"duration": 0.0002615298144519329, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"SimulationStage\" object has no field \"end_time\""}, "traceback": [{"path": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py", "lineno": 357, "message": ""}, {"path": "concurrent_task_scheduler/dependency_tracking/tracker.py", "lineno": 289, "message": "in update_stage_status"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestDependencyTracker object at 0x7f49519277f0>\n\n    def test_update_stage_status(self):\n        \"\"\"Test updating a stage's status and propagating changes.\"\"\"\n        # Create dependency graph\n        self.tracker.create_dependency_graph(self.simulation)\n    \n        # Update stage-1 to completed\n>       result = self.tracker.update_stage_status(\n            self.simulation,\n            \"stage-1\",\n            SimulationStageStatus.COMPLETED,\n        )\n\ntests/scientific_computing/dependency_tracking/test_dependency_tracker.py:357: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nconcurrent_task_scheduler/dependency_tracking/tracker.py:289: in update_stage_status\n    stage.end_time = now\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = SimulationStage(id='stage-1', name='Stage 1', status=<SimulationStageStatus.COMPLETED: 'completed'>, priority=<Priorit...kpoint_frequency=datetime.timedelta(seconds=3600), last_checkpoint_time=None, checkpoint_path=None, error_message=None)\nname = 'end_time', value = datetime.datetime(2025, 6, 16, 4, 16, 45, 434589)\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"SimulationStage\" object has no field \"end_time\"\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.0001901509240269661, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_get_ready_stages", "lineno": 374, "outcome": "passed", "keywords": ["test_get_ready_stages", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00028994400054216385, "outcome": "passed"}, "call": {"duration": 0.00025985389947891235, "outcome": "passed"}, "teardown": {"duration": 0.00014702090993523598, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_is_stage_ready", "lineno": 392, "outcome": "failed", "keywords": ["test_is_stage_ready", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00027414876967668533, "outcome": "passed"}, "call": {"duration": 0.00044638384133577347, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/dependency_tracking/test_dependency_tracker.py", "lineno": 399, "message": "AssertionError: assert False\n +  where False = is_stage_ready(Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage-1': SimulationStage(id='stage-1',...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1)), 'stage-1')\n +    where is_stage_ready = <concurrent_task_scheduler.dependency_tracking.tracker.DependencyTracker object at 0x7f489dbebac0>.is_stage_ready\n +      where <concurrent_task_scheduler.dependency_tracking.tracker.DependencyTracker object at 0x7f489dbebac0> = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestDependencyTracker object at 0x7f4951927b50>.tracker\n +    and   Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage-1': SimulationStage(id='stage-1',...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1)) = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestDependencyTracker object at 0x7f4951927b50>.simulation"}, "traceback": [{"path": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py", "lineno": 399, "message": "AssertionError"}], "longrepr": "self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestDependencyTracker object at 0x7f4951927b50>\n\n    def test_is_stage_ready(self):\n        \"\"\"Test checking if a stage is ready to execute.\"\"\"\n        # Create dependency graph\n        self.tracker.create_dependency_graph(self.simulation)\n    \n        # Stage-1 should be ready\n>       assert self.tracker.is_stage_ready(self.simulation, \"stage-1\")\nE       AssertionError: assert False\nE        +  where False = is_stage_ready(Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage-1': SimulationStage(id='stage-1',...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1)), 'stage-1')\nE        +    where is_stage_ready = <concurrent_task_scheduler.dependency_tracking.tracker.DependencyTracker object at 0x7f489dbebac0>.is_stage_ready\nE        +      where <concurrent_task_scheduler.dependency_tracking.tracker.DependencyTracker object at 0x7f489dbebac0> = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestDependencyTracker object at 0x7f4951927b50>.tracker\nE        +    and   Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage-1': SimulationStage(id='stage-1',...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1)) = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestDependencyTracker object at 0x7f4951927b50>.simulation\n\ntests/scientific_computing/dependency_tracking/test_dependency_tracker.py:399: AssertionError"}, "teardown": {"duration": 0.00019380822777748108, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_get_blocking_stages", "lineno": 409, "outcome": "passed", "keywords": ["test_get_blocking_stages", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00029030488803982735, "outcome": "passed"}, "call": {"duration": 0.0002499236725270748, "outcome": "passed"}, "teardown": {"duration": 0.00014973198994994164, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_get_critical_path", "lineno": 425, "outcome": "passed", "keywords": ["test_get_critical_path", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002649822272360325, "outcome": "passed"}, "call": {"duration": 0.00023559574037790298, "outcome": "passed"}, "teardown": {"duration": 0.00013450020924210548, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_bypass_dependency", "lineno": 439, "outcome": "passed", "keywords": ["test_bypass_dependency", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00025611789897084236, "outcome": "passed"}, "call": {"duration": 0.00021898187696933746, "outcome": "passed"}, "teardown": {"duration": 0.00013433396816253662, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_add_dynamic_dependency", "lineno": 464, "outcome": "failed", "keywords": ["test_add_dynamic_dependency", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002641347236931324, "outcome": "passed"}, "call": {"duration": 0.00023987330496311188, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/dependency_tracking/tracker.py", "lineno": 491, "message": "AttributeError: 'list' object has no attribute 'add'"}, "traceback": [{"path": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py", "lineno": 471, "message": ""}, {"path": "concurrent_task_scheduler/dependency_tracking/tracker.py", "lineno": 491, "message": "AttributeError"}], "longrepr": "self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestDependencyTracker object at 0x7f4951970190>\n\n    def test_add_dynamic_dependency(self):\n        \"\"\"Test adding a dynamically discovered dependency at runtime.\"\"\"\n        # Create dependency graph\n        self.tracker.create_dependency_graph(self.simulation)\n    \n        # Add a dynamic dependency from stage-1 to stage-3\n>       result = self.tracker.add_dynamic_dependency(\n            self.simulation,\n            \"stage-1\",\n            \"stage-3\",\n            DependencyType.DATA,\n        )\n\ntests/scientific_computing/dependency_tracking/test_dependency_tracker.py:471: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <concurrent_task_scheduler.dependency_tracking.tracker.DependencyTracker object at 0x7f489e216380>\nsimulation = Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage-1': SimulationStage(id='stage-1',...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\nfrom_stage_id = 'stage-1', to_stage_id = 'stage-3'\ndependency_type = <DependencyType.DATA: 'data'>, condition = None\n\n    def add_dynamic_dependency(\n        self,\n        simulation: Simulation,\n        from_stage_id: str,\n        to_stage_id: str,\n        dependency_type: DependencyType = DependencyType.DATA,\n        condition: Optional[str] = None,\n    ) -> Result[bool]:\n        \"\"\"Add a dynamically discovered dependency at runtime.\"\"\"\n        # Validate stages\n        if from_stage_id not in simulation.stages:\n            return Result.err(f\"Stage {from_stage_id} not found in simulation {simulation.id}\")\n    \n        if to_stage_id not in simulation.stages:\n            return Result.err(f\"Stage {to_stage_id} not found in simulation {simulation.id}\")\n    \n        # Add to simulation's stage dependencies\n        to_stage = simulation.stages[to_stage_id]\n        if from_stage_id not in to_stage.dependencies:\n>           to_stage.dependencies.add(from_stage_id)\nE           AttributeError: 'list' object has no attribute 'add'\n\nconcurrent_task_scheduler/dependency_tracking/tracker.py:491: AttributeError"}, "teardown": {"duration": 0.00018166517838835716, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_get_execution_plan", "lineno": 490, "outcome": "passed", "keywords": ["test_get_execution_plan", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.000286945141851902, "outcome": "passed"}, "call": {"duration": 0.0002340148203074932, "outcome": "passed"}, "teardown": {"duration": 0.00013390975072979927, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_validate_simulation", "lineno": 506, "outcome": "failed", "keywords": ["test_validate_simulation", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0004188353195786476, "outcome": "passed"}, "call": {"duration": 0.000191601924598217, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/dependency_tracking/test_dependency_tracker.py", "lineno": 514, "message": "AttributeError: 'list' object has no attribute 'add'"}, "traceback": [{"path": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py", "lineno": 514, "message": "AttributeError"}], "longrepr": "self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestDependencyTracker object at 0x7f49519704f0>\n\n    def test_validate_simulation(self):\n        \"\"\"Test validating the simulation dependency structure.\"\"\"\n        # The test simulation should be valid\n        errors = self.tracker.validate_simulation(self.simulation)\n        assert not errors\n    \n        # Add a dependency to a non-existent stage\n>       self.simulation.stages[\"stage-2\"].dependencies.add(\"non-existent-stage\")\nE       AttributeError: 'list' object has no attribute 'add'\n\ntests/scientific_computing/dependency_tracking/test_dependency_tracker.py:514: AttributeError"}, "teardown": {"duration": 0.00020068185403943062, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_register_template", "lineno": 540, "outcome": "passed", "keywords": ["test_register_template", "TestWorkflowManager", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00023868726566433907, "outcome": "passed"}, "call": {"duration": 0.00023754220455884933, "outcome": "passed"}, "teardown": {"duration": 0.0001491829752922058, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_create_instance", "lineno": 555, "outcome": "failed", "keywords": ["test_create_instance", "TestWorkflowManager", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00022579822689294815, "outcome": "passed"}, "call": {"duration": 0.0003384300507605076, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 665, "message": "AttributeError: 'list' object has no attribute 'add'"}, "traceback": [{"path": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py", "lineno": 568, "message": ""}, {"path": "concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 665, "message": "AttributeError"}], "longrepr": "self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestWorkflowManager object at 0x7f4951970ac0>\n\n    def test_create_instance(self):\n        \"\"\"Test creating a workflow instance from a template.\"\"\"\n        # Create a simple sequential template\n        template = WorkflowTemplate.create_sequential(\n            name=\"Test Sequential Workflow\",\n            stage_names=[\"Stage 1\", \"Stage 2\", \"Stage 3\"],\n        )\n    \n        # Register the template\n        self.manager.register_template(template)\n    \n        # Create an instance\n>       result = self.manager.create_instance(template.id, self.simulation)\n\ntests/scientific_computing/dependency_tracking/test_dependency_tracker.py:568: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <concurrent_task_scheduler.dependency_tracking.workflow.WorkflowManager object at 0x7f489e022c20>\ntemplate_id = 'wf_template_20250616041645_e936338217241904'\nsimulation = Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage_20250616041645_4386962402cbcfcb':...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\nparameters = None\n\n    def create_instance(\n        self,\n        template_id: str,\n        simulation: Simulation,\n        parameters: Optional[Dict[str, Any]] = None,\n    ) -> Result[WorkflowInstance]:\n        \"\"\"Create a workflow instance from a template.\"\"\"\n        if template_id not in self.templates:\n            return Result.err(f\"Template {template_id} not found\")\n    \n        template = self.templates[template_id]\n    \n        # Create instance\n        instance_id = generate_id(\"wf_instance\")\n        instance = WorkflowInstance(\n            instance_id=instance_id,\n            template=template,\n            simulation=simulation,\n        )\n    \n        if parameters:\n            instance.parameters = parameters.copy()\n    \n        # Map stages\n        for template_stage_id, template_stage in template.stages.items():\n            # Check if stage already exists in simulation\n            existing_stage = None\n            for stage_id, stage in simulation.stages.items():\n                if stage.name == template_stage.name:\n                    existing_stage = stage_id\n                    break\n    \n            if existing_stage:\n                # Use existing stage\n                instance.map_stage(template_stage_id, existing_stage)\n            else:\n                # Create new stage\n                stage_id = generate_id(\"stage\")\n    \n                # Convert from timedelta to hours for estimated_duration\n                estimated_duration_hours = template_stage.estimated_duration.total_seconds() / 3600\n    \n                # Create stage in simulation\n                stage = SimulationStage(\n                    id=stage_id,\n                    name=template_stage.name,\n                    description=template_stage.description,\n                    estimated_duration=timedelta(hours=estimated_duration_hours),\n                    resource_requirements=[],  # Would need conversion from template format\n                    status=SimulationStageStatus.PENDING,\n                )\n    \n                simulation.stages[stage_id] = stage\n                instance.map_stage(template_stage_id, stage_id)\n    \n        # Set up dependencies based on template transitions\n        for transition in template.transitions:\n            from_sim_stage = instance.get_simulation_stage_id(transition.from_stage_id)\n            to_sim_stage = instance.get_simulation_stage_id(transition.to_stage_id)\n    \n            if from_sim_stage and to_sim_stage:\n                # Add dependency in simulation\n>               simulation.stages[to_sim_stage].dependencies.add(from_sim_stage)\nE               AttributeError: 'list' object has no attribute 'add'\n\nconcurrent_task_scheduler/dependency_tracking/workflow.py:665: AttributeError"}, "teardown": {"duration": 0.0001973598264157772, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_create_linear_workflow", "lineno": 595, "outcome": "failed", "keywords": ["test_create_linear_workflow", "TestWorkflowManager", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002337200567126274, "outcome": "passed"}, "call": {"duration": 0.00034840498119592667, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 665, "message": "AttributeError: 'list' object has no attribute 'add'"}, "traceback": [{"path": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py", "lineno": 599, "message": ""}, {"path": "concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 857, "message": "in create_linear_workflow"}, {"path": "concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 665, "message": "AttributeError"}], "longrepr": "self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestWorkflowManager object at 0x7f4951970c70>\n\n    def test_create_linear_workflow(self):\n        \"\"\"Test creating a simple linear workflow.\"\"\"\n        # Create a linear workflow\n>       result = self.manager.create_linear_workflow(\n            self.simulation,\n            [\"Stage A\", \"Stage B\", \"Stage C\"],\n        )\n\ntests/scientific_computing/dependency_tracking/test_dependency_tracker.py:599: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nconcurrent_task_scheduler/dependency_tracking/workflow.py:857: in create_linear_workflow\n    instance_result = self.create_instance(template.id, simulation)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <concurrent_task_scheduler.dependency_tracking.workflow.WorkflowManager object at 0x7f489e2a8700>\ntemplate_id = 'wf_template_20250616041645_f8177b9e962497d6'\nsimulation = Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage_20250616041645_c4a49a80307ed389':...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\nparameters = None\n\n    def create_instance(\n        self,\n        template_id: str,\n        simulation: Simulation,\n        parameters: Optional[Dict[str, Any]] = None,\n    ) -> Result[WorkflowInstance]:\n        \"\"\"Create a workflow instance from a template.\"\"\"\n        if template_id not in self.templates:\n            return Result.err(f\"Template {template_id} not found\")\n    \n        template = self.templates[template_id]\n    \n        # Create instance\n        instance_id = generate_id(\"wf_instance\")\n        instance = WorkflowInstance(\n            instance_id=instance_id,\n            template=template,\n            simulation=simulation,\n        )\n    \n        if parameters:\n            instance.parameters = parameters.copy()\n    \n        # Map stages\n        for template_stage_id, template_stage in template.stages.items():\n            # Check if stage already exists in simulation\n            existing_stage = None\n            for stage_id, stage in simulation.stages.items():\n                if stage.name == template_stage.name:\n                    existing_stage = stage_id\n                    break\n    \n            if existing_stage:\n                # Use existing stage\n                instance.map_stage(template_stage_id, existing_stage)\n            else:\n                # Create new stage\n                stage_id = generate_id(\"stage\")\n    \n                # Convert from timedelta to hours for estimated_duration\n                estimated_duration_hours = template_stage.estimated_duration.total_seconds() / 3600\n    \n                # Create stage in simulation\n                stage = SimulationStage(\n                    id=stage_id,\n                    name=template_stage.name,\n                    description=template_stage.description,\n                    estimated_duration=timedelta(hours=estimated_duration_hours),\n                    resource_requirements=[],  # Would need conversion from template format\n                    status=SimulationStageStatus.PENDING,\n                )\n    \n                simulation.stages[stage_id] = stage\n                instance.map_stage(template_stage_id, stage_id)\n    \n        # Set up dependencies based on template transitions\n        for transition in template.transitions:\n            from_sim_stage = instance.get_simulation_stage_id(transition.from_stage_id)\n            to_sim_stage = instance.get_simulation_stage_id(transition.to_stage_id)\n    \n            if from_sim_stage and to_sim_stage:\n                # Add dependency in simulation\n>               simulation.stages[to_sim_stage].dependencies.add(from_sim_stage)\nE               AttributeError: 'list' object has no attribute 'add'\n\nconcurrent_task_scheduler/dependency_tracking/workflow.py:665: AttributeError"}, "teardown": {"duration": 0.00019529089331626892, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_create_parallel_workflow", "lineno": 636, "outcome": "failed", "keywords": ["test_create_parallel_workflow", "TestWorkflowManager", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00024079298600554466, "outcome": "passed"}, "call": {"duration": 0.0004005851224064827, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 665, "message": "AttributeError: 'list' object has no attribute 'add'"}, "traceback": [{"path": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py", "lineno": 640, "message": ""}, {"path": "concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 925, "message": "in create_parallel_workflow"}, {"path": "concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 665, "message": "AttributeError"}], "longrepr": "self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestWorkflowManager object at 0x7f4951970e20>\n\n    def test_create_parallel_workflow(self):\n        \"\"\"Test creating a simple parallel workflow.\"\"\"\n        # Create a parallel workflow\n>       result = self.manager.create_parallel_workflow(\n            self.simulation,\n            [\"Stage X\", \"Stage Y\", \"Stage Z\"],\n        )\n\ntests/scientific_computing/dependency_tracking/test_dependency_tracker.py:640: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nconcurrent_task_scheduler/dependency_tracking/workflow.py:925: in create_parallel_workflow\n    return self.create_instance(template.id, simulation)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <concurrent_task_scheduler.dependency_tracking.workflow.WorkflowManager object at 0x7f489dd1de10>\ntemplate_id = 'wf_template_20250616041645_98cf629cb748e58d'\nsimulation = Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage_20250616041645_383008864c7e1c59':...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\nparameters = None\n\n    def create_instance(\n        self,\n        template_id: str,\n        simulation: Simulation,\n        parameters: Optional[Dict[str, Any]] = None,\n    ) -> Result[WorkflowInstance]:\n        \"\"\"Create a workflow instance from a template.\"\"\"\n        if template_id not in self.templates:\n            return Result.err(f\"Template {template_id} not found\")\n    \n        template = self.templates[template_id]\n    \n        # Create instance\n        instance_id = generate_id(\"wf_instance\")\n        instance = WorkflowInstance(\n            instance_id=instance_id,\n            template=template,\n            simulation=simulation,\n        )\n    \n        if parameters:\n            instance.parameters = parameters.copy()\n    \n        # Map stages\n        for template_stage_id, template_stage in template.stages.items():\n            # Check if stage already exists in simulation\n            existing_stage = None\n            for stage_id, stage in simulation.stages.items():\n                if stage.name == template_stage.name:\n                    existing_stage = stage_id\n                    break\n    \n            if existing_stage:\n                # Use existing stage\n                instance.map_stage(template_stage_id, existing_stage)\n            else:\n                # Create new stage\n                stage_id = generate_id(\"stage\")\n    \n                # Convert from timedelta to hours for estimated_duration\n                estimated_duration_hours = template_stage.estimated_duration.total_seconds() / 3600\n    \n                # Create stage in simulation\n                stage = SimulationStage(\n                    id=stage_id,\n                    name=template_stage.name,\n                    description=template_stage.description,\n                    estimated_duration=timedelta(hours=estimated_duration_hours),\n                    resource_requirements=[],  # Would need conversion from template format\n                    status=SimulationStageStatus.PENDING,\n                )\n    \n                simulation.stages[stage_id] = stage\n                instance.map_stage(template_stage_id, stage_id)\n    \n        # Set up dependencies based on template transitions\n        for transition in template.transitions:\n            from_sim_stage = instance.get_simulation_stage_id(transition.from_stage_id)\n            to_sim_stage = instance.get_simulation_stage_id(transition.to_stage_id)\n    \n            if from_sim_stage and to_sim_stage:\n                # Add dependency in simulation\n>               simulation.stages[to_sim_stage].dependencies.add(from_sim_stage)\nE               AttributeError: 'list' object has no attribute 'add'\n\nconcurrent_task_scheduler/dependency_tracking/workflow.py:665: AttributeError"}, "teardown": {"duration": 0.00023189745843410492, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_get_next_stages", "lineno": 673, "outcome": "failed", "keywords": ["test_get_next_stages", "TestWorkflowManager", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002717329189181328, "outcome": "passed"}, "call": {"duration": 0.00035845162346959114, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 665, "message": "AttributeError: 'list' object has no attribute 'add'"}, "traceback": [{"path": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py", "lineno": 677, "message": ""}, {"path": "concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 857, "message": "in create_linear_workflow"}, {"path": "concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 665, "message": "AttributeError"}], "longrepr": "self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestWorkflowManager object at 0x7f4951927ee0>\n\n    def test_get_next_stages(self):\n        \"\"\"Test getting the next stages to execute in a workflow instance.\"\"\"\n        # Create a linear workflow\n>       result = self.manager.create_linear_workflow(\n            self.simulation,\n            [\"Stage 1\", \"Stage 2\", \"Stage 3\"],\n        )\n\ntests/scientific_computing/dependency_tracking/test_dependency_tracker.py:677: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nconcurrent_task_scheduler/dependency_tracking/workflow.py:857: in create_linear_workflow\n    instance_result = self.create_instance(template.id, simulation)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <concurrent_task_scheduler.dependency_tracking.workflow.WorkflowManager object at 0x7f49f43e7670>\ntemplate_id = 'wf_template_20250616041645_6f9ee36f5e1407d5'\nsimulation = Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage_20250616041645_cc9b998e9d4c9622':...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\nparameters = None\n\n    def create_instance(\n        self,\n        template_id: str,\n        simulation: Simulation,\n        parameters: Optional[Dict[str, Any]] = None,\n    ) -> Result[WorkflowInstance]:\n        \"\"\"Create a workflow instance from a template.\"\"\"\n        if template_id not in self.templates:\n            return Result.err(f\"Template {template_id} not found\")\n    \n        template = self.templates[template_id]\n    \n        # Create instance\n        instance_id = generate_id(\"wf_instance\")\n        instance = WorkflowInstance(\n            instance_id=instance_id,\n            template=template,\n            simulation=simulation,\n        )\n    \n        if parameters:\n            instance.parameters = parameters.copy()\n    \n        # Map stages\n        for template_stage_id, template_stage in template.stages.items():\n            # Check if stage already exists in simulation\n            existing_stage = None\n            for stage_id, stage in simulation.stages.items():\n                if stage.name == template_stage.name:\n                    existing_stage = stage_id\n                    break\n    \n            if existing_stage:\n                # Use existing stage\n                instance.map_stage(template_stage_id, existing_stage)\n            else:\n                # Create new stage\n                stage_id = generate_id(\"stage\")\n    \n                # Convert from timedelta to hours for estimated_duration\n                estimated_duration_hours = template_stage.estimated_duration.total_seconds() / 3600\n    \n                # Create stage in simulation\n                stage = SimulationStage(\n                    id=stage_id,\n                    name=template_stage.name,\n                    description=template_stage.description,\n                    estimated_duration=timedelta(hours=estimated_duration_hours),\n                    resource_requirements=[],  # Would need conversion from template format\n                    status=SimulationStageStatus.PENDING,\n                )\n    \n                simulation.stages[stage_id] = stage\n                instance.map_stage(template_stage_id, stage_id)\n    \n        # Set up dependencies based on template transitions\n        for transition in template.transitions:\n            from_sim_stage = instance.get_simulation_stage_id(transition.from_stage_id)\n            to_sim_stage = instance.get_simulation_stage_id(transition.to_stage_id)\n    \n            if from_sim_stage and to_sim_stage:\n                # Add dependency in simulation\n>               simulation.stages[to_sim_stage].dependencies.add(from_sim_stage)\nE               AttributeError: 'list' object has no attribute 'add'\n\nconcurrent_task_scheduler/dependency_tracking/workflow.py:665: AttributeError"}, "teardown": {"duration": 0.00019175000488758087, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_update_instance", "lineno": 726, "outcome": "failed", "keywords": ["test_update_instance", "TestWorkflowManager", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00025664782151579857, "outcome": "passed"}, "call": {"duration": 0.0003462000750005245, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 665, "message": "AttributeError: 'list' object has no attribute 'add'"}, "traceback": [{"path": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py", "lineno": 730, "message": ""}, {"path": "concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 857, "message": "in create_linear_workflow"}, {"path": "concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 665, "message": "AttributeError"}], "longrepr": "self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestWorkflowManager object at 0x7f4951927460>\n\n    def test_update_instance(self):\n        \"\"\"Test updating a stage status in a workflow instance.\"\"\"\n        # Create a linear workflow\n>       result = self.manager.create_linear_workflow(\n            self.simulation,\n            [\"Stage 1\", \"Stage 2\", \"Stage 3\"],\n        )\n\ntests/scientific_computing/dependency_tracking/test_dependency_tracker.py:730: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nconcurrent_task_scheduler/dependency_tracking/workflow.py:857: in create_linear_workflow\n    instance_result = self.create_instance(template.id, simulation)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <concurrent_task_scheduler.dependency_tracking.workflow.WorkflowManager object at 0x7f489dc26bc0>\ntemplate_id = 'wf_template_20250616041645_67288c114aa573e6'\nsimulation = Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage_20250616041645_3c3986dafad943be':...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\nparameters = None\n\n    def create_instance(\n        self,\n        template_id: str,\n        simulation: Simulation,\n        parameters: Optional[Dict[str, Any]] = None,\n    ) -> Result[WorkflowInstance]:\n        \"\"\"Create a workflow instance from a template.\"\"\"\n        if template_id not in self.templates:\n            return Result.err(f\"Template {template_id} not found\")\n    \n        template = self.templates[template_id]\n    \n        # Create instance\n        instance_id = generate_id(\"wf_instance\")\n        instance = WorkflowInstance(\n            instance_id=instance_id,\n            template=template,\n            simulation=simulation,\n        )\n    \n        if parameters:\n            instance.parameters = parameters.copy()\n    \n        # Map stages\n        for template_stage_id, template_stage in template.stages.items():\n            # Check if stage already exists in simulation\n            existing_stage = None\n            for stage_id, stage in simulation.stages.items():\n                if stage.name == template_stage.name:\n                    existing_stage = stage_id\n                    break\n    \n            if existing_stage:\n                # Use existing stage\n                instance.map_stage(template_stage_id, existing_stage)\n            else:\n                # Create new stage\n                stage_id = generate_id(\"stage\")\n    \n                # Convert from timedelta to hours for estimated_duration\n                estimated_duration_hours = template_stage.estimated_duration.total_seconds() / 3600\n    \n                # Create stage in simulation\n                stage = SimulationStage(\n                    id=stage_id,\n                    name=template_stage.name,\n                    description=template_stage.description,\n                    estimated_duration=timedelta(hours=estimated_duration_hours),\n                    resource_requirements=[],  # Would need conversion from template format\n                    status=SimulationStageStatus.PENDING,\n                )\n    \n                simulation.stages[stage_id] = stage\n                instance.map_stage(template_stage_id, stage_id)\n    \n        # Set up dependencies based on template transitions\n        for transition in template.transitions:\n            from_sim_stage = instance.get_simulation_stage_id(transition.from_stage_id)\n            to_sim_stage = instance.get_simulation_stage_id(transition.to_stage_id)\n    \n            if from_sim_stage and to_sim_stage:\n                # Add dependency in simulation\n>               simulation.stages[to_sim_stage].dependencies.add(from_sim_stage)\nE               AttributeError: 'list' object has no attribute 'add'\n\nconcurrent_task_scheduler/dependency_tracking/workflow.py:665: AttributeError"}, "teardown": {"duration": 0.00019102590158581734, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_get_workflow_status", "lineno": 781, "outcome": "failed", "keywords": ["test_get_workflow_status", "TestWorkflowManager", "test_dependency_tracker.py", "dependency_tracking", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.000254941638559103, "outcome": "passed"}, "call": {"duration": 0.0003422689624130726, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 665, "message": "AttributeError: 'list' object has no attribute 'add'"}, "traceback": [{"path": "tests/scientific_computing/dependency_tracking/test_dependency_tracker.py", "lineno": 785, "message": ""}, {"path": "concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 857, "message": "in create_linear_workflow"}, {"path": "concurrent_task_scheduler/dependency_tracking/workflow.py", "lineno": 665, "message": "AttributeError"}], "longrepr": "self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestWorkflowManager object at 0x7f4951926740>\n\n    def test_get_workflow_status(self):\n        \"\"\"Test getting the current status of a workflow instance.\"\"\"\n        # Create a linear workflow\n>       result = self.manager.create_linear_workflow(\n            self.simulation,\n            [\"Stage 1\", \"Stage 2\", \"Stage 3\"],\n        )\n\ntests/scientific_computing/dependency_tracking/test_dependency_tracker.py:785: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nconcurrent_task_scheduler/dependency_tracking/workflow.py:857: in create_linear_workflow\n    instance_result = self.create_instance(template.id, simulation)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <concurrent_task_scheduler.dependency_tracking.workflow.WorkflowManager object at 0x7f489e139f60>\ntemplate_id = 'wf_template_20250616041645_17fe7b6643d4951a'\nsimulation = Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage_20250616041645_4d96bd7321fc5ea3':...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\nparameters = None\n\n    def create_instance(\n        self,\n        template_id: str,\n        simulation: Simulation,\n        parameters: Optional[Dict[str, Any]] = None,\n    ) -> Result[WorkflowInstance]:\n        \"\"\"Create a workflow instance from a template.\"\"\"\n        if template_id not in self.templates:\n            return Result.err(f\"Template {template_id} not found\")\n    \n        template = self.templates[template_id]\n    \n        # Create instance\n        instance_id = generate_id(\"wf_instance\")\n        instance = WorkflowInstance(\n            instance_id=instance_id,\n            template=template,\n            simulation=simulation,\n        )\n    \n        if parameters:\n            instance.parameters = parameters.copy()\n    \n        # Map stages\n        for template_stage_id, template_stage in template.stages.items():\n            # Check if stage already exists in simulation\n            existing_stage = None\n            for stage_id, stage in simulation.stages.items():\n                if stage.name == template_stage.name:\n                    existing_stage = stage_id\n                    break\n    \n            if existing_stage:\n                # Use existing stage\n                instance.map_stage(template_stage_id, existing_stage)\n            else:\n                # Create new stage\n                stage_id = generate_id(\"stage\")\n    \n                # Convert from timedelta to hours for estimated_duration\n                estimated_duration_hours = template_stage.estimated_duration.total_seconds() / 3600\n    \n                # Create stage in simulation\n                stage = SimulationStage(\n                    id=stage_id,\n                    name=template_stage.name,\n                    description=template_stage.description,\n                    estimated_duration=timedelta(hours=estimated_duration_hours),\n                    resource_requirements=[],  # Would need conversion from template format\n                    status=SimulationStageStatus.PENDING,\n                )\n    \n                simulation.stages[stage_id] = stage\n                instance.map_stage(template_stage_id, stage_id)\n    \n        # Set up dependencies based on template transitions\n        for transition in template.transitions:\n            from_sim_stage = instance.get_simulation_stage_id(transition.from_stage_id)\n            to_sim_stage = instance.get_simulation_stage_id(transition.to_stage_id)\n    \n            if from_sim_stage and to_sim_stage:\n                # Add dependency in simulation\n>               simulation.stages[to_sim_stage].dependencies.add(from_sim_stage)\nE               AttributeError: 'list' object has no attribute 'add'\n\nconcurrent_task_scheduler/dependency_tracking/workflow.py:665: AttributeError"}, "teardown": {"duration": 0.0002021966502070427, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_checkpoint_manager_init", "lineno": 105, "outcome": "passed", "keywords": ["test_checkpoint_manager_init", "test_checkpoint_manager.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0006746742874383926, "outcome": "passed"}, "call": {"duration": 0.00016832118853926659, "outcome": "passed"}, "teardown": {"duration": 0.0002535209059715271, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_register_policy", "lineno": 115, "outcome": "passed", "keywords": ["test_register_policy", "test_checkpoint_manager.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00035162828862667084, "outcome": "passed"}, "call": {"duration": 0.00019525503739714622, "outcome": "passed"}, "teardown": {"duration": 0.0002201749011874199, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_get_default_policy", "lineno": 133, "outcome": "passed", "keywords": ["test_get_default_policy", "test_checkpoint_manager.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00034689297899603844, "outcome": "passed"}, "call": {"duration": 0.00016091996803879738, "outcome": "passed"}, "teardown": {"duration": 0.00020857993513345718, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_create_manager_for_simulation", "lineno": 148, "outcome": "passed", "keywords": ["test_create_manager_for_simulation", "test_checkpoint_manager.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.000474459957331419, "outcome": "passed"}, "call": {"duration": 0.00025416817516088486, "outcome": "passed"}, "teardown": {"duration": 0.0003108070231974125, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_update_policy", "lineno": 182, "outcome": "passed", "keywords": ["test_update_policy", "test_checkpoint_manager.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0004457230679690838, "outcome": "passed"}, "call": {"duration": 0.0002488452009856701, "outcome": "passed"}, "teardown": {"duration": 0.000263826921582222, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_should_create_checkpoint", "lineno": 217, "outcome": "passed", "keywords": ["test_should_create_checkpoint", "test_checkpoint_manager.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00047222571447491646, "outcome": "passed"}, "call": {"duration": 0.00023440318182110786, "outcome": "passed"}, "teardown": {"duration": 0.00026606908068060875, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_create_checkpoint", "lineno": 244, "outcome": "passed", "keywords": ["test_create_checkpoint", "test_checkpoint_manager.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0004649590700864792, "outcome": "passed"}, "call": {"duration": 0.010005543939769268, "outcome": "passed"}, "teardown": {"duration": 0.0006590173579752445, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_validate_checkpoint", "lineno": 284, "outcome": "xfailed", "keywords": ["test_validate_checkpoint", "xfail", "pytestmark", "test_checkpoint_manager.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0004869019612669945, "outcome": "passed"}, "call": {"duration": 0.012831662781536579, "outcome": "skipped", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/failure_resilience/test_checkpoint_manager.py", "lineno": 316, "message": "AssertionError: assert <ValidationRe...ALID: 'valid'> == <ValidationRe... 'incomplete'>\n  \n  - incomplete\n  + valid"}, "traceback": [{"path": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py", "lineno": 316, "message": "AssertionError"}], "log": [{"name": "concurrent_task_scheduler.failure_resilience.checkpoint_manager", "msg": "Checkpoint nonexistent not found for simulation sim_climate_test", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/failure_resilience/checkpoint_manager.py", "filename": "checkpoint_manager.py", "module": "checkpoint_manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 421, "funcName": "validate_checkpoint", "created": 1750047405.7417016, "msecs": 741.0, "relativeCreated": 3338.151693344116, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}], "longrepr": "checkpoint_manager = <concurrent_task_scheduler.failure_resilience.checkpoint_manager.CheckpointManager object at 0x7f489e49b820>\nsample_simulation = Simulation(id='sim_climate_test', name='Climate Model Test', description=None, stages={'stage_preprocess': SimulationS...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\n\n    @pytest.mark.xfail(reason=\"Checkpoint validation not fully implemented yet\")\n    def test_validate_checkpoint(checkpoint_manager, sample_simulation):\n        \"\"\"Test validating a checkpoint.\"\"\"\n        # Create checkpoint first\n        result = checkpoint_manager.create_checkpoint(sample_simulation)\n        assert result.success\n        checkpoint = result.value\n    \n        # Validate checkpoint\n        validation_result = checkpoint_manager.validate_checkpoint(checkpoint.id, sample_simulation.id)\n        assert validation_result == ValidationResult.VALID\n    \n        # Test validation caching\n        # This call should use the cached result\n        validation_result = checkpoint_manager.validate_checkpoint(checkpoint.id, sample_simulation.id)\n        assert validation_result == ValidationResult.VALID\n    \n        # Test with non-existent checkpoint\n        validation_result = checkpoint_manager.validate_checkpoint(\"nonexistent\", sample_simulation.id)\n        assert validation_result == ValidationResult.MISSING\n    \n        # Test with incomplete checkpoint\n        # Create a checkpoint, then delete one of the required files\n        result = checkpoint_manager.create_checkpoint(sample_simulation, description=\"Incomplete test\")\n        assert result.success\n        checkpoint2 = result.value\n    \n        # Delete the data file\n        os.remove(os.path.join(checkpoint2.path, \"data.bin\"))\n    \n        validation_result = checkpoint_manager.validate_checkpoint(checkpoint2.id, sample_simulation.id)\n>       assert validation_result == ValidationResult.INCOMPLETE\nE       AssertionError: assert <ValidationRe...ALID: 'valid'> == <ValidationRe... 'incomplete'>\nE         \nE         - incomplete\nE         + valid\n\ntests/scientific_computing/failure_resilience/test_checkpoint_manager.py:316: AssertionError"}, "teardown": {"duration": 0.00038739293813705444, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_get_latest_checkpoint", "lineno": 330, "outcome": "passed", "keywords": ["test_get_latest_checkpoint", "test_checkpoint_manager.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00047765299677848816, "outcome": "passed"}, "call": {"duration": 0.014359504915773869, "outcome": "passed"}, "teardown": {"duration": 0.0006472822278738022, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_get_all_checkpoints", "lineno": 360, "outcome": "passed", "keywords": ["test_get_all_checkpoints", "test_checkpoint_manager.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00048152683302760124, "outcome": "passed"}, "call": {"duration": 0.014107096940279007, "outcome": "passed"}, "teardown": {"duration": 0.0006240671500563622, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_restore_from_checkpoint", "lineno": 383, "outcome": "xfailed", "keywords": ["test_restore_from_checkpoint", "xfail", "pytestmark", "test_checkpoint_manager.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00048353569582104683, "outcome": "passed"}, "call": {"duration": 0.012264040298759937, "outcome": "skipped", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/failure_resilience/test_checkpoint_manager.py", "lineno": 415, "message": "assert not True\n +  where True = Result(success=True, value=True, error=None).success"}, "traceback": [{"path": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py", "lineno": 415, "message": "AssertionError"}], "longrepr": "checkpoint_manager = <concurrent_task_scheduler.failure_resilience.checkpoint_manager.CheckpointManager object at 0x7f489d1ff100>\nsample_simulation = Simulation(id='sim_climate_test', name='Climate Model Test', description=None, stages={'stage_preprocess': SimulationS...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\n\n    @pytest.mark.xfail(reason=\"Checkpoint validation and restoration not fully implemented yet\")\n    def test_restore_from_checkpoint(checkpoint_manager, sample_simulation):\n        \"\"\"Test restoring a simulation from a checkpoint.\"\"\"\n        # Create checkpoint first\n        result = checkpoint_manager.create_checkpoint(sample_simulation)\n        assert result.success\n        checkpoint = result.value\n    \n        # Restore\n        restore_result = checkpoint_manager.restore_from_checkpoint(checkpoint.id, sample_simulation.id)\n        assert restore_result.success\n    \n        # Verify restore count was incremented\n        checkpoint = checkpoint_manager.checkpoints[sample_simulation.id][checkpoint.id]\n        assert checkpoint.restore_count == 1\n        assert checkpoint.last_restore_time is not None\n    \n        # Test with non-existent checkpoint\n        restore_result = checkpoint_manager.restore_from_checkpoint(\"nonexistent\", sample_simulation.id)\n        assert not restore_result.success\n    \n        # Test with invalid checkpoint\n        # Create a checkpoint, then delete one of the required files\n        result = checkpoint_manager.create_checkpoint(sample_simulation)\n        assert result.success\n        checkpoint2 = result.value\n    \n        # Delete the data file\n        os.remove(os.path.join(checkpoint2.path, \"data.bin\"))\n    \n        restore_result = checkpoint_manager.restore_from_checkpoint(checkpoint2.id, sample_simulation.id)\n>       assert not restore_result.success\nE       assert not True\nE        +  where True = Result(success=True, value=True, error=None).success\n\ntests/scientific_computing/failure_resilience/test_checkpoint_manager.py:415: AssertionError"}, "teardown": {"duration": 0.00038141990080475807, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_checkpoint_manager.py::test_checkpoint_coordinator", "lineno": 418, "outcome": "passed", "keywords": ["test_checkpoint_coordinator", "test_checkpoint_manager.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0005207122303545475, "outcome": "passed"}, "call": {"duration": 0.004771015606820583, "outcome": "passed"}, "teardown": {"duration": 0.0004752376116812229, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_failure_detector_init", "lineno": 110, "outcome": "passed", "keywords": ["test_failure_detector_init", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00021835975348949432, "outcome": "passed"}, "call": {"duration": 0.00014198431745171547, "outcome": "passed"}, "teardown": {"duration": 0.00012675300240516663, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_record_heartbeat", "lineno": 121, "outcome": "passed", "keywords": ["test_record_heartbeat", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00017002318054437637, "outcome": "passed"}, "call": {"duration": 0.00013225479051470757, "outcome": "passed"}, "teardown": {"duration": 0.0001202472485601902, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_check_node_health", "lineno": 130, "outcome": "failed", "keywords": ["test_check_node_health", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00025344034656882286, "outcome": "passed"}, "call": {"duration": 0.00023240968585014343, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/failure_resilience/test_failure_detector.py", "lineno": 146, "message": "assert False\n +  where False = is_healthy()\n +    where is_healthy = <concurrent_task_scheduler.failure_resilience.failure_detector.NodeHealthCheck object at 0x7f489dc30640>.is_healthy"}, "traceback": [{"path": "tests/scientific_computing/failure_resilience/test_failure_detector.py", "lineno": 146, "message": "AssertionError"}], "longrepr": "failure_detector = <concurrent_task_scheduler.failure_resilience.failure_detector.FailureDetector object at 0x7f489dc300d0>\nsample_compute_node = ComputeNode(id='node_001', name='Compute Node 1', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0,...igned_simulations=[], last_failure_time=None, maintenance_window=None, location='data_center_1', reliability_score=1.0)\n\n    def test_check_node_health(failure_detector, sample_compute_node):\n        \"\"\"Test checking the health of a node.\"\"\"\n        # First record a heartbeat so it's healthy\n        failure_detector.record_heartbeat(sample_compute_node.id)\n    \n        # Check health\n        health_check = failure_detector.check_node_health(sample_compute_node)\n    \n        assert isinstance(health_check, NodeHealthCheck)\n        assert health_check.node_id == sample_compute_node.id\n        assert health_check.status == sample_compute_node.status\n        assert isinstance(health_check.metrics, dict)\n        assert \"cpu_load\" in health_check.metrics\n        assert \"memory_usage\" in health_check.metrics\n        assert \"disk_usage\" in health_check.metrics\n>       assert health_check.is_healthy()\nE       assert False\nE        +  where False = is_healthy()\nE        +    where is_healthy = <concurrent_task_scheduler.failure_resilience.failure_detector.NodeHealthCheck object at 0x7f489dc30640>.is_healthy\n\ntests/scientific_computing/failure_resilience/test_failure_detector.py:146: AssertionError"}, "teardown": {"duration": 0.00019260309636592865, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_check_simulation_health", "lineno": 163, "outcome": "passed", "keywords": ["test_check_simulation_health", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0003432859666645527, "outcome": "passed"}, "call": {"duration": 0.00017016800120472908, "outcome": "passed"}, "teardown": {"duration": 0.00014565233141183853, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_update_simulation_progress", "lineno": 185, "outcome": "passed", "keywords": ["test_update_simulation_progress", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00017666397616267204, "outcome": "passed"}, "call": {"duration": 0.000141276977956295, "outcome": "passed"}, "teardown": {"duration": 0.00012116041034460068, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_update_simulation_status", "lineno": 197, "outcome": "passed", "keywords": ["test_update_simulation_status", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00016751699149608612, "outcome": "passed"}, "call": {"duration": 0.00012664636597037315, "outcome": "passed"}, "teardown": {"duration": 0.00012374809011816978, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_detect_node_failures", "lineno": 209, "outcome": "passed", "keywords": ["test_detect_node_failures", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00024878792464733124, "outcome": "passed"}, "call": {"duration": 0.00024792877957224846, "outcome": "passed"}, "teardown": {"duration": 0.00014741206541657448, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_detect_simulation_failures", "lineno": 224, "outcome": "passed", "keywords": ["test_detect_simulation_failures", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0003337427042424679, "outcome": "passed"}, "call": {"duration": 0.00019187480211257935, "outcome": "passed"}, "teardown": {"duration": 0.00013675400987267494, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_report_failure", "lineno": 246, "outcome": "passed", "keywords": ["test_report_failure", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00016450928524136543, "outcome": "passed"}, "call": {"duration": 0.00017732009291648865, "outcome": "passed"}, "teardown": {"duration": 0.00012116320431232452, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_get_active_failures", "lineno": 283, "outcome": "passed", "keywords": ["test_get_active_failures", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0001704641617834568, "outcome": "passed"}, "call": {"duration": 0.00019437121227383614, "outcome": "passed"}, "teardown": {"duration": 0.0001271129585802555, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_resolve_failure", "lineno": 329, "outcome": "passed", "keywords": ["test_resolve_failure", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00016926182433962822, "outcome": "passed"}, "call": {"duration": 0.00018381699919700623, "outcome": "passed"}, "teardown": {"duration": 0.00012668361887335777, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_reliability_metrics", "lineno": 358, "outcome": "failed", "keywords": ["test_reliability_metrics", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00027615297585725784, "outcome": "passed"}, "call": {"duration": 0.000238015316426754, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/failure_resilience/test_failure_detector.py", "lineno": 368, "message": "assert False\n +  where False = is_healthy()\n +    where is_healthy = <concurrent_task_scheduler.failure_resilience.failure_detector.NodeHealthCheck object at 0x7f489dd73310>.is_healthy"}, "traceback": [{"path": "tests/scientific_computing/failure_resilience/test_failure_detector.py", "lineno": 368, "message": "AssertionError"}], "longrepr": "failure_detector = <concurrent_task_scheduler.failure_resilience.failure_detector.FailureDetector object at 0x7f489dd71f30>\nsample_compute_node = ComputeNode(id='node_001', name='Compute Node 1', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0,...igned_simulations=[], last_failure_time=None, maintenance_window=None, location='data_center_1', reliability_score=1.0)\n\n    def test_reliability_metrics(failure_detector, sample_compute_node):\n        \"\"\"Test reliability metrics calculation.\"\"\"\n        # Record some health checks\n        node_id = sample_compute_node.id\n    \n        # Record healthy checks\n        for _ in range(5):\n            failure_detector.record_heartbeat(node_id)\n            health_check = failure_detector.check_node_health(sample_compute_node)\n>           assert health_check.is_healthy()\nE           assert False\nE            +  where False = is_healthy()\nE            +    where is_healthy = <concurrent_task_scheduler.failure_resilience.failure_detector.NodeHealthCheck object at 0x7f489dd73310>.is_healthy\n\ntests/scientific_computing/failure_resilience/test_failure_detector.py:368: AssertionError"}, "teardown": {"duration": 0.0002209818921983242, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_failure_type_determination", "lineno": 386, "outcome": "passed", "keywords": ["test_failure_type_determination", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00020494312047958374, "outcome": "passed"}, "call": {"duration": 0.0001516439951956272, "outcome": "passed"}, "teardown": {"duration": 0.00012663193047046661, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_recovery_manager_init", "lineno": 399, "outcome": "passed", "keywords": ["test_recovery_manager_init", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00025233207270503044, "outcome": "passed"}, "call": {"duration": 0.00013566017150878906, "outcome": "passed"}, "teardown": {"duration": 0.00014532776549458504, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_get_recovery_strategy", "lineno": 408, "outcome": "passed", "keywords": ["test_get_recovery_strategy", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00023363903164863586, "outcome": "passed"}, "call": {"duration": 0.00014467304572463036, "outcome": "passed"}, "teardown": {"duration": 0.000142672099173069, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_initiate_recovery", "lineno": 445, "outcome": "passed", "keywords": ["test_initiate_recovery", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00021946802735328674, "outcome": "passed"}, "call": {"duration": 0.00023991521447896957, "outcome": "passed"}, "teardown": {"duration": 0.0001833760179579258, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_complete_recovery", "lineno": 491, "outcome": "passed", "keywords": ["test_complete_recovery", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002490077167749405, "outcome": "passed"}, "call": {"duration": 0.00021140091121196747, "outcome": "passed"}, "teardown": {"duration": 0.00014844536781311035, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_get_active_recoveries", "lineno": 528, "outcome": "passed", "keywords": ["test_get_active_recoveries", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00023555336520075798, "outcome": "passed"}, "call": {"duration": 0.0002194410189986229, "outcome": "passed"}, "teardown": {"duration": 0.00015345122665166855, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_cancel_recovery", "lineno": 567, "outcome": "passed", "keywords": ["test_cancel_recovery", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00023895222693681717, "outcome": "passed"}, "call": {"duration": 0.00019202334806323051, "outcome": "passed"}, "teardown": {"duration": 0.00016235513612627983, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_detector.py::test_process_failures", "lineno": 602, "outcome": "passed", "keywords": ["test_process_failures", "test_failure_detector.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00023390492424368858, "outcome": "passed"}, "call": {"duration": 0.0003450200892984867, "outcome": "passed"}, "teardown": {"duration": 0.00016789603978395462, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_record_heartbeat", "lineno": 108, "outcome": "passed", "keywords": ["test_record_heartbeat", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00019848207011818886, "outcome": "passed"}, "call": {"duration": 0.00013547763228416443, "outcome": "passed"}, "teardown": {"duration": 0.00013054674491286278, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_check_node_health", "lineno": 115, "outcome": "failed", "keywords": ["test_check_node_health", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002558729611337185, "outcome": "passed"}, "call": {"duration": 0.00034703686833381653, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/failure_resilience/test_failure_resilience.py", "lineno": 125, "message": "AssertionError: assert <NodeStatus.ONLINE: 'online'> == <NodeStatus.ONLINE: 'online'>\n +  where <NodeStatus.ONLINE: 'online'> = <concurrent_task_scheduler.failure_resilience.failure_detector.NodeHealthCheck object at 0x7f489e499090>.status\n +  and   <NodeStatus.ONLINE: 'online'> = NodeStatus.ONLINE"}, "traceback": [{"path": "tests/scientific_computing/failure_resilience/test_failure_resilience.py", "lineno": 125, "message": "AssertionError"}], "longrepr": "self = <tests.scientific_computing.failure_resilience.test_failure_resilience.TestFailureDetector object at 0x7f49517d7520>\nfailure_detector = <concurrent_task_scheduler.failure_resilience.failure_detector.FailureDetector object at 0x7f489e49af20>\nmock_node = ComputeNode(id='node-123', name='compute-123', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, gp...igned_simulations=[], last_failure_time=None, maintenance_window=None, location='data_center_1', reliability_score=1.0)\n\n    def test_check_node_health(self, failure_detector, mock_node):\n        \"\"\"Test checking node health.\"\"\"\n        # Record a heartbeat first\n        failure_detector.record_heartbeat(mock_node.id)\n    \n        # Check health\n        health_check = failure_detector.check_node_health(mock_node)\n    \n        assert health_check.node_id == mock_node.id\n>       assert health_check.status == NodeStatus.ONLINE\nE       AssertionError: assert <NodeStatus.ONLINE: 'online'> == <NodeStatus.ONLINE: 'online'>\nE        +  where <NodeStatus.ONLINE: 'online'> = <concurrent_task_scheduler.failure_resilience.failure_detector.NodeHealthCheck object at 0x7f489e499090>.status\nE        +  and   <NodeStatus.ONLINE: 'online'> = NodeStatus.ONLINE\n\ntests/scientific_computing/failure_resilience/test_failure_resilience.py:125: AssertionError"}, "teardown": {"duration": 0.00019257329404354095, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_check_simulation_health", "lineno": 180, "outcome": "error", "keywords": ["test_check_simulation_health", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00034028012305498123, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"SimulationStage\" object has no field \"start_time\""}, "traceback": [{"path": "tests/scientific_computing/failure_resilience/test_failure_resilience.py", "lineno": 105, "message": ""}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.scientific_computing.failure_resilience.test_failure_resilience.TestFailureDetector object at 0x7f49517d76d0>\n\n    @pytest.fixture\n    def mock_simulation(self):\n        \"\"\"Create a mock simulation for testing.\"\"\"\n        sim = MockSimulation(\n            id=\"sim-123\",\n            name=\"Test Simulation\",\n            description=\"A test simulation\",\n            status=SimulationStatus.RUNNING,\n            priority=SimulationPriority.HIGH,\n            stages={}\n        )\n    \n        # Add a stage\n        stage = SimulationStage(\n            id=\"stage-1\",\n            name=\"Processing Stage\",\n            status=SimulationStageStatus.RUNNING,\n            progress=0.5,\n            estimated_duration=timedelta(hours=2),\n        )\n        sim.stages = {\"stage-1\": stage}\n    \n        # Mock the start time\n>       stage.start_time = datetime.now() - timedelta(minutes=30)  # Started 30 minutes ago\n\ntests/scientific_computing/failure_resilience/test_failure_resilience.py:105: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = SimulationStage(id='stage-1', name='Processing Stage', status=<TaskStatus.RUNNING: 'running'>, priority=<Priority.MEDI...kpoint_frequency=datetime.timedelta(seconds=3600), last_checkpoint_time=None, checkpoint_path=None, error_message=None)\nname = 'start_time', value = datetime.datetime(2025, 6, 16, 3, 46, 45, 861857)\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"SimulationStage\" object has no field \"start_time\"\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.00019112834706902504, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_update_simulation_progress", "lineno": 209, "outcome": "passed", "keywords": ["test_update_simulation_progress", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00020341575145721436, "outcome": "passed"}, "call": {"duration": 0.00014374172315001488, "outcome": "passed"}, "teardown": {"duration": 0.00012657186016440392, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_detect_node_failures", "lineno": 221, "outcome": "failed", "keywords": ["test_detect_node_failures", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00025567691773176193, "outcome": "passed"}, "call": {"duration": 0.00031475163996219635, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/failure_resilience/test_failure_resilience.py", "lineno": 251, "message": "assert 0 == 1\n +  where 0 = len([])"}, "traceback": [{"path": "tests/scientific_computing/failure_resilience/test_failure_resilience.py", "lineno": 251, "message": "AssertionError"}], "longrepr": "self = <tests.scientific_computing.failure_resilience.test_failure_resilience.TestFailureDetector object at 0x7f49517b6bc0>\nfailure_detector = <concurrent_task_scheduler.failure_resilience.failure_detector.FailureDetector object at 0x7f489dfa2f20>\nmock_node = ComputeNode(id='node-123', name='compute-123', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, gp...igned_simulations=[], last_failure_time=None, maintenance_window=None, location='data_center_1', reliability_score=1.0)\n\n    def test_detect_node_failures(self, failure_detector, mock_node):\n        \"\"\"Test detecting node failures.\"\"\"\n        # Create mock nodes\n        nodes = {\n            \"node-123\": mock_node,\n            \"problem-node\": ComputeNode(\n                id=\"problem-node\",\n                name=\"problem\",\n                status=NodeStatus.ONLINE,\n                # Set up with critical disk usage to trigger failure\n                current_load={\"cpu\": 0.5, \"memory\": 0.5, \"storage\": 0.99},\n                cpu_cores=32,\n                memory_gb=128,\n                storage_gb=1000,\n                node_type=NodeType.COMPUTE,\n                gpu_count=0,\n                network_bandwidth_gbps=10,\n                location=\"data_center_1\"\n            ),\n        }\n    \n        # Record heartbeats\n        for node_id in nodes:\n            failure_detector.record_heartbeat(node_id)\n    \n        # Detect failures\n        failures = failure_detector.detect_node_failures(nodes)\n    \n        # Should detect one failure (high disk usage)\n>       assert len(failures) == 1\nE       assert 0 == 1\nE        +  where 0 = len([])\n\ntests/scientific_computing/failure_resilience/test_failure_resilience.py:251: AssertionError"}, "teardown": {"duration": 0.00019016116857528687, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_detect_simulation_failures", "lineno": 255, "outcome": "error", "keywords": ["test_detect_simulation_failures", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00032049696892499924, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"SimulationStage\" object has no field \"start_time\""}, "traceback": [{"path": "tests/scientific_computing/failure_resilience/test_failure_resilience.py", "lineno": 105, "message": ""}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.scientific_computing.failure_resilience.test_failure_resilience.TestFailureDetector object at 0x7f49517d7760>\n\n    @pytest.fixture\n    def mock_simulation(self):\n        \"\"\"Create a mock simulation for testing.\"\"\"\n        sim = MockSimulation(\n            id=\"sim-123\",\n            name=\"Test Simulation\",\n            description=\"A test simulation\",\n            status=SimulationStatus.RUNNING,\n            priority=SimulationPriority.HIGH,\n            stages={}\n        )\n    \n        # Add a stage\n        stage = SimulationStage(\n            id=\"stage-1\",\n            name=\"Processing Stage\",\n            status=SimulationStageStatus.RUNNING,\n            progress=0.5,\n            estimated_duration=timedelta(hours=2),\n        )\n        sim.stages = {\"stage-1\": stage}\n    \n        # Mock the start time\n>       stage.start_time = datetime.now() - timedelta(minutes=30)  # Started 30 minutes ago\n\ntests/scientific_computing/failure_resilience/test_failure_resilience.py:105: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = SimulationStage(id='stage-1', name='Processing Stage', status=<TaskStatus.RUNNING: 'running'>, priority=<Priority.MEDI...kpoint_frequency=datetime.timedelta(seconds=3600), last_checkpoint_time=None, checkpoint_path=None, error_message=None)\nname = 'start_time', value = datetime.datetime(2025, 6, 16, 3, 46, 45, 909794)\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"SimulationStage\" object has no field \"start_time\"\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.0002305838279426098, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_report_failure", "lineno": 299, "outcome": "passed", "keywords": ["test_report_failure", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00022163894027471542, "outcome": "passed"}, "call": {"duration": 0.0001838821917772293, "outcome": "passed"}, "teardown": {"duration": 0.00012904498726129532, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_get_active_failures", "lineno": 319, "outcome": "passed", "keywords": ["test_get_active_failures", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00018669525161385536, "outcome": "passed"}, "call": {"duration": 0.00019387854263186455, "outcome": "passed"}, "teardown": {"duration": 0.00012569734826683998, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_resolve_failure", "lineno": 361, "outcome": "passed", "keywords": ["test_resolve_failure", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0001710299402475357, "outcome": "passed"}, "call": {"duration": 0.00018843309953808784, "outcome": "passed"}, "teardown": {"duration": 0.00012693507596850395, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_get_node_reliability_score", "lineno": 387, "outcome": "passed", "keywords": ["test_get_node_reliability_score", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00026159314438700676, "outcome": "passed"}, "call": {"duration": 0.0001491168513894081, "outcome": "passed"}, "teardown": {"duration": 0.00013627717271447182, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_get_recovery_strategy", "lineno": 421, "outcome": "passed", "keywords": ["test_get_recovery_strategy", "TestFailureRecoveryManager", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00023521482944488525, "outcome": "passed"}, "call": {"duration": 0.0001656687818467617, "outcome": "passed"}, "teardown": {"duration": 0.0001476607285439968, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_initiate_recovery", "lineno": 451, "outcome": "passed", "keywords": ["test_initiate_recovery", "TestFailureRecoveryManager", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00023543927818536758, "outcome": "passed"}, "call": {"duration": 0.00020788470283150673, "outcome": "passed"}, "teardown": {"duration": 0.0001477329060435295, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_complete_recovery", "lineno": 483, "outcome": "passed", "keywords": ["test_complete_recovery", "TestFailureRecoveryManager", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002249041572213173, "outcome": "passed"}, "call": {"duration": 0.00021169381216168404, "outcome": "passed"}, "teardown": {"duration": 0.00014996295794844627, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_cancel_recovery", "lineno": 528, "outcome": "passed", "keywords": ["test_cancel_recovery", "TestFailureRecoveryManager", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00023274123668670654, "outcome": "passed"}, "call": {"duration": 0.00019517727196216583, "outcome": "passed"}, "teardown": {"duration": 0.00014815013855695724, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_process_failures", "lineno": 558, "outcome": "passed", "keywords": ["test_process_failures", "TestFailureRecoveryManager", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00022424012422561646, "outcome": "passed"}, "call": {"duration": 0.00024076923727989197, "outcome": "passed"}, "teardown": {"duration": 0.0001529911532998085, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_set_resilience_level", "lineno": 681, "outcome": "passed", "keywords": ["test_set_resilience_level", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0005536507815122604, "outcome": "passed"}, "call": {"duration": 0.00014898786321282387, "outcome": "passed"}, "teardown": {"duration": 0.0002834140323102474, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_record_event", "lineno": 699, "outcome": "passed", "keywords": ["test_record_event", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00049594696611166, "outcome": "passed"}, "call": {"duration": 0.0001828162930905819, "outcome": "passed"}, "teardown": {"duration": 0.00026346882805228233, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_schedule_checkpoint", "lineno": 721, "outcome": "passed", "keywords": ["test_schedule_checkpoint", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0005638720467686653, "outcome": "passed"}, "call": {"duration": 0.00015146424993872643, "outcome": "passed"}, "teardown": {"duration": 0.00026404717937111855, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_handle_node_status_change", "lineno": 741, "outcome": "passed", "keywords": ["test_handle_node_status_change", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0005432358011603355, "outcome": "passed"}, "call": {"duration": 0.00027066096663475037, "outcome": "passed"}, "teardown": {"duration": 0.00027521513402462006, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_handle_simulation_status_change", "lineno": 772, "outcome": "passed", "keywords": ["test_handle_simulation_status_change", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0005647051148116589, "outcome": "passed"}, "call": {"duration": 0.00025009317323565483, "outcome": "passed"}, "teardown": {"duration": 0.0002869628369808197, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_handle_stage_status_change", "lineno": 811, "outcome": "passed", "keywords": ["test_handle_stage_status_change", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.000539806205779314, "outcome": "passed"}, "call": {"duration": 0.00023903511464595795, "outcome": "passed"}, "teardown": {"duration": 0.00027093105018138885, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_handle_failure_detection", "lineno": 843, "outcome": "passed", "keywords": ["test_handle_failure_detection", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00048099085688591003, "outcome": "passed"}, "call": {"duration": 0.00023429980501532555, "outcome": "passed"}, "teardown": {"duration": 0.0002731350250542164, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_complete_recovery", "lineno": 873, "outcome": "passed", "keywords": ["test_complete_recovery", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00045488588511943817, "outcome": "passed"}, "call": {"duration": 0.00021172594279050827, "outcome": "passed"}, "teardown": {"duration": 0.0002559400163590908, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_detect_and_handle_failures", "lineno": 910, "outcome": "xfailed", "keywords": ["test_detect_and_handle_failures", "xfail", "pytestmark", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0006264485418796539, "outcome": "passed"}, "call": {"duration": 0.0008858391083776951, "outcome": "skipped", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/failure_resilience/test_failure_resilience.py", "lineno": 968, "message": "assert 0 > 0\n +  where 0 = len([])"}, "traceback": [{"path": "tests/scientific_computing/failure_resilience/test_failure_resilience.py", "lineno": 968, "message": "AssertionError"}], "longrepr": "self = <tests.scientific_computing.failure_resilience.test_failure_resilience.TestResilienceCoordinator object at 0x7f4951810640>\nresilience_coordinator = <concurrent_task_scheduler.failure_resilience.resilience_coordinator.ResilienceCoordinator object at 0x7f489e198b50>\nmock_node = ComputeNode(id='node-123', name='compute-123', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, gp...igned_simulations=[], last_failure_time=None, maintenance_window=None, location='data_center_1', reliability_score=1.0)\nmock_simulation = MockSimulation(id='sim-123', name='Test Simulation', description='A test simulation', stages={'stage-1': SimulationSta...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\n\n    @pytest.mark.xfail(reason=\"Test is inconsistent due to implementation details\")\n    def test_detect_and_handle_failures(self, resilience_coordinator, mock_node, mock_simulation):\n        \"\"\"Test end-to-end failure detection and handling.\"\"\"\n        # Create problematic node and simulation\n        problem_node = ComputeNode(\n            id=\"problem-node\",\n            name=\"problem\",\n            status=NodeStatus.ONLINE,\n            current_load={\"cpu\": 0.99, \"memory\": 0.5, \"storage\": 0.5},  # High CPU load\n            cpu_cores=32,\n            memory_gb=128,\n            storage_gb=1000,\n            node_type=NodeType.COMPUTE,\n            gpu_count=0,\n            network_bandwidth_gbps=10,\n            location=\"data_center_1\"\n        )\n    \n        nodes = {\n            \"node-123\": mock_node,\n            \"problem-node\": problem_node,\n        }\n    \n        # Record heartbeats\n        for node_id in nodes:\n            resilience_coordinator.failure_detector.record_heartbeat(node_id)\n    \n        # Create problematic simulation\n        stalled_sim = StalledSimulation(\n            id=\"stalled-sim\",\n            name=\"Stalled Simulation\",\n            status=SimulationStatus.RUNNING,\n            stages={}\n        )\n    \n        # Set up simulation to appear stalled\n        resilience_coordinator.failure_detector.simulation_health_history[\"stalled-sim\"] = {\n            \"last_progress_update\": datetime.now() - timedelta(minutes=120),  # No progress for 2 hours\n        }\n    \n        simulations = {\n            \"sim-123\": mock_simulation,\n            \"stalled-sim\": stalled_sim,\n        }\n    \n        # Set low threshold for testing\n        resilience_coordinator.failure_detector.thresholds[\"simulation_progress_stall_minutes\"] = 60\n    \n        # Detect and handle failures\n        with patch.object(resilience_coordinator.recovery_manager, 'initiate_recovery', return_value=Result.ok(RecoveryStrategy.RESTART)):\n            handled_failures = resilience_coordinator.detect_and_handle_failures(nodes, simulations)\n    \n        # Should detect and handle at least one failure\n        assert len(handled_failures) > 0\n    \n        # The CPU load failure and stalled simulation should be detected\n        node_failures = resilience_coordinator.failure_detector.get_active_failures(node_id=\"problem-node\")\n>       assert len(node_failures) > 0\nE       assert 0 > 0\nE        +  where 0 = len([])\n\ntests/scientific_computing/failure_resilience/test_failure_resilience.py:968: AssertionError"}, "teardown": {"duration": 0.00035246042534708977, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_resilience_coordinator_init", "lineno": 152, "outcome": "passed", "keywords": ["test_resilience_coordinator_init", "test_resilience_coordinator.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0005106143653392792, "outcome": "passed"}, "call": {"duration": 0.00015218695625662804, "outcome": "passed"}, "teardown": {"duration": 0.0002790386788547039, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_set_resilience_level", "lineno": 163, "outcome": "passed", "keywords": ["test_set_resilience_level", "test_resilience_coordinator.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00047910306602716446, "outcome": "passed"}, "call": {"duration": 0.00014690915122628212, "outcome": "passed"}, "teardown": {"duration": 0.0002652658149600029, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_record_event", "lineno": 181, "outcome": "passed", "keywords": ["test_record_event", "test_resilience_coordinator.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0004537990316748619, "outcome": "passed"}, "call": {"duration": 0.00016799289733171463, "outcome": "passed"}, "teardown": {"duration": 0.0003168368712067604, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_schedule_checkpoint", "lineno": 203, "outcome": "passed", "keywords": ["test_schedule_checkpoint", "test_resilience_coordinator.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0005817296914756298, "outcome": "passed"}, "call": {"duration": 0.00015633320435881615, "outcome": "passed"}, "teardown": {"duration": 0.00027777766808867455, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_process_scheduled_checkpoints", "lineno": 219, "outcome": "passed", "keywords": ["test_process_scheduled_checkpoints", "test_resilience_coordinator.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0005817660130560398, "outcome": "passed"}, "call": {"duration": 0.005013552960008383, "outcome": "passed"}, "teardown": {"duration": 0.0005776970647275448, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_handle_node_status_change", "lineno": 238, "outcome": "passed", "keywords": ["test_handle_node_status_change", "test_resilience_coordinator.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0006638560444116592, "outcome": "passed"}, "call": {"duration": 0.0002537625841796398, "outcome": "passed"}, "teardown": {"duration": 0.0003027082420885563, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_handle_simulation_status_change", "lineno": 290, "outcome": "passed", "keywords": ["test_handle_simulation_status_change", "test_resilience_coordinator.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0005727401003241539, "outcome": "passed"}, "call": {"duration": 0.0002513313665986061, "outcome": "passed"}, "teardown": {"duration": 0.00027577485889196396, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_handle_stage_status_change", "lineno": 344, "outcome": "passed", "keywords": ["test_handle_stage_status_change", "test_resilience_coordinator.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0005866470746695995, "outcome": "passed"}, "call": {"duration": 0.0002462300471961498, "outcome": "passed"}, "teardown": {"duration": 0.0002907407470047474, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_handle_failure_detection", "lineno": 397, "outcome": "passed", "keywords": ["test_handle_failure_detection", "test_resilience_coordinator.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0005638850852847099, "outcome": "passed"}, "call": {"duration": 0.0002267458476126194, "outcome": "passed"}, "teardown": {"duration": 0.00027928268536925316, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_complete_recovery", "lineno": 429, "outcome": "passed", "keywords": ["test_complete_recovery", "test_resilience_coordinator.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0005644909106194973, "outcome": "passed"}, "call": {"duration": 0.00030868174508213997, "outcome": "passed"}, "teardown": {"duration": 0.0002903910353779793, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_get_active_recoveries", "lineno": 486, "outcome": "passed", "keywords": ["test_get_active_recoveries", "test_resilience_coordinator.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00045794807374477386, "outcome": "passed"}, "call": {"duration": 0.00023130513727664948, "outcome": "passed"}, "teardown": {"duration": 0.00026914896443486214, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_get_checkpoint_schedule", "lineno": 521, "outcome": "passed", "keywords": ["test_get_checkpoint_schedule", "test_resilience_coordinator.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0005680308677256107, "outcome": "passed"}, "call": {"duration": 0.00015879515558481216, "outcome": "passed"}, "teardown": {"duration": 0.0002733059227466583, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_get_resilience_metrics", "lineno": 536, "outcome": "passed", "keywords": ["test_get_resilience_metrics", "test_resilience_coordinator.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0005035228095948696, "outcome": "passed"}, "call": {"duration": 0.00015581399202346802, "outcome": "passed"}, "teardown": {"duration": 0.0002654758282005787, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_detect_and_handle_failures", "lineno": 554, "outcome": "xpassed", "keywords": ["test_detect_and_handle_failures", "xfail", "pytestmark", "test_resilience_coordinator.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0006390083581209183, "outcome": "passed"}, "call": {"duration": 0.0002640970051288605, "outcome": "passed"}, "teardown": {"duration": 0.00035197706893086433, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py::test_process_checkpoints", "lineno": 577, "outcome": "xfailed", "keywords": ["test_process_checkpoints", "xfail", "pytestmark", "test_resilience_coordinator.py", "failure_resilience", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0006229369901120663, "outcome": "passed"}, "call": {"duration": 0.005054058041423559, "outcome": "skipped", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/failure_resilience/test_resilience_coordinator.py", "lineno": 591, "message": "AssertionError: assert 'sim_climate_test' in {}\n +  where 'sim_climate_test' = MockSimulation(id='sim_climate_test', name='Climate Model Test', description=None, stages={'stage_preprocess': Simulat...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1)).id\n +  and   {} = <concurrent_task_scheduler.failure_resilience.checkpoint_manager.CheckpointCoordinator object at 0x7f489e27d5a0>.scheduled_checkpoints\n +    where <concurrent_task_scheduler.failure_resilience.checkpoint_manager.CheckpointCoordinator object at 0x7f489e27d5a0> = <concurrent_task_scheduler.failure_resilience.resilience_coordinator.ResilienceCoordinator object at 0x7f489e27dcf0>.checkpoint_coordinator"}, "traceback": [{"path": "tests/scientific_computing/failure_resilience/test_resilience_coordinator.py", "lineno": 591, "message": "AssertionError"}], "longrepr": "resilience_coordinator = <concurrent_task_scheduler.failure_resilience.resilience_coordinator.ResilienceCoordinator object at 0x7f489e27dcf0>\nsample_simulation = MockSimulation(id='sim_climate_test', name='Climate Model Test', description=None, stages={'stage_preprocess': Simulat...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\n\n    @pytest.mark.xfail(reason=\"Test depends on specific timing for checkpoint scheduling\")\n    def test_process_checkpoints(resilience_coordinator, sample_simulation):\n        \"\"\"Test processing checkpoints for simulations.\"\"\"\n        # Create dictionaries for simulations\n        simulations = {sample_simulation.id: sample_simulation}\n    \n        # Ensure the simulation is schedulable\n        resilience_coordinator.checkpoint_coordinator.last_checkpoint_time[sample_simulation.id] = datetime.now() - timedelta(hours=2)\n    \n        # Process checkpoints\n        processed = resilience_coordinator.process_checkpoints(simulations)\n    \n        # Should schedule checkpoint\n>       assert sample_simulation.id in resilience_coordinator.checkpoint_coordinator.scheduled_checkpoints\nE       AssertionError: assert 'sim_climate_test' in {}\nE        +  where 'sim_climate_test' = MockSimulation(id='sim_climate_test', name='Climate Model Test', description=None, stages={'stage_preprocess': Simulat...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1)).id\nE        +  and   {} = <concurrent_task_scheduler.failure_resilience.checkpoint_manager.CheckpointCoordinator object at 0x7f489e27d5a0>.scheduled_checkpoints\nE        +    where <concurrent_task_scheduler.failure_resilience.checkpoint_manager.CheckpointCoordinator object at 0x7f489e27d5a0> = <concurrent_task_scheduler.failure_resilience.resilience_coordinator.ResilienceCoordinator object at 0x7f489e27dcf0>.checkpoint_coordinator\n\ntests/scientific_computing/failure_resilience/test_resilience_coordinator.py:591: AssertionError"}, "teardown": {"duration": 0.0005678748711943626, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_submit_simulation", "lineno": 175, "outcome": "failed", "keywords": ["test_submit_simulation", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0006652022711932659, "outcome": "passed"}, "call": {"duration": 0.00019431579858064651, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"SimulationStage\" object has no field \"start_time\""}, "traceback": [{"path": "tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 182, "message": ""}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951880df0>\n\n    def test_submit_simulation(self):\n        \"\"\"Test submitting a simulation to the job manager.\"\"\"\n        # Reset the simulation to DEFINED status\n        self.simulation.status = SimulationStatus.DEFINED\n        for stage in self.simulation.stages.values():\n            stage.status = SimulationStageStatus.PENDING\n>           stage.start_time = None\n\ntests/scientific_computing/job_management/test_long_running_job_manager.py:182: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = SimulationStage(id='stage-1', name='Preprocessing', status=<SimulationStageStatus.PENDING: 'pending'>, priority=<Prior...kpoint_frequency=datetime.timedelta(seconds=3600), last_checkpoint_time=None, checkpoint_path=None, error_message=None)\nname = 'start_time', value = None\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"SimulationStage\" object has no field \"start_time\"\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.00018803821876645088, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_submit_with_max_concurrent_limit", "lineno": 191, "outcome": "passed", "keywords": ["test_submit_with_max_concurrent_limit", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00046144798398017883, "outcome": "passed"}, "call": {"duration": 0.0006423159502446651, "outcome": "passed", "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.1037157, "msecs": 103.0, "relativeCreated": 3700.1657485961914, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.1038973, "msecs": 103.0, "relativeCreated": 3700.347423553467, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}]}, "teardown": {"duration": 0.00014732219278812408, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_processing_queue", "lineno": 215, "outcome": "failed", "keywords": ["test_processing_queue", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0003859829157590866, "outcome": "passed"}, "call": {"duration": 0.0008465368300676346, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 226, "message": "AssertionError: assert 'sim-test-1' not in {'sim-test-1': Simulation(id='sim-test-1', name='Climate Model Test', description='Test simulation of climate model', ...ulation'], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=6))}\n +  where 'sim-test-1' = Simulation(id='sim-test-1', name='Climate Model Test', description='Test simulation of climate model', stages={'stage-...mulation'], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=6)).id\n +    where Simulation(id='sim-test-1', name='Climate Model Test', description='Test simulation of climate model', stages={'stage-...mulation'], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=6)) = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881150>.simulation\n +  and   {'sim-test-1': Simulation(id='sim-test-1', name='Climate Model Test', description='Test simulation of climate model', ...ulation'], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=6))} = <concurrent_task_scheduler.job_management.scheduler.LongRunningJobManager object at 0x7f489e215a80>.queued_simulations\n +    where <concurrent_task_scheduler.job_management.scheduler.LongRunningJobManager object at 0x7f489e215a80> = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881150>.job_manager"}, "traceback": [{"path": "tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 226, "message": "AssertionError"}], "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.1052783, "msecs": 105.0, "relativeCreated": 3701.728343963623, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.1053598, "msecs": 105.0, "relativeCreated": 3701.809883117676, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}], "longrepr": "self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881150>\n\n    def test_processing_queue(self):\n        \"\"\"Test processing the queue of simulations.\"\"\"\n        # Submit a simulation\n        result = self.job_manager.submit_simulation(self.simulation)\n        assert result.success\n    \n        # Force process queue (normally called internally)\n        self.job_manager._process_queue()\n    \n        # Verify simulation moved from queued to running\n>       assert self.simulation.id not in self.job_manager.queued_simulations\nE       AssertionError: assert 'sim-test-1' not in {'sim-test-1': Simulation(id='sim-test-1', name='Climate Model Test', description='Test simulation of climate model', ...ulation'], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=6))}\nE        +  where 'sim-test-1' = Simulation(id='sim-test-1', name='Climate Model Test', description='Test simulation of climate model', stages={'stage-...mulation'], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=6)).id\nE        +    where Simulation(id='sim-test-1', name='Climate Model Test', description='Test simulation of climate model', stages={'stage-...mulation'], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=6)) = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881150>.simulation\nE        +  and   {'sim-test-1': Simulation(id='sim-test-1', name='Climate Model Test', description='Test simulation of climate model', ...ulation'], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=6))} = <concurrent_task_scheduler.job_management.scheduler.LongRunningJobManager object at 0x7f489e215a80>.queued_simulations\nE        +    where <concurrent_task_scheduler.job_management.scheduler.LongRunningJobManager object at 0x7f489e215a80> = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881150>.job_manager\n\ntests/scientific_computing/job_management/test_long_running_job_manager.py:226: AssertionError"}, "teardown": {"duration": 0.00017970707267522812, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_pause_and_resume_simulation", "lineno": 235, "outcome": "failed", "keywords": ["test_pause_and_resume_simulation", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0004067476838827133, "outcome": "passed"}, "call": {"duration": 0.00036526797339320183, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 244, "message": "AssertionError: assert False\n +  where False = Result(success=False, value=None, error='Simulation sim-test-1 not found or not running').success"}, "traceback": [{"path": "tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 244, "message": "AssertionError"}], "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.1154165, "msecs": 115.0, "relativeCreated": 3711.866617202759, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.1154962, "msecs": 115.0, "relativeCreated": 3711.9462490081787, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}], "longrepr": "self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951880ac0>\n\n    def test_pause_and_resume_simulation(self):\n        \"\"\"Test pausing and resuming a simulation.\"\"\"\n        # Submit and process\n        self.job_manager.submit_simulation(self.simulation)\n        self.job_manager._process_queue()\n    \n        # Pause the simulation\n        result = self.job_manager.pause_simulation(self.simulation.id)\n>       assert result.success\nE       AssertionError: assert False\nE        +  where False = Result(success=False, value=None, error='Simulation sim-test-1 not found or not running').success\n\ntests/scientific_computing/job_management/test_long_running_job_manager.py:244: AssertionError"}, "teardown": {"duration": 0.000179193913936615, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_simulation_stage_transitions", "lineno": 256, "outcome": "failed", "keywords": ["test_simulation_stage_transitions", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0004174457862973213, "outcome": "passed"}, "call": {"duration": 0.0004615671932697296, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 264, "message": "AssertionError: assert <TaskStatus.PENDING: 'pending'> == <SimulationStageStatus.RUNNING: 'running'>\n +  where <TaskStatus.PENDING: 'pending'> = SimulationStage(id='stage-1', name='Preprocessing', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.MEDIUM:...kpoint_frequency=datetime.timedelta(seconds=3600), last_checkpoint_time=None, checkpoint_path=None, error_message=None).status\n +  and   <SimulationStageStatus.RUNNING: 'running'> = SimulationStageStatus.RUNNING"}, "traceback": [{"path": "tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 264, "message": "AssertionError"}], "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.1247776, "msecs": 124.0, "relativeCreated": 3721.2276458740234, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.1248543, "msecs": 124.0, "relativeCreated": 3721.304416656494, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}], "longrepr": "self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951880280>\n\n    def test_simulation_stage_transitions(self):\n        \"\"\"Test simulation stage transitions based on dependencies.\"\"\"\n        # Submit and process\n        self.job_manager.submit_simulation(self.simulation)\n        self.job_manager._process_queue()\n    \n        # Initially, only the first stage should be running\n>       assert self.simulation.stages[\"stage-1\"].status == SimulationStageStatus.RUNNING\nE       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <SimulationStageStatus.RUNNING: 'running'>\nE        +  where <TaskStatus.PENDING: 'pending'> = SimulationStage(id='stage-1', name='Preprocessing', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.MEDIUM:...kpoint_frequency=datetime.timedelta(seconds=3600), last_checkpoint_time=None, checkpoint_path=None, error_message=None).status\nE        +  and   <SimulationStageStatus.RUNNING: 'running'> = SimulationStageStatus.RUNNING\n\ntests/scientific_computing/job_management/test_long_running_job_manager.py:264: AssertionError"}, "teardown": {"duration": 0.00018656719475984573, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_handle_node_failure", "lineno": 285, "outcome": "failed", "keywords": ["test_handle_node_failure", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00040419120341539383, "outcome": "passed"}, "call": {"duration": 0.0006744535639882088, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 300, "message": "AssertionError: assert <SimulationSt...: 'scheduled'> == <SimulationSt...SED: 'paused'>\n  \n  - paused\n  + scheduled"}, "traceback": [{"path": "tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 300, "message": "AssertionError"}], "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.134298, "msecs": 134.0, "relativeCreated": 3730.748176574707, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.134377, "msecs": 134.0, "relativeCreated": 3730.8270931243896, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}], "longrepr": "self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881240>\n\n    def test_handle_node_failure(self):\n        \"\"\"Test handling of node failures.\"\"\"\n        # Submit and process\n        self.job_manager.submit_simulation(self.simulation)\n        self.job_manager._process_queue()\n    \n        # Mark a node as offline\n        affected_node = self.nodes[0]\n        affected_node.assigned_simulations = [self.simulation.id]\n    \n        # Update node status\n        self.job_manager.update_node_status(affected_node.id, NodeStatus.OFFLINE)\n    \n        # Verify simulation is paused\n>       assert self.simulation.status == SimulationStatus.PAUSED\nE       AssertionError: assert <SimulationSt...: 'scheduled'> == <SimulationSt...SED: 'paused'>\nE         \nE         - paused\nE         + scheduled\n\ntests/scientific_computing/job_management/test_long_running_job_manager.py:300: AssertionError"}, "teardown": {"duration": 0.00018117530271410942, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_checkpoint_creation", "lineno": 301, "outcome": "failed", "keywords": ["test_checkpoint_creation", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0004214337095618248, "outcome": "passed"}, "call": {"duration": 0.00072527676820755, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/unittest/mock.py", "lineno": 908, "message": "AssertionError: Expected 'create_checkpoint' to have been called once. Called 0 times."}, "traceback": [{"path": "tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 316, "message": ""}, {"path": "../../../../.pyenv/versions/3.10.11/lib/python3.10/unittest/mock.py", "lineno": 908, "message": "AssertionError"}], "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.1443977, "msecs": 144.0, "relativeCreated": 3740.8478260040283, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.144474, "msecs": 144.0, "relativeCreated": 3740.924119949341, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}], "longrepr": "self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f49518813f0>\n\n    def test_checkpoint_creation(self):\n        \"\"\"Test creation of checkpoints for simulations.\"\"\"\n        # Submit and process\n        self.job_manager.submit_simulation(self.simulation)\n        self.job_manager._process_queue()\n    \n        # Mock the checkpoint manager\n        mock_checkpoint_manager = MagicMock()\n        self.job_manager.checkpoint_manager = mock_checkpoint_manager\n    \n        # Pause the simulation (which should trigger checkpointing)\n        self.job_manager.pause_simulation(self.simulation.id)\n    \n        # Verify checkpoint manager was called\n>       mock_checkpoint_manager.create_checkpoint.assert_called_once()\n\ntests/scientific_computing/job_management/test_long_running_job_manager.py:316: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <MagicMock name='mock.create_checkpoint' id='139949867276112'>\n\n    def assert_called_once(self):\n        \"\"\"assert that the mock was called only once.\n        \"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to have been called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'create_checkpoint' to have been called once. Called 0 times.\n\n../../../../.pyenv/versions/3.10.11/lib/python3.10/unittest/mock.py:908: AssertionError"}, "teardown": {"duration": 0.00019721314311027527, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_maintenance_handling", "lineno": 317, "outcome": "failed", "keywords": ["test_maintenance_handling", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0004254779778420925, "outcome": "passed"}, "call": {"duration": 0.0008322708308696747, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 354, "message": "AssertionError: assert <SimulationSt...: 'scheduled'> == <SimulationSt...SED: 'paused'>\n  \n  - paused\n  + scheduled"}, "traceback": [{"path": "tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 354, "message": "AssertionError"}], "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.1843216, "msecs": 184.0, "relativeCreated": 3780.7717323303223, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.184406, "msecs": 184.0, "relativeCreated": 3780.856132507324, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "Reservation for simulation sim-test-1 overlaps with maintenance window maint_20250616041643_42881187492809b2", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 246, "funcName": "reserve_resources", "created": 1750047406.1845016, "msecs": 184.0, "relativeCreated": 3780.951738357544, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}], "longrepr": "self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f49518815a0>\n\n    def test_maintenance_handling(self):\n        \"\"\"Test handling of maintenance windows.\"\"\"\n        # Submit and process\n        self.job_manager.submit_simulation(self.simulation)\n        self.job_manager._process_queue()\n    \n        # Create a maintenance window using the scheduler's method\n        now = datetime.now()\n        window = self.scheduler.add_maintenance_window(\n            start_time=now + timedelta(hours=1),\n            end_time=now + timedelta(hours=3),\n            description=\"Test maintenance\",\n            affected_nodes=[node.id for node in self.nodes[:5]],  # First 5 nodes\n            severity=\"major\",\n        )\n    \n        # Explicitly assign the simulation to the affected nodes\n        for node in self.nodes[:5]:\n            if not hasattr(node, 'assigned_simulations'):\n                node.assigned_simulations = []\n            node.assigned_simulations.append(self.simulation.id)\n            # Add a reservation that conflicts with maintenance\n            result = self.scheduler.reserve_resources(\n                simulation=self.simulation,\n                start_time=now + timedelta(minutes=30),\n                duration=timedelta(hours=4)  # Overlaps with maintenance\n            )\n            if result.success:\n                reservation = result.value\n                self.scheduler.activate_reservation(reservation.id)\n            break  # Only need one node to be affected\n    \n        # Let the job manager handle the maintenance\n        self.job_manager.handle_maintenance_window(window)\n    \n        # Check if simulation was paused\n>       assert self.simulation.status == SimulationStatus.PAUSED\nE       AssertionError: assert <SimulationSt...: 'scheduled'> == <SimulationSt...SED: 'paused'>\nE         \nE         - paused\nE         + scheduled\n\ntests/scientific_computing/job_management/test_long_running_job_manager.py:354: AssertionError"}, "teardown": {"duration": 0.0001853378489613533, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_preemption_policies", "lineno": 355, "outcome": "failed", "keywords": ["test_preemption_policies", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0004245131276547909, "outcome": "passed"}, "call": {"duration": 0.0009926250204443932, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 404, "message": "AssertionError: assert <SimulationSt...: 'scheduled'> == <SimulationSt...SED: 'paused'>\n  \n  - paused\n  + scheduled"}, "traceback": [{"path": "tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 404, "message": "AssertionError"}], "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.194971, "msecs": 194.0, "relativeCreated": 3791.4211750030518, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.195054, "msecs": 195.0, "relativeCreated": 3791.504144668579, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.1950994, "msecs": 195.0, "relativeCreated": 3791.5494441986084, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.1951537, "msecs": 195.0, "relativeCreated": 3791.6038036346436, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.1951966, "msecs": 195.0, "relativeCreated": 3791.646718978882, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}], "longrepr": "self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881750>\n\n    def test_preemption_policies(self):\n        \"\"\"Test preemption policies for simulations of different priorities.\"\"\"\n        # Create simulations with different priorities\n        low_priority_sim = self._create_test_simulation()\n        low_priority_sim.id = \"sim-low\"\n        low_priority_sim.priority = SimulationPriority.LOW\n    \n        high_priority_sim = self._create_test_simulation()\n        high_priority_sim.id = \"sim-high\"\n        high_priority_sim.priority = SimulationPriority.HIGH\n    \n        critical_sim = self._create_test_simulation()\n        critical_sim.id = \"sim-critical\"\n        critical_sim.priority = SimulationPriority.CRITICAL\n    \n        # Submit all simulations\n        self.job_manager.max_concurrent_simulations = 10  # Increase the limit\n        self.job_manager.submit_simulation(low_priority_sim)\n        self.job_manager.submit_simulation(high_priority_sim)\n        self.job_manager._process_queue()\n    \n        # Force the system into a resource shortage\n        for node in self.nodes:\n            node.current_load = {\n                ResourceType.CPU: node.cpu_cores * 0.9,  # 90% CPU usage\n                ResourceType.MEMORY: node.memory_gb * 0.9,  # 90% memory usage\n            }\n    \n        # Create a special method for the test to ensure preemption happens\n        def preempt_low_priority_for_test(self):\n            # Pause all low priority simulations in the system\n            for sim_id, sim in list(self.running_simulations.items()):\n                if sim.priority == SimulationPriority.LOW:\n                    self.pause_simulation(sim_id)\n    \n        # Monkey patch the method to the job manager for this test\n        self.job_manager.preempt_low_priority_for_test = types.MethodType(preempt_low_priority_for_test, self.job_manager)\n    \n        # Try to submit a critical simulation\n        self.job_manager.submit_simulation(critical_sim)\n    \n        # Force preempt low priority simulations for test\n        self.job_manager.preempt_low_priority_for_test()\n    \n        # Now process the queue\n        self.job_manager._process_queue()\n    \n        # Verify low priority simulation was preempted\n>       assert low_priority_sim.status == SimulationStatus.PAUSED\nE       AssertionError: assert <SimulationSt...: 'scheduled'> == <SimulationSt...SED: 'paused'>\nE         \nE         - paused\nE         + scheduled\n\ntests/scientific_computing/job_management/test_long_running_job_manager.py:404: AssertionError"}, "teardown": {"duration": 0.0002354830503463745, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_long_term_reservation", "lineno": 411, "outcome": "failed", "keywords": ["test_long_term_reservation", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0004544612020254135, "outcome": "passed"}, "call": {"duration": 0.0005369847640395164, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 425, "message": "assert 0 > 0\n +  where 0 = len([])"}, "traceback": [{"path": "tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 425, "message": "AssertionError"}], "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.2573638, "msecs": 257.0, "relativeCreated": 3853.813886642456, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.257455, "msecs": 257.0, "relativeCreated": 3853.905200958252, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}], "longrepr": "self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881900>\n\n    def test_long_term_reservation(self):\n        \"\"\"Test long-term resource reservation for extended simulations.\"\"\"\n        # Create a long-running simulation\n        long_sim = self._create_test_simulation()\n        long_sim.id = \"sim-long\"\n        long_sim.estimated_total_duration = timedelta(days=30)  # 30-day simulation\n    \n        # Submit the simulation\n        self.job_manager.submit_simulation(long_sim)\n        self.job_manager._process_queue()\n    \n        # Check if a long-term reservation was created\n        reservations = self.scheduler.get_reservations_for_simulation(long_sim.id)\n>       assert len(reservations) > 0\nE       assert 0 > 0\nE        +  where 0 = len([])\n\ntests/scientific_computing/job_management/test_long_running_job_manager.py:425: AssertionError"}, "teardown": {"duration": 0.00018390407785773277, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_node_allocation_optimization", "lineno": 431, "outcome": "failed", "keywords": ["test_node_allocation_optimization", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0004350431263446808, "outcome": "passed"}, "call": {"duration": 0.0008716410957276821, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 456, "message": "AssertionError: assert <SimulationSt...: 'scheduled'> == <SimulationSt...NG: 'running'>\n  \n  - running\n  + scheduled"}, "traceback": [{"path": "tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 456, "message": "AssertionError"}], "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.267721, "msecs": 267.0, "relativeCreated": 3864.171028137207, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.267802, "msecs": 267.0, "relativeCreated": 3864.2520904541016, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.2678483, "msecs": 267.0, "relativeCreated": 3864.2983436584473, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}], "longrepr": "self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881ab0>\n\n    def test_node_allocation_optimization(self):\n        \"\"\"Test optimization of node allocation for efficiency.\"\"\"\n        # Create simulations with different resource requirements\n        sim1 = self._create_test_simulation()\n        sim1.id = \"sim-cpu-intensive\"\n        for stage in sim1.stages.values():\n            for req in stage.resource_requirements:\n                if req.resource_type == ResourceType.CPU:\n                    req.amount *= 2  # Double CPU requirements\n    \n        sim2 = self._create_test_simulation()\n        sim2.id = \"sim-gpu-intensive\"\n        for stage in sim2.stages.values():\n            for req in stage.resource_requirements:\n                if req.resource_type == ResourceType.GPU:\n                    req.amount *= 2  # Double GPU requirements\n    \n        # Submit both simulations\n        self.job_manager.max_concurrent_simulations = 10\n        self.job_manager.submit_simulation(sim1)\n        self.job_manager.submit_simulation(sim2)\n        self.job_manager._process_queue()\n    \n        # Both should be running\n>       assert sim1.status == SimulationStatus.RUNNING\nE       AssertionError: assert <SimulationSt...: 'scheduled'> == <SimulationSt...NG: 'running'>\nE         \nE         - running\nE         + scheduled\n\ntests/scientific_computing/job_management/test_long_running_job_manager.py:456: AssertionError"}, "teardown": {"duration": 0.00018090056255459785, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_dynamic_priority_adjustment", "lineno": 500, "outcome": "failed", "keywords": ["test_dynamic_priority_adjustment", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00041310302913188934, "outcome": "passed"}, "call": {"duration": 0.00042724935337901115, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 530, "message": "assert 2 > 2"}, "traceback": [{"path": "tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 530, "message": "AssertionError"}], "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.2778084, "msecs": 277.0, "relativeCreated": 3874.258518218994, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.2778857, "msecs": 277.0, "relativeCreated": 3874.335765838623, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.2779472, "msecs": 277.0, "relativeCreated": 3874.3972778320312, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}], "longrepr": "self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881c60>\n\n    def test_dynamic_priority_adjustment(self):\n        \"\"\"Test dynamic priority adjustment based on system conditions.\"\"\"\n        # Submit a simulation\n        self.job_manager.submit_simulation(self.simulation)\n        self.job_manager._process_queue()\n    \n        # Record current priority\n        original_priority = self.simulation.priority\n    \n        # Simulate system overload\n        for node in self.nodes:\n            node.current_load = {\n                ResourceType.CPU: node.cpu_cores * 0.95,  # 95% CPU usage\n                ResourceType.MEMORY: node.memory_gb * 0.95,  # 95% memory usage\n            }\n    \n        # Trigger system condition check\n        self.job_manager._adjust_priorities_based_on_system_load()\n    \n        # Verify priority was lowered (higher value means lower priority)\n        priority_order = {\n            SimulationPriority.CRITICAL: 0,\n            SimulationPriority.HIGH: 1,\n            SimulationPriority.MEDIUM: 2,\n            SimulationPriority.LOW: 3,\n            SimulationPriority.BACKGROUND: 4\n        }\n    \n        # Numerical comparison - higher number = lower priority\n>       assert priority_order[self.simulation.priority] > priority_order[original_priority]\nE       assert 2 > 2\n\ntests/scientific_computing/job_management/test_long_running_job_manager.py:530: AssertionError"}, "teardown": {"duration": 0.00018135318532586098, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_preemption_protection", "lineno": 531, "outcome": "passed", "keywords": ["test_preemption_protection", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00041749002411961555, "outcome": "passed"}, "call": {"duration": 0.0001583332195878029, "outcome": "passed"}, "teardown": {"duration": 0.00014784466475248337, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_recovery_from_failure", "lineno": 548, "outcome": "failed", "keywords": ["test_recovery_from_failure", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00038699014112353325, "outcome": "passed"}, "call": {"duration": 0.001019021961838007, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 572, "message": "AssertionError: assert False\n +  where False = Result(success=False, value=None, error='Simulation sim-test-1 not found').success"}, "traceback": [{"path": "tests/scientific_computing/job_management/test_long_running_job_manager.py", "lineno": 572, "message": "AssertionError"}], "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.2889647, "msecs": 288.0, "relativeCreated": 3885.4148387908936, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "No available nodes to process queue", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 707, "funcName": "_process_queue", "created": 1750047406.2890422, "msecs": 289.0, "relativeCreated": 3885.4923248291016, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}], "longrepr": "self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881fc0>\n\n    def test_recovery_from_failure(self):\n        \"\"\"Test recovery of a simulation after node failure.\"\"\"\n        # Submit and process\n        self.job_manager.submit_simulation(self.simulation)\n        self.job_manager._process_queue()\n    \n        # Mock the checkpoint manager\n        mock_checkpoint_manager = MagicMock()\n        self.job_manager.checkpoint_manager = mock_checkpoint_manager\n    \n        # Set up a checkpoint\n        mock_checkpoint_manager.get_latest_checkpoint.return_value = \"checkpoint-1\"\n        mock_checkpoint_manager.load_checkpoint.return_value = True\n    \n        # Mark a node as offline\n        affected_node = self.nodes[0]\n        affected_node.assigned_simulations = [self.simulation.id]\n        self.job_manager.update_node_status(affected_node.id, NodeStatus.OFFLINE)\n    \n        # Simulate recovery effort\n        result = self.job_manager.recover_simulation(self.simulation.id)\n    \n        # Verify successful recovery\n>       assert result.success\nE       AssertionError: assert False\nE        +  where False = Result(success=False, value=None, error='Simulation sim-test-1 not found').success\n\ntests/scientific_computing/job_management/test_long_running_job_manager.py:572: AssertionError"}, "teardown": {"duration": 0.0001818779855966568, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestJobScheduler::test_reserve_resources", "lineno": 582, "outcome": "passed", "keywords": ["test_reserve_resources", "TestJobScheduler", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002210652455687523, "outcome": "passed"}, "call": {"duration": 0.00028596026822924614, "outcome": "passed"}, "teardown": {"duration": 0.0001362878829240799, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestJobScheduler::test_activate_reservation", "lineno": 637, "outcome": "passed", "keywords": ["test_activate_reservation", "TestJobScheduler", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00021931296214461327, "outcome": "passed"}, "call": {"duration": 0.0002231718972325325, "outcome": "passed"}, "teardown": {"duration": 0.00013592001050710678, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestJobScheduler::test_active_reservations", "lineno": 669, "outcome": "passed", "keywords": ["test_active_reservations", "TestJobScheduler", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002051088958978653, "outcome": "passed"}, "call": {"duration": 0.00023645814508199692, "outcome": "passed"}, "teardown": {"duration": 0.00013168901205062866, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestJobScheduler::test_maintenance_windows", "lineno": 713, "outcome": "passed", "keywords": ["test_maintenance_windows", "TestJobScheduler", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00019485270604491234, "outcome": "passed"}, "call": {"duration": 0.00023075193166732788, "outcome": "passed"}, "teardown": {"duration": 0.0001276480033993721, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/job_management/test_long_running_job_manager.py::TestJobScheduler::test_preemption_history", "lineno": 768, "outcome": "passed", "keywords": ["test_preemption_history", "TestJobScheduler", "test_long_running_job_manager.py", "job_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00019243592396378517, "outcome": "passed"}, "call": {"duration": 0.0002037729136645794, "outcome": "passed"}, "teardown": {"duration": 0.00013147341087460518, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py::test_resource_data_collector_init", "lineno": 104, "outcome": "passed", "keywords": ["test_resource_data_collector_init", "test_data_collector.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00041935406625270844, "outcome": "passed"}, "call": {"duration": 0.0001434660516679287, "outcome": "passed"}, "teardown": {"duration": 0.00012491317465901375, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py::test_record_data_point", "lineno": 118, "outcome": "passed", "keywords": ["test_record_data_point", "test_data_collector.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00019041122868657112, "outcome": "passed"}, "call": {"duration": 0.0005834531038999557, "outcome": "passed"}, "teardown": {"duration": 0.0001367330551147461, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py::test_collect_simulation_data", "lineno": 156, "outcome": "passed", "keywords": ["test_collect_simulation_data", "test_data_collector.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00030611781403422356, "outcome": "passed"}, "call": {"duration": 0.00022709602490067482, "outcome": "passed"}, "teardown": {"duration": 0.00014238618314266205, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py::test_collect_node_data", "lineno": 183, "outcome": "passed", "keywords": ["test_collect_node_data", "test_data_collector.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00017828308045864105, "outcome": "passed"}, "call": {"duration": 0.0008867350406944752, "outcome": "passed"}, "teardown": {"duration": 0.00014187023043632507, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py::test_get_resource_history", "lineno": 220, "outcome": "passed", "keywords": ["test_get_resource_history", "test_data_collector.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00034029409289360046, "outcome": "passed"}, "call": {"duration": 0.043519153259694576, "outcome": "passed"}, "teardown": {"duration": 0.0001771547831594944, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py::test_aggregate_data_points", "lineno": 288, "outcome": "passed", "keywords": ["test_aggregate_data_points", "test_data_collector.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00021371478214859962, "outcome": "passed"}, "call": {"duration": 0.023181248921900988, "outcome": "passed"}, "teardown": {"duration": 0.0001547113060951233, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py::test_collect_batch_data", "lineno": 348, "outcome": "passed", "keywords": ["test_collect_batch_data", "test_data_collector.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00033383211120963097, "outcome": "passed"}, "call": {"duration": 0.00029470399022102356, "outcome": "passed"}, "teardown": {"duration": 0.00014674291014671326, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_data_collector.py::test_resource_usage_analyzer", "lineno": 384, "outcome": "passed", "keywords": ["test_resource_usage_analyzer", "test_data_collector.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0003657899796962738, "outcome": "passed"}, "call": {"duration": 0.11532796313986182, "outcome": "passed"}, "teardown": {"duration": 0.00021191919222474098, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_forecaster.py::test_forecaster_init", "lineno": 156, "outcome": "passed", "keywords": ["test_forecaster_init", "test_forecaster.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00030144210904836655, "outcome": "passed"}, "call": {"duration": 0.0001420280896127224, "outcome": "passed"}, "teardown": {"duration": 0.0001535089686512947, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_forecaster.py::test_extract_time_features", "lineno": 165, "outcome": "passed", "keywords": ["test_extract_time_features", "test_forecaster.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00024918606504797935, "outcome": "passed"}, "call": {"duration": 0.00019036978483200073, "outcome": "passed"}, "teardown": {"duration": 0.0001567220315337181, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_forecaster.py::test_train_model", "lineno": 195, "outcome": "passed", "keywords": ["test_train_model", "test_forecaster.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.025068514980375767, "outcome": "passed"}, "call": {"duration": 1.1202167998999357, "outcome": "passed"}, "teardown": {"duration": 0.0002696928568184376, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_forecaster.py::test_forecast_resource_usage", "lineno": 250, "outcome": "passed", "keywords": ["test_forecast_resource_usage", "test_forecaster.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.024765124078840017, "outcome": "passed"}, "call": {"duration": 1.0169480666518211, "outcome": "passed"}, "teardown": {"duration": 0.00034175580367445946, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_forecaster.py::test_update_forecast_with_actuals", "lineno": 325, "outcome": "passed", "keywords": ["test_update_forecast_with_actuals", "test_forecaster.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.025570194236934185, "outcome": "passed"}, "call": {"duration": 0.22011053003370762, "outcome": "passed"}, "teardown": {"duration": 0.0002299039624631405, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_forecaster.py::test_create_resource_projection", "lineno": 365, "outcome": "passed", "keywords": ["test_create_resource_projection", "test_forecaster.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0821191780269146, "outcome": "passed"}, "call": {"duration": 1.1569380131550133, "outcome": "passed"}, "teardown": {"duration": 0.00039256270974874496, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_forecaster.py::test_detect_anomalies", "lineno": 413, "outcome": "passed", "keywords": ["test_detect_anomalies", "test_forecaster.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.025501294992864132, "outcome": "passed"}, "call": {"duration": 0.05310662090778351, "outcome": "passed"}, "teardown": {"duration": 0.000254135113209486, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py::test_optimizer_init", "lineno": 151, "outcome": "passed", "keywords": ["test_optimizer_init", "test_optimizer.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00040419725701212883, "outcome": "passed"}, "call": {"duration": 0.0001645549200475216, "outcome": "passed"}, "teardown": {"duration": 0.00018721586093306541, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py::test_optimize_simulation_resources", "lineno": 168, "outcome": "passed", "keywords": ["test_optimize_simulation_resources", "test_optimizer.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0004218481481075287, "outcome": "passed"}, "call": {"duration": 12.02407308621332, "outcome": "passed"}, "teardown": {"duration": 0.000508094672113657, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py::test_generate_capacity_plan", "lineno": 232, "outcome": "passed", "keywords": ["test_generate_capacity_plan", "test_optimizer.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0011976663954555988, "outcome": "passed"}, "call": {"duration": 0.0006630788557231426, "outcome": "passed"}, "teardown": {"duration": 0.00021414272487163544, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py::test_resource_allocation_recommendation", "lineno": 267, "outcome": "passed", "keywords": ["test_resource_allocation_recommendation", "test_optimizer.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00013215560466051102, "outcome": "passed"}, "call": {"duration": 0.00017686421051621437, "outcome": "passed"}, "teardown": {"duration": 0.00010995520278811455, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py::test_capacity_planning_recommendation", "lineno": 318, "outcome": "passed", "keywords": ["test_capacity_planning_recommendation", "test_optimizer.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00011589797213673592, "outcome": "passed"}, "call": {"duration": 0.00015373900532722473, "outcome": "passed"}, "teardown": {"duration": 0.00010488275438547134, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py::test_resource_cost", "lineno": 368, "outcome": "passed", "keywords": ["test_resource_cost", "test_optimizer.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0003204718232154846, "outcome": "passed"}, "call": {"duration": 0.00015003886073827744, "outcome": "passed"}, "teardown": {"duration": 0.00017882278189063072, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py::test_mock_allocation_and_capacity", "lineno": 375, "outcome": "passed", "keywords": ["test_mock_allocation_and_capacity", "test_optimizer.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0004852772690355778, "outcome": "passed"}, "call": {"duration": 0.0001671840436756611, "outcome": "passed"}, "teardown": {"duration": 0.00020070327445864677, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_optimizer.py::test_justification_generation", "lineno": 402, "outcome": "passed", "keywords": ["test_justification_generation", "test_optimizer.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00029918598011136055, "outcome": "passed"}, "call": {"duration": 0.0012552901171147823, "outcome": "passed"}, "teardown": {"duration": 0.00017848704010248184, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_reporter.py::test_reporter_init", "lineno": 145, "outcome": "passed", "keywords": ["test_reporter_init", "test_reporter.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0004111998714506626, "outcome": "passed"}, "call": {"duration": 0.00014103297144174576, "outcome": "passed"}, "teardown": {"duration": 0.00021408125758171082, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_reporter.py::test_generate_utilization_report", "lineno": 154, "outcome": "passed", "keywords": ["test_generate_utilization_report", "test_reporter.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.003984680864959955, "outcome": "passed"}, "call": {"duration": 0.3705003820359707, "outcome": "passed"}, "teardown": {"duration": 0.00032138824462890625, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_reporter.py::test_generate_forecast_report", "lineno": 252, "outcome": "passed", "keywords": ["test_generate_forecast_report", "test_reporter.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.003911765292286873, "outcome": "passed"}, "call": {"duration": 0.26877530198544264, "outcome": "passed"}, "teardown": {"duration": 0.00033632107079029083, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_reporter.py::test_generate_grant_report", "lineno": 332, "outcome": "passed", "keywords": ["test_generate_grant_report", "test_reporter.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0038248789496719837, "outcome": "passed"}, "call": {"duration": 0.5734643931500614, "outcome": "passed"}, "teardown": {"duration": 0.0003197113983333111, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_reporter.py::test_generate_recommendation_report", "lineno": 422, "outcome": "passed", "keywords": ["test_generate_recommendation_report", "test_reporter.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.003788840025663376, "outcome": "passed"}, "call": {"duration": 2.5697451047599316, "outcome": "passed"}, "teardown": {"duration": 0.000524827279150486, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/resource_forecasting/test_reporter.py::test_get_report", "lineno": 483, "outcome": "passed", "keywords": ["test_get_report", "test_reporter.py", "resource_forecasting", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.004310408607125282, "outcome": "passed"}, "call": {"duration": 0.06247467175126076, "outcome": "passed"}, "teardown": {"duration": 0.00029834918677806854, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement::test_empty_resource_allocation", "lineno": 302, "outcome": "passed", "keywords": ["test_empty_resource_allocation", "TestEdgeCasePriorityManagement", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0008806721307337284, "outcome": "passed"}, "call": {"duration": 0.001420367043465376, "outcome": "passed"}, "teardown": {"duration": 0.00016738194972276688, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement::test_very_high_priority_scenario", "lineno": 371, "outcome": "passed", "keywords": ["test_very_high_priority_scenario", "TestEdgeCasePriorityManagement", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00031677121296525, "outcome": "passed"}, "call": {"duration": 0.001148940995335579, "outcome": "passed"}, "teardown": {"duration": 0.00015421584248542786, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement::test_unusual_resource_types", "lineno": 470, "outcome": "passed", "keywords": ["test_unusual_resource_types", "TestEdgeCasePriorityManagement", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0003145080991089344, "outcome": "passed"}, "call": {"duration": 0.0010457891039550304, "outcome": "passed"}, "teardown": {"duration": 0.00015178509056568146, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement::test_paused_scenario_handling", "lineno": 545, "outcome": "passed", "keywords": ["test_paused_scenario_handling", "TestEdgeCasePriorityManagement", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0003107469528913498, "outcome": "passed"}, "call": {"duration": 0.0010879202745854855, "outcome": "passed"}, "teardown": {"duration": 0.00015150709077715874, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.BALANCED]", "lineno": 624, "outcome": "passed", "keywords": ["test_reallocation_strategy[ResourceReallocationStrategy.BALANCED]", "parametrize", "pytestmark", "ResourceReallocationStrategy.BALANCED", "TestResourceAllocationStrategies", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00039396900683641434, "outcome": "passed"}, "call": {"duration": 0.0003120820038020611, "outcome": "passed"}, "teardown": {"duration": 0.00015320023521780968, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.PROPORTIONAL]", "lineno": 624, "outcome": "passed", "keywords": ["test_reallocation_strategy[ResourceReallocationStrategy.PROPORTIONAL]", "parametrize", "pytestmark", "ResourceReallocationStrategy.PROPORTIONAL", "TestResourceAllocationStrategies", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00036548636853694916, "outcome": "passed"}, "call": {"duration": 0.00023476267233490944, "outcome": "passed"}, "teardown": {"duration": 0.00014887703582644463, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.THRESHOLD_BASED]", "lineno": 624, "outcome": "passed", "keywords": ["test_reallocation_strategy[ResourceReallocationStrategy.THRESHOLD_BASED]", "parametrize", "pytestmark", "ResourceReallocationStrategy.THRESHOLD_BASED", "TestResourceAllocationStrategies", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0003546541556715965, "outcome": "passed"}, "call": {"duration": 0.0002419562079012394, "outcome": "passed"}, "teardown": {"duration": 0.00015078624710440636, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.GRADUAL]", "lineno": 624, "outcome": "passed", "keywords": ["test_reallocation_strategy[ResourceReallocationStrategy.GRADUAL]", "parametrize", "pytestmark", "ResourceReallocationStrategy.GRADUAL", "TestResourceAllocationStrategies", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00034868717193603516, "outcome": "passed"}, "call": {"duration": 0.00024122558534145355, "outcome": "passed"}, "teardown": {"duration": 0.0001495140604674816, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.AGGRESSIVE]", "lineno": 624, "outcome": "passed", "keywords": ["test_reallocation_strategy[ResourceReallocationStrategy.AGGRESSIVE]", "parametrize", "pytestmark", "ResourceReallocationStrategy.AGGRESSIVE", "TestResourceAllocationStrategies", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0003598262555897236, "outcome": "passed"}, "call": {"duration": 0.00022321008145809174, "outcome": "passed"}, "teardown": {"duration": 0.00014895200729370117, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestDynamicPriorityEvents::test_sudden_priority_change", "lineno": 722, "outcome": "passed", "keywords": ["test_sudden_priority_change", "TestDynamicPriorityEvents", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00041522271931171417, "outcome": "passed"}, "call": {"duration": 0.0011248569935560226, "outcome": "passed"}, "teardown": {"duration": 0.00014945119619369507, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestDynamicPriorityEvents::test_queue_for_recalculation", "lineno": 899, "outcome": "passed", "keywords": ["test_queue_for_recalculation", "TestDynamicPriorityEvents", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0003123469650745392, "outcome": "passed"}, "call": {"duration": 0.0011095893569290638, "outcome": "passed"}, "teardown": {"duration": 0.0001610768958926201, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestDynamicPriorityEvents::test_recompute_all_priorities", "lineno": 971, "outcome": "passed", "keywords": ["test_recompute_all_priorities", "TestDynamicPriorityEvents", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00031289132311940193, "outcome": "passed"}, "call": {"duration": 0.002612779848277569, "outcome": "passed"}, "teardown": {"duration": 0.0001554451882839203, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestPriorityTrendAnalysis::test_priority_trend_tracking", "lineno": 1117, "outcome": "passed", "keywords": ["test_priority_trend_tracking", "TestPriorityTrendAnalysis", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0003323731943964958, "outcome": "passed"}, "call": {"duration": 0.0002255598083138466, "outcome": "passed"}, "teardown": {"duration": 0.00016209669411182404, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestResourceAllocationHistory::test_resource_allocation_tracking", "lineno": 1203, "outcome": "passed", "keywords": ["test_resource_allocation_tracking", "TestResourceAllocationHistory", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00030610617250204086, "outcome": "passed"}, "call": {"duration": 0.0031430632807314396, "outcome": "passed"}, "teardown": {"duration": 0.00017347512766718864, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestErrorHandling::test_invalid_priority_override", "lineno": 1333, "outcome": "passed", "keywords": ["test_invalid_priority_override", "TestErrorHandling", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00044303108006715775, "outcome": "passed"}, "call": {"duration": 0.00021131522953510284, "outcome": "passed"}, "teardown": {"duration": 0.0001373537816107273, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestErrorHandling::test_empty_scenario_list", "lineno": 1388, "outcome": "passed", "keywords": ["test_empty_scenario_list", "TestErrorHandling", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00012119021266698837, "outcome": "passed"}, "call": {"duration": 0.00040371017530560493, "outcome": "passed", "log": [{"name": "concurrent_task_scheduler.scenario_management.priority_manager", "msg": "Need at least 2 scenarios for resource reallocation", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/scenario_management/priority_manager.py", "filename": "priority_manager.py", "module": "priority_manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 412, "funcName": "reallocate_resources", "created": 1750047426.2061794, "msecs": 206.0, "relativeCreated": 23802.629470825195, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.scenario_management.priority_manager", "msg": "Need at least 2 scenarios to compare, skipping adjustment", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/scenario_management/priority_manager.py", "filename": "priority_manager.py", "module": "priority_manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 329, "funcName": "compare_and_adjust_priorities", "created": 1750047426.2063444, "msecs": 206.0, "relativeCreated": 23802.794456481934, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}]}, "teardown": {"duration": 0.00012138998135924339, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_advanced_priority_management.py::TestErrorHandling::test_single_scenario", "lineno": 1399, "outcome": "passed", "keywords": ["test_single_scenario", "TestErrorHandling", "test_advanced_priority_management.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0003207726404070854, "outcome": "passed"}, "call": {"duration": 0.00030127493664622307, "outcome": "passed", "log": [{"name": "concurrent_task_scheduler.scenario_management.priority_manager", "msg": "Need at least 2 scenarios to compare, skipping adjustment", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/scenario_management/priority_manager.py", "filename": "priority_manager.py", "module": "priority_manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 329, "funcName": "compare_and_adjust_priorities", "created": 1750047426.2074993, "msecs": 207.0, "relativeCreated": 23803.9493560791, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}, {"name": "concurrent_task_scheduler.scenario_management.priority_manager", "msg": "Need at least 2 scenarios for resource reallocation", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/scenario_management/priority_manager.py", "filename": "priority_manager.py", "module": "priority_manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 412, "funcName": "reallocate_resources", "created": 1750047426.2075756, "msecs": 207.0, "relativeCreated": 23804.025650024414, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}]}, "teardown": {"duration": 0.00014434615150094032, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_initialization", "lineno": 211, "outcome": "passed", "keywords": ["test_initialization", "TestScenarioComparator", "test_comparator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0010296190157532692, "outcome": "passed"}, "call": {"duration": 0.00047778990119695663, "outcome": "passed"}, "teardown": {"duration": 0.00015793321654200554, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_scenarios_direct", "lineno": 226, "outcome": "passed", "keywords": ["test_compare_scenarios_direct", "TestScenarioComparator", "test_comparator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.001030969899147749, "outcome": "passed"}, "call": {"duration": 0.0002872990444302559, "outcome": "passed"}, "teardown": {"duration": 0.0001763608306646347, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_scenarios_weighted", "lineno": 262, "outcome": "passed", "keywords": ["test_compare_scenarios_weighted", "TestScenarioComparator", "test_comparator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0011231289245188236, "outcome": "passed"}, "call": {"duration": 0.00029602600261569023, "outcome": "passed"}, "teardown": {"duration": 0.00015800585970282555, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_scenarios_relative", "lineno": 284, "outcome": "passed", "keywords": ["test_compare_scenarios_relative", "TestScenarioComparator", "test_comparator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0009933719411492348, "outcome": "passed"}, "call": {"duration": 0.0004375288262963295, "outcome": "passed"}, "teardown": {"duration": 0.0001636771485209465, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_scenarios_rank_based", "lineno": 318, "outcome": "passed", "keywords": ["test_compare_scenarios_rank_based", "TestScenarioComparator", "test_comparator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.001008761115372181, "outcome": "passed"}, "call": {"duration": 0.0003288751468062401, "outcome": "passed"}, "teardown": {"duration": 0.00015596672892570496, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_get_comparison_history", "lineno": 341, "outcome": "passed", "keywords": ["test_get_comparison_history", "TestScenarioComparator", "test_comparator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0011248551309108734, "outcome": "passed"}, "call": {"duration": 0.0003480869345366955, "outcome": "passed"}, "teardown": {"duration": 0.00015742797404527664, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_multiple_scenarios", "lineno": 377, "outcome": "passed", "keywords": ["test_compare_multiple_scenarios", "TestScenarioComparator", "test_comparator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0011086268350481987, "outcome": "passed"}, "call": {"duration": 0.000650930218398571, "outcome": "passed"}, "teardown": {"duration": 0.00017279665917158127, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_group_scenarios_by_similarity", "lineno": 403, "outcome": "passed", "keywords": ["test_group_scenarios_by_similarity", "TestScenarioComparator", "test_comparator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002517830580472946, "outcome": "passed"}, "call": {"duration": 0.00028938986361026764, "outcome": "passed"}, "teardown": {"duration": 0.00013956474140286446, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_comparator.py::TestScenarioComparator::test_identify_complementary_scenarios", "lineno": 442, "outcome": "passed", "keywords": ["test_identify_complementary_scenarios", "TestScenarioComparator", "test_comparator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0010167709551751614, "outcome": "passed"}, "call": {"duration": 0.0004462581127882004, "outcome": "passed"}, "teardown": {"duration": 0.00016783084720373154, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_initialization", "lineno": 84, "outcome": "passed", "keywords": ["test_initialization", "TestScenarioEvaluator", "test_evaluator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00014752568677067757, "outcome": "passed"}, "call": {"duration": 0.00021752994507551193, "outcome": "passed"}, "teardown": {"duration": 0.00011715013533830643, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluate_scenario", "lineno": 101, "outcome": "passed", "keywords": ["test_evaluate_scenario", "TestScenarioEvaluator", "test_evaluator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00026390329003334045, "outcome": "passed"}, "call": {"duration": 0.0002997671253979206, "outcome": "passed"}, "teardown": {"duration": 0.00013851793482899666, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluation_with_custom_weights", "lineno": 125, "outcome": "passed", "keywords": ["test_evaluation_with_custom_weights", "TestScenarioEvaluator", "test_evaluator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00024253129959106445, "outcome": "passed"}, "call": {"duration": 0.0003114258870482445, "outcome": "passed"}, "teardown": {"duration": 0.00014476384967565536, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluate_empty_scenario", "lineno": 157, "outcome": "passed", "keywords": ["test_evaluate_empty_scenario", "TestScenarioEvaluator", "test_evaluator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00011949706822633743, "outcome": "passed"}, "call": {"duration": 0.00021892739459872246, "outcome": "passed"}, "teardown": {"duration": 0.00010804273188114166, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_manual_rating_integration", "lineno": 179, "outcome": "passed", "keywords": ["test_manual_rating_integration", "TestScenarioEvaluator", "test_evaluator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002466361038386822, "outcome": "passed"}, "call": {"duration": 0.00032750098034739494, "outcome": "passed"}, "teardown": {"duration": 0.00014469586312770844, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluation_history_tracking", "lineno": 206, "outcome": "passed", "keywords": ["test_evaluation_history_tracking", "TestScenarioEvaluator", "test_evaluator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00023511098697781563, "outcome": "passed"}, "call": {"duration": 0.0003376491367816925, "outcome": "passed"}, "teardown": {"duration": 0.00014268280938267708, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_compare_evaluations", "lineno": 226, "outcome": "passed", "keywords": ["test_compare_evaluations", "TestScenarioEvaluator", "test_evaluator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00022568181157112122, "outcome": "passed"}, "call": {"duration": 0.00030266912654042244, "outcome": "passed"}, "teardown": {"duration": 0.0001335572451353073, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluation_criteria_functions", "lineno": 262, "outcome": "passed", "keywords": ["test_evaluation_criteria_functions", "__wrapped__", "patchings", "TestScenarioEvaluator", "test_evaluator.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00022281939163804054, "outcome": "passed"}, "call": {"duration": 0.0005610845983028412, "outcome": "passed"}, "teardown": {"duration": 0.00015252595767378807, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_initialization", "lineno": 241, "outcome": "passed", "keywords": ["test_initialization", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0016415007412433624, "outcome": "passed"}, "call": {"duration": 0.0009007761254906654, "outcome": "passed"}, "teardown": {"duration": 0.00017966004088521004, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_needs_evaluation", "lineno": 266, "outcome": "passed", "keywords": ["test_needs_evaluation", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00024934299290180206, "outcome": "passed"}, "call": {"duration": 0.00021613063290715218, "outcome": "passed"}, "teardown": {"duration": 0.00014153029769659042, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_evaluate_scenario_priority", "lineno": 294, "outcome": "passed", "keywords": ["test_evaluate_scenario_priority", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0008689751848578453, "outcome": "passed"}, "call": {"duration": 0.002078477293252945, "outcome": "passed"}, "teardown": {"duration": 0.000176937784999609, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_update_scenario_priority", "lineno": 315, "outcome": "passed", "keywords": ["test_update_scenario_priority", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0008481568656861782, "outcome": "passed"}, "call": {"duration": 0.0019144830293953419, "outcome": "passed"}, "teardown": {"duration": 0.00016173534095287323, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_compare_and_adjust_priorities", "lineno": 362, "outcome": "passed", "keywords": ["test_compare_and_adjust_priorities", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0016239900141954422, "outcome": "passed"}, "call": {"duration": 0.0017134598456323147, "outcome": "passed", "log": [{"name": "concurrent_task_scheduler.scenario_management.priority_manager", "msg": "Need at least 2 scenarios to compare, skipping adjustment", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/scenario_management/priority_manager.py", "filename": "priority_manager.py", "module": "priority_manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 329, "funcName": "compare_and_adjust_priorities", "created": 1750047426.251263, "msecs": 251.0, "relativeCreated": 23847.712993621826, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}]}, "teardown": {"duration": 0.00018056994304060936, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_reallocate_resources", "lineno": 398, "outcome": "passed", "keywords": ["test_reallocate_resources", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002681012265384197, "outcome": "passed"}, "call": {"duration": 0.0005488288588821888, "outcome": "passed", "log": [{"name": "concurrent_task_scheduler.scenario_management.priority_manager", "msg": "Need at least 2 scenarios for resource reallocation", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/scenario_management/priority_manager.py", "filename": "priority_manager.py", "module": "priority_manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 412, "funcName": "reallocate_resources", "created": 1750047426.25281, "msecs": 252.0, "relativeCreated": 23849.260091781616, "thread": 139955644696384, "threadName": "MainThread", "processName": "MainProcess", "process": 1922860}]}, "teardown": {"duration": 0.00013681082054972649, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_manual_priority_override", "lineno": 492, "outcome": "passed", "keywords": ["test_manual_priority_override", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00024095270782709122, "outcome": "passed"}, "call": {"duration": 0.00020080013200640678, "outcome": "passed"}, "teardown": {"duration": 0.00014401227235794067, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_recompute_all_priorities", "lineno": 529, "outcome": "passed", "keywords": ["test_recompute_all_priorities", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0016334857791662216, "outcome": "passed"}, "call": {"duration": 0.0015932242386043072, "outcome": "passed"}, "teardown": {"duration": 0.00017600972205400467, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_get_priority_changes", "lineno": 552, "outcome": "passed", "keywords": ["test_get_priority_changes", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002463189885020256, "outcome": "passed"}, "call": {"duration": 0.00020883092656731606, "outcome": "passed"}, "teardown": {"duration": 0.00014005834236741066, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_get_priority_trend", "lineno": 593, "outcome": "passed", "keywords": ["test_get_priority_trend", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00022676214575767517, "outcome": "passed"}, "call": {"duration": 0.00021984102204442024, "outcome": "passed"}, "teardown": {"duration": 0.00013935193419456482, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_priority_manager.py::TestPriorityManager::test_get_resource_allocation_history", "lineno": 629, "outcome": "passed", "keywords": ["test_get_resource_allocation_history", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00022236676886677742, "outcome": "passed"}, "call": {"duration": 0.0001981118693947792, "outcome": "passed"}, "teardown": {"duration": 0.00014430424198508263, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_end_to_end_workflow", "lineno": 230, "outcome": "passed", "keywords": ["test_end_to_end_workflow", "TestScenarioManagementIntegration", "test_scenario_management_integration.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.000359236728399992, "outcome": "passed"}, "call": {"duration": 0.0008484986610710621, "outcome": "passed"}, "teardown": {"duration": 0.00017225882038474083, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_scenario_metric_updates_propagation", "lineno": 331, "outcome": "passed", "keywords": ["test_scenario_metric_updates_propagation", "TestScenarioManagementIntegration", "test_scenario_management_integration.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00033699069172143936, "outcome": "passed"}, "call": {"duration": 0.0007772678509354591, "outcome": "passed"}, "teardown": {"duration": 0.00015893392264842987, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_priority_manager_adaptation", "lineno": 377, "outcome": "passed", "keywords": ["test_priority_manager_adaptation", "TestScenarioManagementIntegration", "test_scenario_management_integration.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00034747691825032234, "outcome": "passed"}, "call": {"duration": 0.0009849467314779758, "outcome": "passed"}, "teardown": {"duration": 0.0001607821322977543, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_manual_override_integration", "lineno": 435, "outcome": "passed", "keywords": ["test_manual_override_integration", "TestScenarioManagementIntegration", "test_scenario_management_integration.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00034310808405280113, "outcome": "passed"}, "call": {"duration": 0.000497136265039444, "outcome": "passed"}, "teardown": {"duration": 0.00016150996088981628, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScientificMetric::test_initialization", "lineno": 24, "outcome": "passed", "keywords": ["test_initialization", "TestScientificMetric", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00013244478031992912, "outcome": "passed"}, "call": {"duration": 0.0001584598794579506, "outcome": "passed"}, "teardown": {"duration": 0.00010678591206669807, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScientificMetric::test_normalized_score", "lineno": 58, "outcome": "passed", "keywords": ["test_normalized_score", "TestScientificMetric", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00011029327288269997, "outcome": "passed"}, "call": {"duration": 0.00020545395091176033, "outcome": "passed"}, "teardown": {"duration": 0.00011349981650710106, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestResearchObjective::test_initialization", "lineno": 140, "outcome": "passed", "keywords": ["test_initialization", "TestResearchObjective", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00011206930503249168, "outcome": "passed"}, "call": {"duration": 0.00014747213572263718, "outcome": "passed"}, "teardown": {"duration": 0.00010445015504956245, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestResearchObjective::test_is_relevant_to_scenario", "lineno": 158, "outcome": "passed", "keywords": ["test_is_relevant_to_scenario", "TestResearchObjective", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00010976195335388184, "outcome": "passed"}, "call": {"duration": 0.00016064802184700966, "outcome": "passed"}, "teardown": {"duration": 0.00011088326573371887, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenario::test_initialization", "lineno": 191, "outcome": "passed", "keywords": ["test_initialization", "TestScenario", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00010959291830658913, "outcome": "passed"}, "call": {"duration": 0.00014878390356898308, "outcome": "passed"}, "teardown": {"duration": 0.00010176235809922218, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenario::test_calculate_priority_score", "lineno": 212, "outcome": "passed", "keywords": ["test_calculate_priority_score", "TestScenario", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00012294668704271317, "outcome": "passed"}, "call": {"duration": 0.00018586404621601105, "outcome": "passed"}, "teardown": {"duration": 0.00010206690058112144, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenario::test_update_priority", "lineno": 260, "outcome": "passed", "keywords": ["test_update_priority", "TestScenario", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00010675797238945961, "outcome": "passed"}, "call": {"duration": 0.00028519099578261375, "outcome": "passed"}, "teardown": {"duration": 0.00011413590982556343, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenario::test_get_simulation_status_counts", "lineno": 290, "outcome": "passed", "keywords": ["test_get_simulation_status_counts", "TestScenario", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00011248700320720673, "outcome": "passed"}, "call": {"duration": 0.0001833033747971058, "outcome": "passed"}, "teardown": {"duration": 0.00010399706661701202, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenario::test_get_derived_priority", "lineno": 334, "outcome": "passed", "keywords": ["test_get_derived_priority", "TestScenario", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00010841665789484978, "outcome": "passed"}, "call": {"duration": 0.00016182800754904747, "outcome": "passed"}, "teardown": {"duration": 0.00010761991143226624, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenario::test_total_progress", "lineno": 358, "outcome": "passed", "keywords": ["test_total_progress", "TestScenario", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0001064441166818142, "outcome": "passed"}, "call": {"duration": 0.004997352138161659, "outcome": "passed"}, "teardown": {"duration": 0.00013090716674923897, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestComparisonResult::test_initialization", "lineno": 394, "outcome": "passed", "keywords": ["test_initialization", "TestComparisonResult", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0001264750026166439, "outcome": "passed"}, "call": {"duration": 0.00015149684622883797, "outcome": "passed"}, "teardown": {"duration": 0.00010422104969620705, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestComparisonResult::test_clear_winner", "lineno": 413, "outcome": "passed", "keywords": ["test_clear_winner", "TestComparisonResult", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00012042606249451637, "outcome": "passed"}, "call": {"duration": 0.0001508970744907856, "outcome": "passed"}, "teardown": {"duration": 0.00010831141844391823, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenarioEvaluationResult::test_initialization", "lineno": 467, "outcome": "passed", "keywords": ["test_initialization", "TestScenarioEvaluationResult", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00012710504233837128, "outcome": "passed"}, "call": {"duration": 0.00015962263569235802, "outcome": "passed"}, "teardown": {"duration": 0.00010322034358978271, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestScenarioEvaluationResult::test_should_adjust_priority", "lineno": 488, "outcome": "passed", "keywords": ["test_should_adjust_priority", "TestScenarioEvaluationResult", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00010715937241911888, "outcome": "passed"}, "call": {"duration": 0.00014190422371029854, "outcome": "passed"}, "teardown": {"duration": 0.00010572420433163643, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestResourceAllocation::test_initialization", "lineno": 513, "outcome": "passed", "keywords": ["test_initialization", "TestResourceAllocation", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00010724598541855812, "outcome": "passed"}, "call": {"duration": 0.00013862084597349167, "outcome": "passed"}, "teardown": {"duration": 9.984662756323814e-05, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestResourceAllocation::test_get_absolute_allocation", "lineno": 531, "outcome": "passed", "keywords": ["test_get_absolute_allocation", "TestResourceAllocation", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0001080147922039032, "outcome": "passed"}, "call": {"duration": 0.0001392783597111702, "outcome": "passed"}, "teardown": {"duration": 0.00010098796337842941, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/scenario_management/test_scenario_model.py::TestResourceAllocation::test_is_valid", "lineno": 551, "outcome": "passed", "keywords": ["test_is_valid", "TestResourceAllocation", "test_scenario_model.py", "scenario_management", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00010719615966081619, "outcome": "passed"}, "call": {"duration": 0.00014998391270637512, "outcome": "passed"}, "teardown": {"duration": 0.00010629883036017418, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_full_system_integration.py::TestEndToEndWorkflow::test_scenario_lifecycle", "lineno": 283, "outcome": "passed", "keywords": ["test_scenario_lifecycle", "TestEndToEndWorkflow", "test_full_system_integration.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0035522878170013428, "outcome": "passed"}, "call": {"duration": 0.004848550073802471, "outcome": "passed", "stdout": "Original priority: 0.7\nNew priority: 0.8999999999999999\nCheckpoint to restore: cp_20250616041706\nScenario lifecycle test completed successfully\n"}, "teardown": {"duration": 0.00019840430468320847, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_full_system_integration.py::TestEndToEndWorkflow::test_multi_scenario_prioritization", "lineno": 440, "outcome": "passed", "keywords": ["test_multi_scenario_prioritization", "TestEndToEndWorkflow", "test_full_system_integration.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0008391127921640873, "outcome": "passed"}, "call": {"duration": 0.0008142990991473198, "outcome": "passed"}, "teardown": {"duration": 0.00019117724150419235, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_full_system_integration.py::TestFailureResilienceWithForecasting::test_forecasting_affects_checkpoint_frequency", "lineno": 616, "outcome": "passed", "keywords": ["test_forecasting_affects_checkpoint_frequency", "TestFailureResilienceWithForecasting", "test_full_system_integration.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0008182930760085583, "outcome": "passed"}, "call": {"duration": 0.0009250380098819733, "outcome": "passed"}, "teardown": {"duration": 0.00018699606880545616, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_full_system_integration.py::TestResourceOptimizationWithPriorities::test_priority_affects_resource_allocation", "lineno": 691, "outcome": "passed", "keywords": ["test_priority_affects_resource_allocation", "TestResourceOptimizationWithPriorities", "test_full_system_integration.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0008108532056212425, "outcome": "passed"}, "call": {"duration": 0.00030516367405653, "outcome": "passed"}, "teardown": {"duration": 0.0001819259487092495, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_compare_and_adjust_priorities", "lineno": 221, "outcome": "passed", "keywords": ["test_compare_and_adjust_priorities", "TestPriorityManager", "test_priority_manager.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002703131176531315, "outcome": "passed"}, "call": {"duration": 0.0390022830106318, "outcome": "passed"}, "teardown": {"duration": 0.0001477738842368126, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_force_update_ignores_threshold", "lineno": 195, "outcome": "passed", "keywords": ["test_force_update_ignores_threshold", "TestPriorityManager", "test_priority_manager.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0001698378473520279, "outcome": "passed"}, "call": {"duration": 0.03895660303533077, "outcome": "passed"}, "teardown": {"duration": 0.0001438329927623272, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_get_priority_changes", "lineno": 482, "outcome": "passed", "keywords": ["test_get_priority_changes", "TestPriorityManager", "test_priority_manager.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00017435289919376373, "outcome": "passed"}, "call": {"duration": 0.03725607879459858, "outcome": "passed"}, "teardown": {"duration": 0.00014641182497143745, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_get_priority_trend", "lineno": 512, "outcome": "passed", "keywords": ["test_get_priority_trend", "TestPriorityManager", "test_priority_manager.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00016887765377759933, "outcome": "passed"}, "call": {"duration": 0.03870838275179267, "outcome": "passed"}, "teardown": {"duration": 0.00014809425920248032, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_get_resource_allocation_history", "lineno": 541, "outcome": "passed", "keywords": ["test_get_resource_allocation_history", "TestPriorityManager", "test_priority_manager.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00014893291518092155, "outcome": "passed"}, "call": {"duration": 0.03838470624759793, "outcome": "passed"}, "teardown": {"duration": 0.00016345595940947533, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_initialization", "lineno": 96, "outcome": "passed", "keywords": ["test_initialization", "TestPriorityManager", "test_priority_manager.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00015388429164886475, "outcome": "passed"}, "call": {"duration": 0.03950127400457859, "outcome": "passed"}, "teardown": {"duration": 0.00015888595953583717, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_manual_priority_override", "lineno": 392, "outcome": "passed", "keywords": ["test_manual_priority_override", "TestPriorityManager", "test_priority_manager.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0001663239672780037, "outcome": "passed"}, "call": {"duration": 0.038077854085713625, "outcome": "passed"}, "teardown": {"duration": 0.0001831836998462677, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_needs_evaluation", "lineno": 104, "outcome": "passed", "keywords": ["test_needs_evaluation", "TestPriorityManager", "test_priority_manager.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00017356127500534058, "outcome": "passed"}, "call": {"duration": 0.037365743424743414, "outcome": "passed"}, "teardown": {"duration": 0.00015775393694639206, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_reallocate_resources", "lineno": 288, "outcome": "passed", "keywords": ["test_reallocate_resources", "TestPriorityManager", "test_priority_manager.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00015681283548474312, "outcome": "passed"}, "call": {"duration": 0.03892261395230889, "outcome": "passed"}, "teardown": {"duration": 0.00014895526692271233, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_reallocation_strategy_threshold_based", "lineno": 344, "outcome": "passed", "keywords": ["test_reallocation_strategy_threshold_based", "TestPriorityManager", "test_priority_manager.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00015720585361123085, "outcome": "passed"}, "call": {"duration": 0.03751890594139695, "outcome": "passed"}, "teardown": {"duration": 0.0001503732055425644, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_recompute_all_priorities", "lineno": 422, "outcome": "passed", "keywords": ["test_recompute_all_priorities", "TestPriorityManager", "test_priority_manager.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00015088124200701714, "outcome": "passed"}, "call": {"duration": 0.038649885915219784, "outcome": "passed"}, "teardown": {"duration": 0.0001611323095858097, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_small_priority_change_below_threshold", "lineno": 163, "outcome": "passed", "keywords": ["test_small_priority_change_below_threshold", "TestPriorityManager", "test_priority_manager.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00015452830120921135, "outcome": "passed"}, "call": {"duration": 0.03768580825999379, "outcome": "passed"}, "teardown": {"duration": 0.00015256507322192192, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_priority_manager.py::TestPriorityManager::test_update_scenario_priority", "lineno": 125, "outcome": "passed", "keywords": ["test_update_scenario_priority", "TestPriorityManager", "test_priority_manager.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00015756487846374512, "outcome": "passed"}, "call": {"duration": 0.03961113328114152, "outcome": "passed"}, "teardown": {"duration": 0.00020563090220093727, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_complementary_scenario_detection", "lineno": 304, "outcome": "passed", "keywords": ["test_complementary_scenario_detection", "TestScenarioManagementIntegration", "test_scenario_management_integration.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.0002627209760248661, "outcome": "passed"}, "call": {"duration": 0.001389247365295887, "outcome": "passed"}, "teardown": {"duration": 0.00015621166676282883, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_end_to_end_priority_workflow", "lineno": 137, "outcome": "passed", "keywords": ["test_end_to_end_priority_workflow", "TestScenarioManagementIntegration", "test_scenario_management_integration.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00014777667820453644, "outcome": "passed"}, "call": {"duration": 0.0012305215932428837, "outcome": "passed"}, "teardown": {"duration": 0.00014694686979055405, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_manual_override_integration", "lineno": 266, "outcome": "passed", "keywords": ["test_manual_override_integration", "TestScenarioManagementIntegration", "test_scenario_management_integration.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00016502104699611664, "outcome": "passed"}, "call": {"duration": 0.0006202948279678822, "outcome": "passed"}, "teardown": {"duration": 0.0001387377269566059, "outcome": "passed"}}, {"nodeid": "tests/scientific_computing/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_priority_adaptation_to_metric_changes", "lineno": 216, "outcome": "passed", "keywords": ["test_priority_adaptation_to_metric_changes", "TestScenarioManagementIntegration", "test_scenario_management_integration.py", "scientific_computing", "tests", "unified", ""], "setup": {"duration": 0.00014518387615680695, "outcome": "passed"}, "call": {"duration": 0.0006859488785266876, "outcome": "passed"}, "teardown": {"duration": 0.00027637090533971786, "outcome": "passed"}}], "warnings": [{"message": "Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "collect", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_config.py", "lineno": 323}, {"message": "`json_encoders` is deprecated. See https://docs.pydantic.dev/2.11/concepts/serialization/#custom-serializers for alternatives. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "collect", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py", "lineno": 293}, {"message": "`json_encoders` is deprecated. See https://docs.pydantic.dev/2.11/concepts/serialization/#custom-serializers for alternatives. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "collect", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py", "lineno": 293}, {"message": "`json_encoders` is deprecated. See https://docs.pydantic.dev/2.11/concepts/serialization/#custom-serializers for alternatives. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "collect", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py", "lineno": 293}, {"message": "`json_encoders` is deprecated. See https://docs.pydantic.dev/2.11/concepts/serialization/#custom-serializers for alternatives. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "collect", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py", "lineno": 293}, {"message": "`json_encoders` is deprecated. See https://docs.pydantic.dev/2.11/concepts/serialization/#custom-serializers for alternatives. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "collect", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py", "lineno": 293}, {"message": "`json_encoders` is deprecated. See https://docs.pydantic.dev/2.11/concepts/serialization/#custom-serializers for alternatives. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "collect", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py", "lineno": 293}, {"message": "`json_encoders` is deprecated. See https://docs.pydantic.dev/2.11/concepts/serialization/#custom-serializers for alternatives. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "collect", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py", "lineno": 293}, {"message": "`json_encoders` is deprecated. See https://docs.pydantic.dev/2.11/concepts/serialization/#custom-serializers for alternatives. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "collect", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py", "lineno": 293}, {"message": "`json_encoders` is deprecated. See https://docs.pydantic.dev/2.11/concepts/serialization/#custom-serializers for alternatives. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "collect", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py", "lineno": 293}, {"message": "`json_encoders` is deprecated. See https://docs.pydantic.dev/2.11/concepts/serialization/#custom-serializers for alternatives. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "collect", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py", "lineno": 293}, {"message": "`json_encoders` is deprecated. See https://docs.pydantic.dev/2.11/concepts/serialization/#custom-serializers for alternatives. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "collect", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py", "lineno": 293}, {"message": "Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "collect", "filename": "/home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_config.py", "lineno": 323}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 466}]}