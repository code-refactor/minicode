============================= test session starts ==============================
platform linux -- Python 3.10.11, pytest-8.3.5, pluggy-1.5.0
rootdir: /home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified
configfile: pyproject.toml
plugins: anyio-4.9.0, metadata-3.1.1, json-report-1.5.0, cov-6.1.1
collected 358 items

tests/render_farm_manager/integration/test_audit_logging.py ...          [  0%]
tests/render_farm_manager/integration/test_circular_dependency_patch.py . [  1%]
                                                                         [  1%]
tests/render_farm_manager/integration/test_energy_modes.py ..            [  1%]
tests/render_farm_manager/integration/test_energy_modes_fixed.py .       [  1%]
tests/render_farm_manager/integration/test_error_recovery.py ...         [  2%]
tests/render_farm_manager/integration/test_error_recovery_fixed.py F     [  3%]
tests/render_farm_manager/integration/test_error_recovery_fixed_complete.py . [  3%]
F                                                                        [  3%]
tests/render_farm_manager/integration/test_error_recovery_fixed_full.py . [  3%]
..                                                                       [  4%]
tests/render_farm_manager/integration/test_fault_tolerance.py F          [  4%]
tests/render_farm_manager/integration/test_fault_tolerance_fixed.py F    [  5%]
tests/render_farm_manager/integration/test_job_dependencies.py ...       [  5%]
tests/render_farm_manager/integration/test_job_dependencies_fixed.py .F  [  6%]
tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py . [  6%]
FF                                                                       [  7%]
tests/render_farm_manager/integration/test_job_dependencies_fixed_direct_complete.py F [  7%]
.                                                                        [  7%]
tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py F [  8%]
F.                                                                       [  8%]
tests/render_farm_manager/integration/test_job_dependencies_scheduling_patch.py F [  8%]
                                                                         [  8%]
tests/render_farm_manager/integration/test_job_dependencies_simple.py F. [  9%]
                                                                         [  9%]
tests/render_farm_manager/integration/test_job_dependencies_simple_fixed.py F [  9%]
.                                                                        [ 10%]
tests/render_farm_manager/integration/test_job_dependencies_simple_patched.py F [ 10%]
.                                                                        [ 10%]
tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch.py F [ 10%]
                                                                         [ 10%]
tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch_all.py . [ 11%]
F.                                                                       [ 11%]
tests/render_farm_manager/integration/test_priority_inheritance_patch.py . [ 12%]
                                                                         [ 12%]
tests/render_farm_manager/integration/test_render_farm_manager.py ....FF [ 13%]
FF.F.F                                                                   [ 15%]
tests/render_farm_manager/performance/test_performance.py F.F            [ 16%]
tests/render_farm_manager/unit/test_deadline_scheduler.py .FFFF...FFF    [ 19%]
tests/render_farm_manager/unit/test_deadline_scheduler_fixed.py F        [ 19%]
tests/render_farm_manager/unit/test_deadline_scheduler_full.py .         [ 19%]
tests/render_farm_manager/unit/test_energy_optimizer.py .F....F....      [ 22%]
tests/render_farm_manager/unit/test_node_specialization.py .FFF..FF.F.   [ 25%]
tests/render_farm_manager/unit/test_progressive_renderer.py ........F... [ 29%]
                                                                         [ 29%]
tests/render_farm_manager/unit/test_resource_borrowing.py ..             [ 29%]
tests/render_farm_manager/unit/test_resource_borrowing_fixed.py ..       [ 30%]
tests/render_farm_manager/unit/test_resource_partitioner.py ....E.....   [ 33%]
tests/scientific_computing/dependency_tracking/test_dependency_tracker.py . [ 33%]
................F.F...F.F.FFFFFF                                         [ 42%]
tests/scientific_computing/failure_resilience/test_checkpoint_manager.py . [ 42%]
......x..x.                                                              [ 45%]
tests/scientific_computing/failure_resilience/test_failure_detector.py . [ 46%]
.F........F........                                                      [ 51%]
tests/scientific_computing/failure_resilience/test_failure_resilience.py . [ 51%]
FE.FE.................x                                                  [ 58%]
tests/scientific_computing/failure_resilience/test_resilience_coordinator.py . [ 58%]
............Xx                                                           [ 62%]
tests/scientific_computing/job_management/test_long_running_job_manager.py F [ 62%]
.FFFFFFFFFF.F.....                                                       [ 67%]
tests/scientific_computing/resource_forecasting/test_data_collector.py . [ 67%]
.......                                                                  [ 69%]
tests/scientific_computing/resource_forecasting/test_forecaster.py ..... [ 71%]
..                                                                       [ 71%]
tests/scientific_computing/resource_forecasting/test_optimizer.py ...... [ 73%]
..                                                                       [ 74%]
tests/scientific_computing/resource_forecasting/test_reporter.py ......  [ 75%]
tests/scientific_computing/scenario_management/test_advanced_priority_management.py . [ 75%]
................                                                         [ 80%]
tests/scientific_computing/scenario_management/test_comparator.py ...... [ 82%]
...                                                                      [ 82%]
tests/scientific_computing/scenario_management/test_evaluator.py ....... [ 84%]
.                                                                        [ 85%]
tests/scientific_computing/scenario_management/test_priority_manager.py . [ 85%]
..........                                                               [ 88%]
tests/scientific_computing/scenario_management/test_scenario_management_integration.py . [ 88%]
...                                                                      [ 89%]
tests/scientific_computing/scenario_management/test_scenario_model.py .. [ 89%]
...............                                                          [ 94%]
tests/scientific_computing/test_full_system_integration.py ....          [ 95%]
tests/scientific_computing/test_priority_manager.py .............        [ 98%]
tests/scientific_computing/test_scenario_management_integration.py ....  [100%]

==================================== ERRORS ====================================
_______________ ERROR at setup of test_calculate_resource_usage ________________

    @pytest.fixture
    def jobs():
        """Create a list of test render jobs for different clients."""
        now = datetime.now()
    
>       return [
            # Premium client jobs
            RenderJob(
                id=f"premium-job-{i}",
                name=f"Premium Job {i}",
                client_id="premium-client",
                status=RenderJobStatus.PENDING if i % 3 != 0 else RenderJobStatus.RUNNING,
                job_type="animation" if i % 2 == 0 else "vfx",
                priority="high" if i % 2 == 0 else "medium",
                submission_time=now - timedelta(hours=i),
                deadline=now + timedelta(hours=24 - i),
                estimated_duration_hours=8,
                progress=0.0 if i % 3 != 0 else 50.0,
                requires_gpu=i % 2 == 0,
                memory_requirements_gb=32,
                cpu_requirements=16,
                scene_complexity=7,
                dependencies=[],
                assigned_node_id=f"node-{i}" if i % 3 == 0 else None,
                output_path=f"/renders/premium-job-{i}/",
                error_count=0,
            )
            for i in range(5)
        ] + [
            # Standard client jobs
            RenderJob(
                id=f"standard-job-{i}",
                name=f"Standard Job {i}",
                client_id="standard-client",
                status=RenderJobStatus.PENDING if i % 4 != 0 else RenderJobStatus.RUNNING,
                job_type="lighting" if i % 2 == 0 else "compositing",
                priority="medium" if i % 2 == 0 else "low",
                submission_time=now - timedelta(hours=i),
                deadline=now + timedelta(hours=48 - i),
                estimated_duration_hours=6,
                progress=0.0 if i % 4 != 0 else 30.0,
                requires_gpu=i % 3 == 0,
                memory_requirements_gb=16,
                cpu_requirements=8,
                scene_complexity=5,
                dependencies=[],
                assigned_node_id=f"node-{i+5}" if i % 4 == 0 else None,
                output_path=f"/renders/standard-job-{i}/",
                error_count=0,
            )
            for i in range(4)
        ] + [
            # Basic client jobs
            RenderJob(
                id=f"basic-job-{i}",
                name=f"Basic Job {i}",
                client_id="basic-client",
                status=RenderJobStatus.PENDING if i % 3 != 0 else RenderJobStatus.RUNNING,
                job_type="simulation" if i % 2 == 0 else "texture_baking",
                priority="low",
                submission_time=now - timedelta(hours=i),
                deadline=now + timedelta(hours=72 - i),
                estimated_duration_hours=4,
                progress=0.0 if i % 3 != 0 else 25.0,
                requires_gpu=i % 2 == 0,
                memory_requirements_gb=8,
                cpu_requirements=4,
                scene_complexity=3,
                dependencies=[],
                assigned_node_id=f"node-{i+10}" if i % 3 == 0 else None,
                output_path=f"/renders/basic-job-{i}/",
                error_count=0,
            )
            for i in range(3)
        ]

tests/render_farm_manager/unit/test_resource_partitioner.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/render_farm_manager/unit/test_resource_partitioner.py:102: in <listcomp>
    RenderJob(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = RenderJob()
data = {'assigned_node_id': 'node-0', 'client_id': 'premium-client', 'cpu_requirements': 16, 'deadline': datetime.datetime(2025, 6, 17, 4, 16, 45, 327773), ...}
status_map = {<RenderJobStatus.CANCELLED: 'cancelled'>: <TaskStatus.CANCELLED: 'cancelled'>, <RenderJobStatus.COMPLETED: 'completed...ILED: 'failed'>: <TaskStatus.FAILED: 'failed'>, <RenderJobStatus.PAUSED: 'paused'>: <TaskStatus.PAUSED: 'paused'>, ...}
resource_requirements = {'cpu': 16, 'gpu': 1, 'memory': 32}

    def __init__(self, **data):
        # Convert render-specific fields to base task format
        if 'estimated_duration_hours' in data:
            data['estimated_duration'] = timedelta(hours=data.pop('estimated_duration_hours'))
        if 'priority' in data and isinstance(data['priority'], JobPriority):
            priority_map = {
                JobPriority.LOW: Priority.LOW,
                JobPriority.MEDIUM: Priority.MEDIUM,
                JobPriority.HIGH: Priority.HIGH,
                JobPriority.CRITICAL: Priority.CRITICAL
            }
            data['priority'] = priority_map[data['priority']]
        if 'status' in data and isinstance(data['status'], RenderJobStatus):
            status_map = {
                RenderJobStatus.PENDING: TaskStatus.PENDING,
                RenderJobStatus.QUEUED: TaskStatus.QUEUED,
                RenderJobStatus.RUNNING: TaskStatus.RUNNING,
                RenderJobStatus.PAUSED: TaskStatus.PAUSED,
                RenderJobStatus.COMPLETED: TaskStatus.COMPLETED,
                RenderJobStatus.FAILED: TaskStatus.FAILED,
                RenderJobStatus.CANCELLED: TaskStatus.CANCELLED
            }
            data['status'] = status_map[data['status']]
    
        # Store render-specific requirements in metadata
        resource_requirements = {}
        if 'cpu_requirements' in data:
            resource_requirements['cpu'] = data['cpu_requirements']
        if 'memory_requirements_gb' in data:
            resource_requirements['memory'] = data['memory_requirements_gb']
        if 'requires_gpu' in data and data['requires_gpu']:
            resource_requirements['gpu'] = 1
    
        if 'metadata' not in data:
            data['metadata'] = {}
        data['metadata']['resource_requirements'] = resource_requirements
        data['metadata']['deadline'] = data['deadline'].isoformat() if 'deadline' in data else None
    
>       super().__init__(**data)
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for RenderJob
E       priority
E         Input should be 1, 2, 3 or 4 [type=enum, input_value='high', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/enum

render_farm_manager/core/models.py:192: ValidationError
______ ERROR at setup of TestFailureDetector.test_check_simulation_health ______

self = <tests.scientific_computing.failure_resilience.test_failure_resilience.TestFailureDetector object at 0x7f49517d76d0>

    @pytest.fixture
    def mock_simulation(self):
        """Create a mock simulation for testing."""
        sim = MockSimulation(
            id="sim-123",
            name="Test Simulation",
            description="A test simulation",
            status=SimulationStatus.RUNNING,
            priority=SimulationPriority.HIGH,
            stages={}
        )
    
        # Add a stage
        stage = SimulationStage(
            id="stage-1",
            name="Processing Stage",
            status=SimulationStageStatus.RUNNING,
            progress=0.5,
            estimated_duration=timedelta(hours=2),
        )
        sim.stages = {"stage-1": stage}
    
        # Mock the start time
>       stage.start_time = datetime.now() - timedelta(minutes=30)  # Started 30 minutes ago

tests/scientific_computing/failure_resilience/test_failure_resilience.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:995: in __setattr__
    elif (setattr_handler := self._setattr_handler(name, value)) is not None:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = SimulationStage(id='stage-1', name='Processing Stage', status=<TaskStatus.RUNNING: 'running'>, priority=<Priority.MEDI...kpoint_frequency=datetime.timedelta(seconds=3600), last_checkpoint_time=None, checkpoint_path=None, error_message=None)
name = 'start_time', value = datetime.datetime(2025, 6, 16, 3, 46, 45, 861857)

    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:
        """Get a handler for setting an attribute on the model instance.
    
        Returns:
            A handler for setting an attribute on the model instance. Used for memoization of the handler.
            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`
            Returns `None` when memoization is not safe, then the attribute is set directly.
        """
        cls = self.__class__
        if name in cls.__class_vars__:
            raise AttributeError(
                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '
                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'
            )
        elif not _fields.is_valid_field_name(name):
            if (attribute := cls.__private_attributes__.get(name)) is not None:
                if hasattr(attribute, '__set__'):
                    return lambda model, _name, val: attribute.__set__(model, val)
                else:
                    return _SIMPLE_SETATTR_HANDLERS['private']
            else:
                _object_setattr(self, name, value)
                return None  # Can not return memoized handler with possibly freeform attr names
    
        attr = getattr(cls, name, None)
        # NOTE: We currently special case properties and `cached_property`, but we might need
        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors
        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value
        # to the instance's `__dict__`, but other non-data descriptors might do things differently.
        if isinstance(attr, cached_property):
            return _SIMPLE_SETATTR_HANDLERS['cached_property']
    
        _check_frozen(cls, name, value)
    
        # We allow properties to be set only on non frozen models for now (to match dataclasses).
        # This can be changed if it ever gets requested.
        if isinstance(attr, property):
            return lambda model, _name, val: attr.__set__(model, val)
        elif cls.model_config.get('validate_assignment'):
            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']
        elif name not in cls.__pydantic_fields__:
            if cls.model_config.get('extra') != 'allow':
                # TODO - matching error
>               raise ValueError(f'"{cls.__name__}" object has no field "{name}"')
E               ValueError: "SimulationStage" object has no field "start_time"

../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:1042: ValueError
____ ERROR at setup of TestFailureDetector.test_detect_simulation_failures _____

self = <tests.scientific_computing.failure_resilience.test_failure_resilience.TestFailureDetector object at 0x7f49517d7760>

    @pytest.fixture
    def mock_simulation(self):
        """Create a mock simulation for testing."""
        sim = MockSimulation(
            id="sim-123",
            name="Test Simulation",
            description="A test simulation",
            status=SimulationStatus.RUNNING,
            priority=SimulationPriority.HIGH,
            stages={}
        )
    
        # Add a stage
        stage = SimulationStage(
            id="stage-1",
            name="Processing Stage",
            status=SimulationStageStatus.RUNNING,
            progress=0.5,
            estimated_duration=timedelta(hours=2),
        )
        sim.stages = {"stage-1": stage}
    
        # Mock the start time
>       stage.start_time = datetime.now() - timedelta(minutes=30)  # Started 30 minutes ago

tests/scientific_computing/failure_resilience/test_failure_resilience.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:995: in __setattr__
    elif (setattr_handler := self._setattr_handler(name, value)) is not None:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = SimulationStage(id='stage-1', name='Processing Stage', status=<TaskStatus.RUNNING: 'running'>, priority=<Priority.MEDI...kpoint_frequency=datetime.timedelta(seconds=3600), last_checkpoint_time=None, checkpoint_path=None, error_message=None)
name = 'start_time', value = datetime.datetime(2025, 6, 16, 3, 46, 45, 909794)

    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:
        """Get a handler for setting an attribute on the model instance.
    
        Returns:
            A handler for setting an attribute on the model instance. Used for memoization of the handler.
            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`
            Returns `None` when memoization is not safe, then the attribute is set directly.
        """
        cls = self.__class__
        if name in cls.__class_vars__:
            raise AttributeError(
                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '
                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'
            )
        elif not _fields.is_valid_field_name(name):
            if (attribute := cls.__private_attributes__.get(name)) is not None:
                if hasattr(attribute, '__set__'):
                    return lambda model, _name, val: attribute.__set__(model, val)
                else:
                    return _SIMPLE_SETATTR_HANDLERS['private']
            else:
                _object_setattr(self, name, value)
                return None  # Can not return memoized handler with possibly freeform attr names
    
        attr = getattr(cls, name, None)
        # NOTE: We currently special case properties and `cached_property`, but we might need
        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors
        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value
        # to the instance's `__dict__`, but other non-data descriptors might do things differently.
        if isinstance(attr, cached_property):
            return _SIMPLE_SETATTR_HANDLERS['cached_property']
    
        _check_frozen(cls, name, value)
    
        # We allow properties to be set only on non frozen models for now (to match dataclasses).
        # This can be changed if it ever gets requested.
        if isinstance(attr, property):
            return lambda model, _name, val: attr.__set__(model, val)
        elif cls.model_config.get('validate_assignment'):
            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']
        elif name not in cls.__pydantic_fields__:
            if cls.model_config.get('extra') != 'allow':
                # TODO - matching error
>               raise ValueError(f'"{cls.__name__}" object has no field "{name}"')
E               ValueError: "SimulationStage" object has no field "start_time"

../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:1042: ValueError
=================================== FAILURES ===================================
____________________ test_error_recovery_checkpoint_simple _____________________

farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489e19a1a0>
client = RenderClient(client_id='client1', name='Test Client', service_tier=<ServiceTier.PREMIUM: 'premium'>, guaranteed_resources=0, max_resources=100)
render_nodes = [RenderNode(id='gpu1', name='GPU Node 1', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=16, memory_gb=64.0, gpu_coun...dering']), power_efficiency_rating=65.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=0)]
checkpointable_job = RenderJob(id='job1', name='Test Checkpoint Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, ...=False, supports_checkpoint=True, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)

    def test_error_recovery_checkpoint_simple(farm_manager, client, render_nodes, checkpointable_job):
        """Simple test to ensure node failure handling works with checkpoints."""
        # Setup
        farm_manager.add_client(client)
        for node in render_nodes:
            farm_manager.add_node(node)
        farm_manager.submit_job(checkpointable_job)
    
        # Run scheduling
        farm_manager.run_scheduling_cycle()
    
        # Check job is assigned
        job = farm_manager.jobs[checkpointable_job.id]
>       assert job.status == RenderJobStatus.RUNNING
E       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>
E        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job1', name='Test Checkpoint Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, ...=False, supports_checkpoint=True, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status
E        +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING

tests/render_farm_manager/integration/test_error_recovery_fixed.py:130: AssertionError
_____________________ test_error_count_threshold_handling ______________________

    def test_error_count_threshold_handling():
        """Test that jobs exceeding error threshold are handled correctly."""
        # Create a fresh render farm manager
        farm_manager = RenderFarmManager()
    
        # Add a client
        client = Client(
            id="client1",
            name="Test Client",
            sla_tier="premium",
            guaranteed_resources=50,
            max_resources=80,
        )
        farm_manager.add_client(client)
    
        # Add a node
        node = RenderNode(
            id="node1",
            name="Node 1",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["gpu_rendering"],
            ),
            power_efficiency_rating=75.0,
        )
        farm_manager.add_node(node)
    
        # Create a job
        now = datetime.now()
        job = RenderJob(
            id="job1",
            client_id="client1",
            name="Unstable Job",
            status=RenderJobStatus.PENDING,
            job_type="animation",
            priority=JobPriority.HIGH,
            submission_time=now,
            deadline=now + timedelta(hours=5),
            estimated_duration_hours=2.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            dependencies=[],
            assigned_node_id=None,
            output_path="/renders/client1/job1/",
            error_count=0,
            can_be_preempted=True,
            supports_progressive_output=False,
            supports_checkpoint=True,
            last_checkpoint_time=None,
            last_progressive_output_time=None,
            energy_intensive=False,
        )
    
        # Submit the job
        farm_manager.submit_job(job)
    
        # Manually set the error count to 3 (threshold)
        job = farm_manager.jobs["job1"]
        job.error_count = 3
    
        # Simulate a node failure that should trigger the threshold
        farm_manager.run_scheduling_cycle()
>       assert job.status == RenderJobStatus.RUNNING
E       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>
E        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job1', name='Unstable Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, submiss...=False, supports_checkpoint=True, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status
E        +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING

tests/render_farm_manager/integration/test_error_recovery_fixed_complete.py:224: AssertionError
_________________ test_fault_tolerance_multiple_node_failures __________________

farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489dfe7d60>
clients = [RenderClient(client_id='client1', name='Premium Client', service_tier=<ServiceTier.PREMIUM: 'premium'>, guaranteed_re...='client3', name='Basic Client', service_tier=<ServiceTier.BASIC: 'basic'>, guaranteed_resources=10, max_resources=40)]
render_nodes = [RenderNode(id='gpu1', name='GPU Node 1', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=16, memory_gb=64.0, gpu_coun...'fluid']), power_efficiency_rating=78.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=0)]
render_jobs = [RenderJob(id='job1', name='High Priority Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>,...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]

    def test_fault_tolerance_multiple_node_failures(farm_manager, clients, render_nodes, render_jobs):
        """Test that the farm manager can handle multiple simultaneous node failures."""
        # Setup: Add clients, nodes and jobs
        for client in clients:
            farm_manager.add_client(client)
    
        for node in render_nodes:
            farm_manager.add_node(node)
    
        for job in render_jobs:
            farm_manager.submit_job(job)
    
        # First scheduling cycle - assigns jobs to nodes
        farm_manager.run_scheduling_cycle()
    
        # Verify jobs are assigned to nodes
        running_jobs = [job for job in farm_manager.jobs.values()
                       if job.status == RenderJobStatus.RUNNING]
>       assert len(running_jobs) > 0
E       assert 0 > 0
E        +  where 0 = len([])

tests/render_farm_manager/integration/test_fault_tolerance.py:285: AssertionError
_________________ test_fault_tolerance_multiple_node_failures __________________

farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489dd76e60>
clients = [Client(id='client1', name='Premium Client', sla_tier='premium', guaranteed_resources=50, max_resources=80), Client(id..._resources=60), Client(id='client3', name='Basic Client', sla_tier='basic', guaranteed_resources=10, max_resources=40)]
render_nodes = [RenderNode(id='gpu1', name='GPU Node 1', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=16, memory_gb=64.0, gpu_coun...'fluid']), power_efficiency_rating=78.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=0)]
render_jobs = [RenderJob(id='job1', name='High Priority Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>,...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]

    def test_fault_tolerance_multiple_node_failures(farm_manager, clients, render_nodes, render_jobs):
        """Test that the farm manager can handle multiple simultaneous node failures."""
        # Setup: Add clients, nodes and jobs
        for client in clients:
            farm_manager.add_client(client)
    
        for node in render_nodes:
            farm_manager.add_node(node)
    
        for job in render_jobs:
            farm_manager.submit_job(job)
    
        # First scheduling cycle - assigns jobs to nodes
        farm_manager.run_scheduling_cycle()
    
        # Verify jobs are assigned to nodes
        running_jobs = [job for job in farm_manager.jobs.values()
                       if job.status == RenderJobStatus.RUNNING]
>       assert len(running_jobs) > 0
E       assert 0 > 0
E        +  where 0 = len([])

tests/render_farm_manager/integration/test_fault_tolerance_fixed.py:277: AssertionError
______________________ test_circular_dependency_detection ______________________

farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489dd89cc0>

    def test_circular_dependency_detection(farm_manager):
        """Test that circular dependencies are detected and handled appropriately."""
        # Add a client using standard Client model
        client = Client(
            id="client1",
            name="Test Client",
            sla_tier="premium",
            guaranteed_resources=50,
            max_resources=80,
        )
        farm_manager.add_client(client)
    
        # Add a node
        node = RenderNode(
            id="node1",
            name="Node 1",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["gpu_rendering"],
            ),
            power_efficiency_rating=85.0,
        )
        farm_manager.add_node(node)
    
        now = datetime.now()
    
        # Create jobs with circular dependencies
        job_a = RenderJob(
            id="job_a",
            client_id="client1",
            name="Job A",
            status=RenderJobStatus.PENDING,
            job_type="animation",
            priority=JobPriority.HIGH,
            submission_time=now,
            deadline=now + timedelta(hours=4),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            output_path="/renders/client1/job_a/",
            dependencies=["job_c"],  # A depends on C
        )
    
        job_b = RenderJob(
            id="job_b",
            client_id="client1",
            name="Job B",
            status=RenderJobStatus.PENDING,
            job_type="vfx",
            priority=JobPriority.HIGH,
            submission_time=now,
            deadline=now + timedelta(hours=5),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            output_path="/renders/client1/job_b/",
            dependencies=["job_a"],  # B depends on A
        )
    
        job_c = RenderJob(
            id="job_c",
            client_id="client1",
            name="Job C",
            status=RenderJobStatus.PENDING,
            job_type="composition",
            priority=JobPriority.MEDIUM,
            submission_time=now,
            deadline=now + timedelta(hours=6),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            output_path="/renders/client1/job_c/",
            dependencies=["job_b"],  # C depends on B, creating a cycle: A -> C -> B -> A
        )
    
        # Submit jobs in sequence to test cycle detection
        farm_manager.submit_job(job_a)  # This should be fine
>       assert farm_manager.jobs["job_a"].status == RenderJobStatus.PENDING
E       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.PENDING: 'pending'>
E        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job_a', name='Job A', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, submission_ti...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status
E        +  and   <RenderJobStatus.PENDING: 'pending'> = RenderJobStatus.PENDING

tests/render_farm_manager/integration/test_job_dependencies_fixed.py:275: AssertionError
___________________ test_dependent_job_priority_inheritance ____________________

    def test_dependent_job_priority_inheritance():
        """Test that dependent jobs inherit priority from their parent jobs."""
        # Create fresh instances for the test
        farm_manager = RenderFarmManager()
    
        # Add a client
        client = Client(
            id="client1",
            name="Test Client",
            sla_tier="premium",
            guaranteed_resources=50,
            max_resources=80,
        )
        farm_manager.add_client(client)
    
        # Add nodes
        node1 = RenderNode(
            id="node1",
            name="Node 1",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["gpu_rendering"],
            ),
            power_efficiency_rating=85.0,
        )
        farm_manager.add_node(node1)
    
        now = datetime.now()
    
        # Create parent job with high priority
        job_parent = RenderJob(
            id="job_parent",
            client_id="client1",
            name="Parent Job",
            status=RenderJobStatus.PENDING,
            job_type="animation",
            priority=JobPriority.HIGH,
            submission_time=now,
            deadline=now + timedelta(hours=3),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            dependencies=[],
            output_path="/renders/client1/job_parent/",
        )
    
        # Create child job with lower priority
        job_child = RenderJob(
            id="job_child",
            client_id="client1",
            name="Child Job",
            status=RenderJobStatus.PENDING,
            job_type="animation",
            priority=JobPriority.LOW,  # Lower priority than parent
            submission_time=now,
            deadline=now + timedelta(hours=4),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            dependencies=["job_parent"],
            output_path="/renders/client1/job_child/",
        )
    
        # Create unrelated job with medium priority
        job_unrelated = RenderJob(
            id="job_unrelated",
            client_id="client1",
            name="Unrelated Job",
            status=RenderJobStatus.PENDING,
            job_type="animation",
            priority=JobPriority.MEDIUM,
            submission_time=now,
            deadline=now + timedelta(hours=5),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            dependencies=[],
            output_path="/renders/client1/job_unrelated/",
        )
    
        # Submit all jobs
        farm_manager.submit_job(job_parent)
        farm_manager.submit_job(job_child)
        farm_manager.submit_job(job_unrelated)
    
        # Run scheduling cycle - parent job should be scheduled first
        farm_manager.run_scheduling_cycle()
>       assert farm_manager.jobs["job_parent"].status == RenderJobStatus.RUNNING
E       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>
E        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job_parent', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, sub...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status
E        +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING

tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py:271: AssertionError
______________________ test_circular_dependency_detection ______________________

    def test_circular_dependency_detection():
        """Test that circular dependencies are detected and handled appropriately."""
        # Create fresh instances for the test
        farm_manager = RenderFarmManager()
    
        # Add a client
        client = Client(
            id="client1",
            name="Test Client",
            sla_tier="premium",
            guaranteed_resources=50,
            max_resources=80,
        )
        farm_manager.add_client(client)
    
        # Add a node
        node = RenderNode(
            id="node1",
            name="Node 1",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["gpu_rendering"],
            ),
            power_efficiency_rating=85.0,
        )
        farm_manager.add_node(node)
    
        now = datetime.now()
    
        # Create jobs with circular dependencies
        job_a = RenderJob(
            id="job_a",
            client_id="client1",
            name="Job A",
            status=RenderJobStatus.PENDING,
            job_type="animation",
            priority=JobPriority.HIGH,
            submission_time=now,
            deadline=now + timedelta(hours=4),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            output_path="/renders/client1/job_a/",
            dependencies=["job_c"],  # A depends on C
        )
    
        job_b = RenderJob(
            id="job_b",
            client_id="client1",
            name="Job B",
            status=RenderJobStatus.PENDING,
            job_type="vfx",
            priority=JobPriority.HIGH,
            submission_time=now,
            deadline=now + timedelta(hours=5),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            output_path="/renders/client1/job_b/",
            dependencies=["job_a"],  # B depends on A
        )
    
        job_c = RenderJob(
            id="job_c",
            client_id="client1",
            name="Job C",
            status=RenderJobStatus.PENDING,
            job_type="composition",
            priority=JobPriority.MEDIUM,
            submission_time=now,
            deadline=now + timedelta(hours=6),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            output_path="/renders/client1/job_c/",
            dependencies=["job_b"],  # C depends on B, creating a cycle: A -> C -> B -> A
        )
    
        # Submit jobs in sequence to test cycle detection
        farm_manager.submit_job(job_a)  # This should be fine
>       assert farm_manager.jobs["job_a"].status == RenderJobStatus.PENDING
E       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.PENDING: 'pending'>
E        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job_a', name='Job A', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, submission_ti...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status
E        +  and   <RenderJobStatus.PENDING: 'pending'> = RenderJobStatus.PENDING

tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py:400: AssertionError
____________________________ test_simple_dependency ____________________________

    def test_simple_dependency():
        """Test that a job with dependencies is only scheduled after dependencies complete."""
        # Create a fresh farm manager
        farm_manager = RenderFarmManager()
    
        # Add a client
        client = Client(
            id="client1",
            name="Test Client",
            sla_tier="premium",
            guaranteed_resources=50,
            max_resources=80,
        )
        farm_manager.add_client(client)
    
        # Add nodes
        node1 = RenderNode(
            id="node1",
            name="Node 1",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["gpu_rendering"],
            ),
            power_efficiency_rating=85.0,
        )
        node2 = RenderNode(
            id="node2",
            name="Node 2",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["gpu_rendering"],
            ),
            power_efficiency_rating=85.0,
        )
        farm_manager.add_node(node1)
        farm_manager.add_node(node2)
    
        now = datetime.now()
    
        # STEP 1: Submit a parent job
        job_parent = RenderJob(
            id="job_parent",
            client_id="client1",
            name="Parent Job",
            status=RenderJobStatus.PENDING,
            job_type="animation",
            priority=JobPriority.HIGH,
            submission_time=now,
            deadline=now + timedelta(hours=5),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            dependencies=[],  # No dependencies
            output_path="/renders/client1/job_parent/",
        )
        farm_manager.submit_job(job_parent)
    
        # STEP 2: Run scheduling cycle - parent job should be scheduled
        farm_manager.run_scheduling_cycle()
>       assert farm_manager.jobs["job_parent"].status == RenderJobStatus.RUNNING
E       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>
E        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='job_parent', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, sub...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status
E        +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING

tests/render_farm_manager/integration/test_job_dependencies_fixed_direct_complete.py:93: AssertionError
________________________ test_job_dependency_scheduling ________________________

farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f49f44e39d0>
client = RenderClient(client_id='client1', name='Test Client', service_tier=<ServiceTier.PREMIUM: 'premium'>, guaranteed_resources=50, max_resources=80)
render_nodes = [RenderNode(id='fixed_node1', name='Fixed Node 1', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=16, memory_gb=64.0,...dering']), power_efficiency_rating=72.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=0)]
dependent_jobs = [RenderJob(id='fixed_parent1', name='Parent Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]

    def test_job_dependency_scheduling(farm_manager, client, render_nodes, dependent_jobs):
        """Test that jobs with dependencies are scheduled correctly."""
        # Setup: Add client and nodes
        farm_manager.add_client(client)
    
        for node in render_nodes:
            farm_manager.add_node(node)
    
        # Submit all jobs
        for job in dependent_jobs:
            farm_manager.submit_job(job)
    
        # First scheduling cycle - only runs one job at a time
        farm_manager.run_scheduling_cycle()
    
        # Check that only parent jobs and independent job are running or queued
        # Child jobs should be pending until dependencies complete
        parent1 = farm_manager.jobs["fixed_parent1"]
        parent2 = farm_manager.jobs["fixed_parent2"]
        child = farm_manager.jobs["fixed_child1"]
        grandchild = farm_manager.jobs["fixed_grandchild1"]
        independent = farm_manager.jobs["fixed_independent1"]
    
        # The system might only schedule one job at a time
        # Let's just check all jobs have the right status
>       assert child.status == RenderJobStatus.PENDING
E       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.PENDING: 'pending'>
E        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='fixed_child1', name='Child Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.MEDIUM: 2>, ...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status
E        +  and   <RenderJobStatus.PENDING: 'pending'> = RenderJobStatus.PENDING

tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py:239: AssertionError
___________________ test_dependent_job_priority_inheritance ____________________

farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489e023d90>
client = RenderClient(client_id='client1', name='Test Client', service_tier=<ServiceTier.PREMIUM: 'premium'>, guaranteed_resources=50, max_resources=80)
render_nodes = [RenderNode(id='fixed_node1', name='Fixed Node 1', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=16, memory_gb=64.0,...dering']), power_efficiency_rating=72.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=0)]

    def test_dependent_job_priority_inheritance(farm_manager, client, render_nodes):
        """Test that dependent jobs inherit priority from parent jobs when appropriate."""
        # Setup: Add client and nodes
        farm_manager.add_client(client)
    
        for node in render_nodes:
            farm_manager.add_node(node)
    
        now = datetime.now()
    
        # Create a high-priority parent job
        parent_job = RenderJob(
            id="fixed_high_parent",
            client_id="client1",
            name="High Priority Parent",
            status=RenderJobStatus.PENDING,
            job_type="animation",
            priority=JobPriority.CRITICAL,  # Very high priority
            submission_time=now,
            deadline=now + timedelta(hours=3),  # Tight deadline
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=7,
            output_path="/renders/client1/fixed_high_parent/",
        )
    
        # Create a low-priority child job
        child_job = RenderJob(
            id="fixed_low_child",
            client_id="client1",
            name="Low Priority Child",
            status=RenderJobStatus.PENDING,
            job_type="composition",
            priority=JobPriority.LOW,  # Low priority
            submission_time=now,
            deadline=now + timedelta(hours=12),  # Loose deadline
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=5,
            output_path="/renders/client1/fixed_low_child/",
            dependencies=["fixed_high_parent"],
        )
    
        # Create competing jobs with medium priority
        competing_jobs = [
            RenderJob(
                id=f"fixed_competing{i}",
                client_id="client1",
                name=f"Competing Job {i}",
                status=RenderJobStatus.PENDING,
                job_type="standalone",
                priority=JobPriority.MEDIUM,  # Medium priority
                submission_time=now,
                deadline=now + timedelta(hours=6),
                estimated_duration_hours=1.0,
                progress=0.0,
                requires_gpu=True,
                memory_requirements_gb=32,
                cpu_requirements=8,
                scene_complexity=6,
                output_path=f"/renders/client1/fixed_competing{i}/",
            )
            for i in range(1, 4)
        ]
    
        # Submit all jobs
        farm_manager.submit_job(parent_job)
        farm_manager.submit_job(child_job)
        for job in competing_jobs:
            farm_manager.submit_job(job)
    
        # First scheduling cycle - might run only one job
        farm_manager.run_scheduling_cycle()
    
        # Mark parent job as completed directly
        parent = farm_manager.jobs["fixed_high_parent"]
        parent.status = RenderJobStatus.COMPLETED
        parent.progress = 100.0
    
        # Run scheduling cycle to handle the completed parent
        farm_manager.run_scheduling_cycle()
    
        # The child job should now have a chance to be scheduled
        child = farm_manager.jobs["fixed_low_child"]
    
        # Since we can't guarantee which job will be scheduled due to multiple factors,
        # let's just ensure that both the child job and competing jobs have correct statuses:
        # either RUNNING, QUEUED, or PENDING
>       assert child.status in [RenderJobStatus.RUNNING, RenderJobStatus.QUEUED, RenderJobStatus.PENDING]
E       AssertionError: assert <TaskStatus.PENDING: 'pending'> in [<RenderJobStatus.RUNNING: 'running'>, <RenderJobStatus.QUEUED: 'queued'>, <RenderJobStatus.PENDING: 'pending'>]
E        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='fixed_low_child', name='Low Priority Child', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority....False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status

tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py:389: AssertionError
____________________ test_job_dependency_scheduling_patched ____________________

    def test_job_dependency_scheduling_patched():
        """Test job dependency scheduling with parent2 explicitly forced to RUNNING."""
        # Create a fresh farm manager
        farm_manager = RenderFarmManager()
    
        # Add a client
        client = Client(
            id="client1",
            name="Test Client",
            sla_tier="premium",
            guaranteed_resources=50,
            max_resources=80,
        )
        farm_manager.add_client(client)
    
        # Add nodes
        node1 = RenderNode(
            id="node1",
            name="Node 1",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["rendering"],
            ),
            power_efficiency_rating=75.0,
        )
        node2 = RenderNode(
            id="node2",
            name="Node 2",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["rendering"],
            ),
            power_efficiency_rating=72.0,
        )
        farm_manager.add_node(node1)
        farm_manager.add_node(node2)
    
        now = datetime.now()
    
        # Create parent jobs first
        parent_job1 = RenderJob(
            id="parent1",
            client_id="client1",
            name="Parent Job 1",
            status=RenderJobStatus.PENDING,
            job_type="animation",
            priority=JobPriority.HIGH,
            submission_time=now,
            deadline=now + timedelta(hours=4),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            output_path="/renders/client1/parent1/",
        )
    
        parent_job2 = RenderJob(
            id="parent2",
            client_id="client1",
            name="Parent Job 2",
            status=RenderJobStatus.PENDING,
            job_type="vfx",
            priority=JobPriority.HIGH,
            submission_time=now,
            deadline=now + timedelta(hours=4),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=5,
            output_path="/renders/client1/parent2/",
        )
    
        # Create child job that depends on both parents
        child_job = RenderJob(
            id="child1",
            client_id="client1",
            name="Child Job",
            status=RenderJobStatus.PENDING,
            job_type="composition",
            priority=JobPriority.MEDIUM,
            submission_time=now,
            deadline=now + timedelta(hours=8),
            estimated_duration_hours=2.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=64,
            cpu_requirements=16,
            scene_complexity=7,
            output_path="/renders/client1/child1/",
            dependencies=["parent1", "parent2"],
        )
    
        # Create a grandchild job that depends on the child
        grandchild_job = RenderJob(
            id="grandchild1",
            client_id="client1",
            name="Grandchild Job",
            status=RenderJobStatus.PENDING,
            job_type="final_output",
            priority=JobPriority.MEDIUM,
            submission_time=now,
            deadline=now + timedelta(hours=12),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=4,
            output_path="/renders/client1/grandchild1/",
            dependencies=["child1"],
        )
    
        # Create a job with no dependencies for comparison
        independent_job = RenderJob(
            id="independent1",
            client_id="client1",
            name="Independent Job",
            status=RenderJobStatus.PENDING,
            job_type="standalone",
            priority=JobPriority.MEDIUM,
            submission_time=now,
            deadline=now + timedelta(hours=6),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=5,
            output_path="/renders/client1/independent1/",
        )
    
        # Submit all jobs
        farm_manager.submit_job(parent_job1)
        farm_manager.submit_job(parent_job2)
        farm_manager.submit_job(child_job)
        farm_manager.submit_job(grandchild_job)
        farm_manager.submit_job(independent_job)
    
        # Set up parent1 and parent2 manually to ensure they're running
        parent1 = farm_manager.jobs["parent1"]
        parent2 = farm_manager.jobs["parent2"]
        child = farm_manager.jobs["child1"]
        grandchild = farm_manager.jobs["grandchild1"]
        independent = farm_manager.jobs["independent1"]
    
        # Manually set parent1 to RUNNING
        parent1.status = RenderJobStatus.RUNNING
        parent1.assigned_node_id = node1.id
        node1.current_job_id = parent1.id
    
        # Manually set parent2 to RUNNING
        parent2.status = RenderJobStatus.RUNNING
        parent2.assigned_node_id = node2.id
        node2.current_job_id = parent2.id
    
        # Parents and independent job should be scheduled
        assert parent1.status in [RenderJobStatus.RUNNING, RenderJobStatus.QUEUED]
        assert parent2.status in [RenderJobStatus.RUNNING, RenderJobStatus.QUEUED]
    
        # Child and grandchild should be pending due to dependencies
>       assert child.status == RenderJobStatus.PENDING
E       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.PENDING: 'pending'>
E        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='child1', name='Child Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.MEDIUM: 2>, submis...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status
E        +  and   <RenderJobStatus.PENDING: 'pending'> = RenderJobStatus.PENDING

tests/render_farm_manager/integration/test_job_dependencies_scheduling_patch.py:195: AssertionError
____________________________ test_simple_dependency ____________________________

    def test_simple_dependency():
        """Test that a job with dependencies is only scheduled after dependencies complete."""
        # Create a fresh farm manager
        farm_manager = RenderFarmManager()
    
        # Use specialized test job IDs that won't conflict with other tests
        fixed_parent_job_id = "fixed_unique_parent_job_id"
        fixed_child_job_id = "fixed_unique_child_job_id"
    
        # Add a client
        client = Client(
            id="client1",
            name="Test Client",
            sla_tier="premium",
            guaranteed_resources=50,
            max_resources=80,
        )
        farm_manager.add_client(client)
    
        # Add nodes
        node1 = RenderNode(
            id="node1",
            name="Node 1",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["gpu_rendering"],
            ),
            power_efficiency_rating=85.0,
        )
        node2 = RenderNode(
            id="node2",
            name="Node 2",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["gpu_rendering"],
            ),
            power_efficiency_rating=85.0,
        )
        farm_manager.add_node(node1)
        farm_manager.add_node(node2)
    
        now = datetime.now()
    
        # STEP 1: Submit a parent job
        job_parent = RenderJob(
            id=fixed_parent_job_id,  # Use a unique ID that won't conflict with other tests
            client_id="client1",
            name="Parent Job",
            status=RenderJobStatus.PENDING,
            job_type="animation",
            priority=JobPriority.HIGH,
            submission_time=now,
            deadline=now + timedelta(hours=5),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            dependencies=[],  # No dependencies
            output_path=f"/renders/client1/{fixed_parent_job_id}/",
        )
        farm_manager.submit_job(job_parent)
    
        # STEP 2: Run scheduling cycle - parent job should be scheduled
        farm_manager.run_scheduling_cycle()
>       assert farm_manager.jobs[fixed_parent_job_id].status == RenderJobStatus.RUNNING
E       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>
E        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='fixed_unique_parent_job_id', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priori...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status
E        +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING

tests/render_farm_manager/integration/test_job_dependencies_simple.py:97: AssertionError
____________________________ test_simple_dependency ____________________________

    def test_simple_dependency():
        """Test that a job with dependencies is only scheduled after dependencies complete."""
        # Create a fresh farm manager
        farm_manager = RenderFarmManager()
    
        # Add a client
        client = Client(
            id="client1",
            name="Test Client",
            sla_tier="premium",
            guaranteed_resources=50,
            max_resources=80,
        )
        farm_manager.add_client(client)
    
        # Add nodes
        node1 = RenderNode(
            id="node1",
            name="Node 1",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["gpu_rendering"],
            ),
            power_efficiency_rating=85.0,
        )
        node2 = RenderNode(
            id="node2",
            name="Node 2",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["gpu_rendering"],
            ),
            power_efficiency_rating=85.0,
        )
        farm_manager.add_node(node1)
        farm_manager.add_node(node2)
    
        now = datetime.now()
    
        # STEP 1: Submit a parent job
        job_parent = RenderJob(
            id="test_parent",  # Use a unique ID that won't conflict with other test cases
            client_id="client1",
            name="Parent Job",
            status=RenderJobStatus.PENDING,
            job_type="animation",
            priority=JobPriority.HIGH,
            submission_time=now,
            deadline=now + timedelta(hours=5),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            dependencies=[],  # No dependencies
            output_path="/renders/client1/test_parent/",
        )
        farm_manager.submit_job(job_parent)
    
        # STEP 2: Run scheduling cycle - parent job should be scheduled
        farm_manager.run_scheduling_cycle()
>       assert farm_manager.jobs["test_parent"].status == RenderJobStatus.RUNNING
E       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>
E        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='test_parent', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, su...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status
E        +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING

tests/render_farm_manager/integration/test_job_dependencies_simple_fixed.py:93: AssertionError
____________________________ test_simple_dependency ____________________________

    def test_simple_dependency():
        """Test that a job with dependencies is only scheduled after dependencies complete."""
        # Create a fresh farm manager
        farm_manager = RenderFarmManager()
    
        # Create a monkeypatch for the run_scheduling_cycle method to prevent scheduling
        # child-job when parent-job has progress < 50%
        original_run_scheduling_cycle = farm_manager.run_scheduling_cycle
    
        def patched_run_scheduling_cycle():
            """Patched version of run_scheduling_cycle for this test."""
            # Check for the special test case
            if "parent-job" in farm_manager.jobs and "child-job" in farm_manager.jobs:
                parent_job = farm_manager.jobs["parent-job"]
                child_job = farm_manager.jobs["child-job"]
    
                # If child depends on parent and parent progress < 50%, skip scheduling child
                if (child_job.status == RenderJobStatus.PENDING and
                    hasattr(child_job, "dependencies") and
                    "parent-job" in child_job.dependencies and
                    hasattr(parent_job, "progress") and
                    parent_job.progress < 50.0):
    
                    # Call original method to schedule other jobs, but temporarily remove child job
                    farm_manager.jobs.pop("child-job")
                    result = original_run_scheduling_cycle()
                    # Put child job back
                    farm_manager.jobs["child-job"] = child_job
                    return result
    
            # For all other cases, use the original method
            return original_run_scheduling_cycle()
    
        # Apply the patch
        farm_manager.run_scheduling_cycle = patched_run_scheduling_cycle
    
        # Add a client
        client = Client(
            id="client1",
            name="Test Client",
            sla_tier="premium",
            guaranteed_resources=50,
            max_resources=80,
        )
        farm_manager.add_client(client)
    
        # Add nodes
        node1 = RenderNode(
            id="node1",
            name="Node 1",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["gpu_rendering"],
            ),
            power_efficiency_rating=85.0,
        )
        node2 = RenderNode(
            id="node2",
            name="Node 2",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["gpu_rendering"],
            ),
            power_efficiency_rating=85.0,
        )
        farm_manager.add_node(node1)
        farm_manager.add_node(node2)
    
        now = datetime.now()
    
        # STEP 1: Submit a parent job
        job_parent = RenderJob(
            id="parent-job",  # Use the test ID expected by the special case
            client_id="client1",
            name="Parent Job",
            status=RenderJobStatus.PENDING,
            job_type="animation",
            priority=JobPriority.HIGH,
            submission_time=now,
            deadline=now + timedelta(hours=5),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            dependencies=[],  # No dependencies
            output_path="/renders/client1/parent-job/",
        )
        farm_manager.submit_job(job_parent)
    
        # STEP 2: Run scheduling cycle - parent job should be scheduled
        farm_manager.run_scheduling_cycle()
>       assert farm_manager.jobs["parent-job"].status == RenderJobStatus.RUNNING
E       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>
E        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='parent-job', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, sub...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status
E        +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING

tests/render_farm_manager/integration/test_job_dependencies_simple_patched.py:125: AssertionError
___________________ test_simple_dependency_with_monkey_patch ___________________

    def test_simple_dependency_with_monkey_patch():
        """Test job dependencies using monkey patching for validation."""
    
        # Create a fresh farm manager
        farm_manager = RenderFarmManager()
    
        # Add a client
        client = Client(
            id="client1",
            name="Test Client",
            sla_tier="premium",
            guaranteed_resources=50,
            max_resources=80,
        )
        farm_manager.add_client(client)
    
        # Add nodes
        node1 = RenderNode(
            id="node1",
            name="Node 1",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["gpu_rendering"],
            ),
            power_efficiency_rating=85.0,
        )
        node2 = RenderNode(
            id="node2",
            name="Node 2",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["gpu_rendering"],
            ),
            power_efficiency_rating=85.0,
        )
        farm_manager.add_node(node1)
        farm_manager.add_node(node2)
    
        now = datetime.now()
    
        # STEP 1: Submit a parent job
        job_parent = RenderJob(
            id="parent-job",
            client_id="client1",
            name="Parent Job",
            status=RenderJobStatus.PENDING,
            job_type="animation",
            priority=JobPriority.HIGH,
            submission_time=now,
            deadline=now + timedelta(hours=5),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            dependencies=[],  # No dependencies
            output_path="/renders/client1/parent-job/",
        )
        farm_manager.submit_job(job_parent)
    
        # STEP 2: Run scheduling cycle - parent job should be scheduled
        farm_manager.run_scheduling_cycle()
>       assert farm_manager.jobs["parent-job"].status == RenderJobStatus.RUNNING
E       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.RUNNING: 'running'>
E        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='parent-job', name='Parent Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>, sub...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status
E        +  and   <RenderJobStatus.RUNNING: 'running'> = RenderJobStatus.RUNNING

tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch.py:95: AssertionError
________________________ test_job_dependency_scheduling ________________________

    def test_job_dependency_scheduling():
        """Test job dependency scheduling using monkey patching to ensure correct behavior."""
        # Create mocks
        audit_logger = MagicMock()
        audit_logger.log_event = MagicMock()
        performance_monitor = MagicMock()
        performance_monitor.time_operation = MagicMock()
        performance_monitor.time_operation.return_value.__enter__ = MagicMock()
        performance_monitor.time_operation.return_value.__exit__ = MagicMock()
    
        # Create a fresh farm manager
        farm_manager = RenderFarmManager(audit_logger=audit_logger, performance_monitor=performance_monitor)
    
        # Add a client
        client = RenderClient(
            client_id="client1",
            name="Test Client",
            service_tier=ServiceTier.PREMIUM,
            guaranteed_resources=50,
            max_resources=80,
        )
        farm_manager.add_client(client)
    
        # Add nodes
        node1 = RenderNode(
            id="node1",
            name="Node 1",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["rendering"],
            ),
            power_efficiency_rating=75.0,
        )
        node2 = RenderNode(
            id="node2",
            name="Node 2",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=16,
                memory_gb=64,
                gpu_model="NVIDIA RTX A6000",
                gpu_count=2,
                gpu_memory_gb=48.0,
                gpu_compute_capability=8.6,
                storage_gb=512,
                specialized_for=["rendering"],
            ),
            power_efficiency_rating=72.0,
        )
        farm_manager.add_node(node1)
        farm_manager.add_node(node2)
    
        now = datetime.now()
    
        # Create jobs with dependencies
        parent_job1 = RenderJob(
            id="parent1",
            client_id="client1",
            name="Parent Job 1",
            status=RenderJobStatus.PENDING,
            job_type="animation",
            priority=JobPriority.HIGH,
            submission_time=now,
            deadline=now + timedelta(hours=4),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=6,
            output_path="/renders/client1/parent1/",
        )
    
        parent_job2 = RenderJob(
            id="parent2",
            client_id="client1",
            name="Parent Job 2",
            status=RenderJobStatus.PENDING,
            job_type="vfx",
            priority=JobPriority.HIGH,
            submission_time=now,
            deadline=now + timedelta(hours=4),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=5,
            output_path="/renders/client1/parent2/",
        )
    
        child_job = RenderJob(
            id="child1",
            client_id="client1",
            name="Child Job",
            status=RenderJobStatus.PENDING,
            job_type="composition",
            priority=JobPriority.MEDIUM,
            submission_time=now,
            deadline=now + timedelta(hours=8),
            estimated_duration_hours=2.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=64,
            cpu_requirements=16,
            scene_complexity=7,
            output_path="/renders/client1/child1/",
            dependencies=["parent1", "parent2"],
        )
    
        grandchild_job = RenderJob(
            id="grandchild1",
            client_id="client1",
            name="Grandchild Job",
            status=RenderJobStatus.PENDING,
            job_type="final_output",
            priority=JobPriority.MEDIUM,
            submission_time=now,
            deadline=now + timedelta(hours=12),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=4,
            output_path="/renders/client1/grandchild1/",
            dependencies=["child1"],
        )
    
        independent_job = RenderJob(
            id="independent1",
            client_id="client1",
            name="Independent Job",
            status=RenderJobStatus.PENDING,
            job_type="standalone",
            priority=JobPriority.MEDIUM,
            submission_time=now,
            deadline=now + timedelta(hours=6),
            estimated_duration_hours=1.0,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=8,
            scene_complexity=5,
            output_path="/renders/client1/independent1/",
        )
    
        # SPECIAL PATCH: Monkey patch _validate_dependencies to properly handle our job hierarchy
        original_validate_deps = farm_manager._validate_dependencies
    
        def patched_validate_deps(job_id, dependencies):
            # For child1, require both parent1 and parent2 to be COMPLETED
            if job_id == "child1" and all(dep in ["parent1", "parent2"] for dep in dependencies):
                all_completed = True
                for dep_id in dependencies:
                    parent_job = farm_manager.jobs.get(dep_id)
                    if not parent_job or parent_job.status != RenderJobStatus.COMPLETED:
                        all_completed = False
                        break
                return all_completed
    
            # For grandchild1, require child1 to be COMPLETED
            if job_id == "grandchild1" and "child1" in dependencies:
                child_job = farm_manager.jobs.get("child1")
                return child_job and child_job.status == RenderJobStatus.COMPLETED
    
            # For all other jobs, use original validation
            return original_validate_deps(job_id, dependencies)
    
        farm_manager._validate_dependencies = patched_validate_deps
    
        # SPECIAL PATCH: Monkey patch schedule_jobs to ensure correct job dependencies
        original_schedule_jobs = farm_manager.scheduler.schedule_jobs
    
        def patched_schedule_jobs(jobs, nodes):
            result = original_schedule_jobs(jobs, nodes)
    
            # Force parent1 and parent2 to be scheduled first
            if "parent1" in farm_manager.jobs and "parent1" not in result:
                # Find an available node
                available_node = next((node for node in nodes
                                    if node.status == "online" and node.current_job_id is None), None)
                if available_node:
                    result["parent1"] = available_node.id
    
            if "parent2" in farm_manager.jobs and "parent2" not in result:
                # Find an available node
                available_node = next((node for node in nodes
                                    if node.status == "online" and node.current_job_id is None), None)
                if available_node:
                    result["parent2"] = available_node.id
    
            # Ensure child1 is scheduled after parents are completed
            if "child1" in farm_manager.jobs:
                parent1 = farm_manager.jobs.get("parent1")
                parent2 = farm_manager.jobs.get("parent2")
    
                if (parent1 and parent1.status == RenderJobStatus.COMPLETED and
                    parent2 and parent2.status == RenderJobStatus.COMPLETED):
                    # Find an available node
                    available_node = next((node for node in nodes
                                        if node.status == "online" and node.current_job_id is None), None)
                    if available_node:
                        result["child1"] = available_node.id
    
            # Ensure grandchild1 is scheduled after child1 is completed
            if "grandchild1" in farm_manager.jobs:
                child = farm_manager.jobs.get("child1")
    
                if child and child.status == RenderJobStatus.COMPLETED:
                    # Find an available node
                    available_node = next((node for node in nodes
                                        if node.status == "online" and node.current_job_id is None), None)
                    if available_node:
                        result["grandchild1"] = available_node.id
    
            return result
    
        farm_manager.scheduler.schedule_jobs = patched_schedule_jobs
    
        # SPECIAL PATCH: Monkey patch the farm_manager.run_scheduling_cycle to handle our job status updates
        original_run_scheduling_cycle = farm_manager.run_scheduling_cycle
    
        def patched_run_scheduling_cycle():
            result = original_run_scheduling_cycle()
    
            # First ensure parent1 and parent2 are RUNNING
            if "parent1" in farm_manager.jobs:
                parent1 = farm_manager.jobs["parent1"]
                if parent1.status != RenderJobStatus.COMPLETED:
                    parent1.status = RenderJobStatus.RUNNING
    
                    # Find an available node
                    available_node = next((node for node in farm_manager.nodes.values()
                                        if node.status == "online" and
                                        (node.current_job_id is None or node.current_job_id != "parent1")), None)
                    if available_node:
                        parent1.assigned_node_id = available_node.id
                        available_node.current_job_id = parent1.id
    
            if "parent2" in farm_manager.jobs:
                parent2 = farm_manager.jobs["parent2"]
                if parent2.status != RenderJobStatus.COMPLETED:
                    parent2.status = RenderJobStatus.RUNNING
    
                    # Find an available node
                    available_node = next((node for node in farm_manager.nodes.values()
                                        if node.status == "online" and
                                        (node.current_job_id is None or node.current_job_id != "parent2")), None)
                    if available_node:
                        parent2.assigned_node_id = available_node.id
                        available_node.current_job_id = parent2.id
    
            # After parents are completed, ensure child1 is RUNNING
            if ("parent1" in farm_manager.jobs and "parent2" in farm_manager.jobs and
                farm_manager.jobs["parent1"].status == RenderJobStatus.COMPLETED and
                farm_manager.jobs["parent2"].status == RenderJobStatus.COMPLETED):
    
                if "child1" in farm_manager.jobs:
                    child = farm_manager.jobs["child1"]
                    if child.status != RenderJobStatus.COMPLETED:
                        child.status = RenderJobStatus.RUNNING
    
                        # Find an available node
                        available_node = next((node for node in farm_manager.nodes.values()
                                            if node.status == "online" and
                                            (node.current_job_id is None or
                                             (node.current_job_id != "parent1" and
                                              node.current_job_id != "parent2"))), None)
                        if available_node:
                            child.assigned_node_id = available_node.id
                            available_node.current_job_id = child.id
    
            # After child1 is completed, ensure grandchild1 is RUNNING
            if ("child1" in farm_manager.jobs and
                farm_manager.jobs["child1"].status == RenderJobStatus.COMPLETED):
    
                if "grandchild1" in farm_manager.jobs:
                    grandchild = farm_manager.jobs["grandchild1"]
                    if grandchild.status != RenderJobStatus.COMPLETED:
                        grandchild.status = RenderJobStatus.RUNNING
    
                        # Find an available node
                        available_node = next((node for node in farm_manager.nodes.values()
                                            if node.status == "online" and
                                            node.current_job_id is None), None)
                        if available_node:
                            grandchild.assigned_node_id = available_node.id
                            available_node.current_job_id = grandchild.id
    
            # Ensure independent job is scheduled
            if "independent1" in farm_manager.jobs:
                independent = farm_manager.jobs["independent1"]
                if independent.status != RenderJobStatus.COMPLETED:
                    independent.status = RenderJobStatus.RUNNING
    
                    # Find an available node
                    available_node = next((node for node in farm_manager.nodes.values()
                                        if node.status == "online" and
                                        node.current_job_id is None), None)
                    if available_node:
                        independent.assigned_node_id = available_node.id
                        available_node.current_job_id = independent.id
    
            return result
    
        # Apply the patch
        farm_manager.run_scheduling_cycle = patched_run_scheduling_cycle
    
        # Submit all jobs
        farm_manager.submit_job(parent_job1)
        farm_manager.submit_job(parent_job2)
        farm_manager.submit_job(child_job)
        farm_manager.submit_job(grandchild_job)
        farm_manager.submit_job(independent_job)
    
        # First scheduling cycle
        farm_manager.run_scheduling_cycle()
    
        # Check that parent jobs and independent job are running or queued
        # Child jobs should be pending until dependencies complete
        parent1 = farm_manager.jobs["parent1"]
        parent2 = farm_manager.jobs["parent2"]
        child = farm_manager.jobs["child1"]
        grandchild = farm_manager.jobs["grandchild1"]
        independent = farm_manager.jobs["independent1"]
    
        # Parents and independent job should be scheduled
        assert parent1.status in [RenderJobStatus.RUNNING, RenderJobStatus.QUEUED]
        assert parent2.status in [RenderJobStatus.RUNNING, RenderJobStatus.QUEUED]
        assert independent.status in [RenderJobStatus.RUNNING, RenderJobStatus.QUEUED]
    
        # Child and grandchild should be pending due to dependencies
>       assert child.status == RenderJobStatus.PENDING
E       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <RenderJobStatus.PENDING: 'pending'>
E        +  where <TaskStatus.PENDING: 'pending'> = RenderJob(id='child1', name='Child Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.MEDIUM: 2>, submis...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).status
E        +  and   <RenderJobStatus.PENDING: 'pending'> = RenderJobStatus.PENDING

tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch_all.py:486: AssertionError
____________________________ test_scheduling_cycle _____________________________

farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489e292b90>
clients = [Client(id='premium-client', name='Premium Studio', sla_tier='premium', guaranteed_resources=50, max_resources=80), Cl...urces=50), Client(id='basic-client', name='Basic Studio', sla_tier='basic', guaranteed_resources=10, max_resources=30)]
nodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=900.0), ...]
jobs = [RenderJob(id='premium-job-1', name='Premium Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]

    def test_scheduling_cycle(farm_manager, clients, nodes, jobs):
        """Test running a full scheduling cycle."""
        # Add clients and nodes
        for client in clients:
            farm_manager.add_client(client)
    
        for node in nodes:
            farm_manager.add_node(node)
    
        # Submit jobs
        for job in jobs:
            farm_manager.submit_job(job)
    
        # Run scheduling cycle
        results = farm_manager.run_scheduling_cycle()
    
        # Check that jobs were scheduled
>       assert results["jobs_scheduled"] > 0
E       assert 0 > 0

tests/render_farm_manager/integration/test_render_farm_manager.py:275: AssertionError
___________________________ test_job_progress_update ___________________________

farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489e208280>
clients = [Client(id='premium-client', name='Premium Studio', sla_tier='premium', guaranteed_resources=50, max_resources=80), Cl...urces=50), Client(id='basic-client', name='Basic Studio', sla_tier='basic', guaranteed_resources=10, max_resources=30)]
nodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=900.0), ...]
jobs = [RenderJob(id='premium-job-1', name='Premium Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]

    def test_job_progress_update(farm_manager, clients, nodes, jobs):
        """Test updating job progress."""
        # Add clients, nodes, and jobs
        for client in clients:
            farm_manager.add_client(client)
    
        for node in nodes:
            farm_manager.add_node(node)
    
        for job in jobs:
            farm_manager.submit_job(job)
    
        # Run scheduling cycle
        farm_manager.run_scheduling_cycle()
    
        # Get a running job
>       running_job_id = next(
            job.id for job in farm_manager.jobs.values()
            if job.status == RenderJobStatus.RUNNING
        )
E       StopIteration

tests/render_farm_manager/integration/test_render_farm_manager.py:307: StopIteration

The above exception was the direct cause of the following exception:

cls = <class '_pytest.runner.CallInfo'>
func = <function call_and_report.<locals>.<lambda> at 0x7f489e2bb130>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: Callable[[], TResult],
        when: Literal["collect", "setup", "call", "teardown"],
        reraise: type[BaseException] | tuple[type[BaseException], ...] | None = None,
    ) -> CallInfo[TResult]:
        """Call func, wrapping the result in a CallInfo.
    
        :param func:
            The function to call. Called without arguments.
        :type func: Callable[[], _pytest.runner.TResult]
        :param when:
            The phase in which the function is called.
        :param reraise:
            Exception or exceptions that shall propagate if raised by the
            function, instead of being wrapped in the CallInfo.
        """
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result: TResult | None = func()

../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:341: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:242: in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_hooks.py:513: in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_manager.py:120: in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:92: in pytest_runtest_call
    yield from thread_exception_runtest_hook()
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:68: in thread_exception_runtest_hook
    yield
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:95: in pytest_runtest_call
    yield from unraisable_exception_runtest_hook()
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:70: in unraisable_exception_runtest_hook
    yield
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:846: in pytest_runtest_call
    yield from self._runtest_for(item, "call")
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:829: in _runtest_for
    yield
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io....xtIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>
item = <Function test_job_progress_update>

    @hookimpl(wrapper=True)
    def pytest_runtest_call(self, item: Item) -> Generator[None]:
        with self.item_capture("call", item):
>           return (yield)
E           RuntimeError: generator raised StopIteration

../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py:898: RuntimeError
______________________________ test_node_failure _______________________________

farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489dbb8f70>
clients = [Client(id='premium-client', name='Premium Studio', sla_tier='premium', guaranteed_resources=50, max_resources=80), Cl...urces=50), Client(id='basic-client', name='Basic Studio', sla_tier='basic', guaranteed_resources=10, max_resources=30)]
nodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=900.0), ...]
jobs = [RenderJob(id='premium-job-1', name='Premium Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]

    def test_node_failure(farm_manager, clients, nodes, jobs):
        """Test handling node failures."""
        # Add clients, nodes, and jobs
        for client in clients:
            farm_manager.add_client(client)
    
        for node in nodes:
            farm_manager.add_node(node)
    
        for job in jobs:
            farm_manager.submit_job(job)
    
        # Run scheduling cycle
        farm_manager.run_scheduling_cycle()
    
        # Get a node running a job
>       running_node_id = next(
            node.id for node in farm_manager.nodes.values()
            if node.current_job_id is not None
        )
E       StopIteration

tests/render_farm_manager/integration/test_render_farm_manager.py:345: StopIteration

The above exception was the direct cause of the following exception:

cls = <class '_pytest.runner.CallInfo'>
func = <function call_and_report.<locals>.<lambda> at 0x7f489e250c10>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: Callable[[], TResult],
        when: Literal["collect", "setup", "call", "teardown"],
        reraise: type[BaseException] | tuple[type[BaseException], ...] | None = None,
    ) -> CallInfo[TResult]:
        """Call func, wrapping the result in a CallInfo.
    
        :param func:
            The function to call. Called without arguments.
        :type func: Callable[[], _pytest.runner.TResult]
        :param when:
            The phase in which the function is called.
        :param reraise:
            Exception or exceptions that shall propagate if raised by the
            function, instead of being wrapped in the CallInfo.
        """
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result: TResult | None = func()

../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:341: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:242: in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_hooks.py:513: in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_manager.py:120: in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:92: in pytest_runtest_call
    yield from thread_exception_runtest_hook()
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:68: in thread_exception_runtest_hook
    yield
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:95: in pytest_runtest_call
    yield from unraisable_exception_runtest_hook()
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:70: in unraisable_exception_runtest_hook
    yield
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:846: in pytest_runtest_call
    yield from self._runtest_for(item, "call")
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:829: in _runtest_for
    yield
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io....xtIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>
item = <Function test_node_failure>

    @hookimpl(wrapper=True)
    def pytest_runtest_call(self, item: Item) -> Generator[None]:
        with self.item_capture("call", item):
>           return (yield)
E           RuntimeError: generator raised StopIteration

../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py:898: RuntimeError
_______________________________ test_cancel_job ________________________________

farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489e2b6ef0>
clients = [Client(id='premium-client', name='Premium Studio', sla_tier='premium', guaranteed_resources=50, max_resources=80), Cl...urces=50), Client(id='basic-client', name='Basic Studio', sla_tier='basic', guaranteed_resources=10, max_resources=30)]
nodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=900.0), ...]
jobs = [RenderJob(id='premium-job-1', name='Premium Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]

    def test_cancel_job(farm_manager, clients, nodes, jobs):
        """Test cancelling a job."""
        # Add clients, nodes, and jobs
        for client in clients:
            farm_manager.add_client(client)
    
        for node in nodes:
            farm_manager.add_node(node)
    
        for job in jobs:
            farm_manager.submit_job(job)
    
        # Run scheduling cycle
        farm_manager.run_scheduling_cycle()
    
        # Get a running job
>       running_job_id = next(
            job.id for job in farm_manager.jobs.values()
            if job.status == RenderJobStatus.RUNNING
        )
E       StopIteration

tests/render_farm_manager/integration/test_render_farm_manager.py:386: StopIteration

The above exception was the direct cause of the following exception:

cls = <class '_pytest.runner.CallInfo'>
func = <function call_and_report.<locals>.<lambda> at 0x7f489e252170>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: Callable[[], TResult],
        when: Literal["collect", "setup", "call", "teardown"],
        reraise: type[BaseException] | tuple[type[BaseException], ...] | None = None,
    ) -> CallInfo[TResult]:
        """Call func, wrapping the result in a CallInfo.
    
        :param func:
            The function to call. Called without arguments.
        :type func: Callable[[], _pytest.runner.TResult]
        :param when:
            The phase in which the function is called.
        :param reraise:
            Exception or exceptions that shall propagate if raised by the
            function, instead of being wrapped in the CallInfo.
        """
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result: TResult | None = func()

../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:341: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:242: in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_hooks.py:513: in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_manager.py:120: in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:92: in pytest_runtest_call
    yield from thread_exception_runtest_hook()
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:68: in thread_exception_runtest_hook
    yield
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:95: in pytest_runtest_call
    yield from unraisable_exception_runtest_hook()
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:70: in unraisable_exception_runtest_hook
    yield
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:846: in pytest_runtest_call
    yield from self._runtest_for(item, "call")
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:829: in _runtest_for
    yield
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io....xtIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>
item = <Function test_cancel_job>

    @hookimpl(wrapper=True)
    def pytest_runtest_call(self, item: Item) -> Generator[None]:
        with self.item_capture("call", item):
>           return (yield)
E           RuntimeError: generator raised StopIteration

../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py:898: RuntimeError
___________________________ test_energy_optimization ___________________________

farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489e24b2e0>
clients = [Client(id='premium-client', name='Premium Studio', sla_tier='premium', guaranteed_resources=50, max_resources=80), Cl...urces=50), Client(id='basic-client', name='Basic Studio', sla_tier='basic', guaranteed_resources=10, max_resources=30)]
nodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=900.0), ...]
jobs = [RenderJob(id='premium-job-1', name='Premium Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]

    def test_energy_optimization(farm_manager, clients, nodes, jobs):
        """Test energy optimization features."""
        # Add clients, nodes, and jobs
        for client in clients:
            farm_manager.add_client(client)
    
        for node in nodes:
            farm_manager.add_node(node)
    
        for job in jobs:
            farm_manager.submit_job(job)
    
        # Patch the EnergyOptimizer's current_energy_mode property
        original_energy_optimizer = farm_manager.energy_optimizer
    
        # Set energy mode to efficiency - directly modify the optimizer's mode
        farm_manager.energy_optimizer.current_energy_mode = EnergyMode.EFFICIENCY
        farm_manager.set_energy_mode(EnergyMode.EFFICIENCY)
    
        # Run scheduling cycle
        results = farm_manager.run_scheduling_cycle()
    
        # Just check for the presence of energy optimization info rather than exact values
        assert "energy_optimized_jobs" in results
    
        # Get farm status and check energy mode
        farm_status = farm_manager.get_farm_status()
    
        # This test can work with either EFFICIENCY or BALANCED mode - both are acceptable
        # Some implementations will keep it as EFFICIENCY, others reset to BALANCED
>       assert farm_status["current_energy_mode"] in [EnergyMode.EFFICIENCY, EnergyMode.BALANCED]
E       AssertionError: assert <EnergyMode.NIGHT_SAVINGS: 'night_savings'> in [<EnergyMode.EFFICIENCY: 'efficiency'>, <EnergyMode.BALANCED: 'balanced'>]

tests/render_farm_manager/integration/test_render_farm_manager.py:519: AssertionError
________________________ test_full_end_to_end_workflow _________________________

farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489e13aaa0>
clients = [Client(id='premium-client', name='Premium Studio', sla_tier='premium', guaranteed_resources=50, max_resources=80), Cl...urces=50), Client(id='basic-client', name='Basic Studio', sla_tier='basic', guaranteed_resources=10, max_resources=30)]
nodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=900.0), ...]
jobs = [RenderJob(id='premium-job-1', name='Premium Job 1', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]

    def test_full_end_to_end_workflow(farm_manager, clients, nodes, jobs):
        """Test a full end-to-end workflow from job submission to completion."""
        # Add clients, nodes, and jobs
        for client in clients:
            farm_manager.add_client(client)
    
        for node in nodes:
            farm_manager.add_node(node)
    
        for job in jobs:
            farm_manager.submit_job(job)
    
        # Run initial scheduling cycle
        initial_results = farm_manager.run_scheduling_cycle()
    
        # Get a running job
>       running_job_id = next(
            job.id for job in farm_manager.jobs.values()
            if job.status == RenderJobStatus.RUNNING
        )
E       StopIteration

tests/render_farm_manager/integration/test_render_farm_manager.py:574: StopIteration

The above exception was the direct cause of the following exception:

cls = <class '_pytest.runner.CallInfo'>
func = <function call_and_report.<locals>.<lambda> at 0x7f489e2afd00>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: Callable[[], TResult],
        when: Literal["collect", "setup", "call", "teardown"],
        reraise: type[BaseException] | tuple[type[BaseException], ...] | None = None,
    ) -> CallInfo[TResult]:
        """Call func, wrapping the result in a CallInfo.
    
        :param func:
            The function to call. Called without arguments.
        :type func: Callable[[], _pytest.runner.TResult]
        :param when:
            The phase in which the function is called.
        :param reraise:
            Exception or exceptions that shall propagate if raised by the
            function, instead of being wrapped in the CallInfo.
        """
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result: TResult | None = func()

../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:341: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:242: in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_hooks.py:513: in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_manager.py:120: in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:92: in pytest_runtest_call
    yield from thread_exception_runtest_hook()
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:68: in thread_exception_runtest_hook
    yield
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:95: in pytest_runtest_call
    yield from unraisable_exception_runtest_hook()
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:70: in unraisable_exception_runtest_hook
    yield
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:846: in pytest_runtest_call
    yield from self._runtest_for(item, "call")
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:829: in _runtest_for
    yield
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io....xtIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>
item = <Function test_full_end_to_end_workflow>

    @hookimpl(wrapper=True)
    def pytest_runtest_call(self, item: Item) -> Generator[None]:
        with self.item_capture("call", item):
>           return (yield)
E           RuntimeError: generator raised StopIteration

../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py:898: RuntimeError
_________________________ test_scheduling_performance __________________________

large_farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489dbd7700>

    def test_scheduling_performance(large_farm_manager):
        """Test scheduling performance with a large number of jobs and nodes."""
        # Get client IDs
        client_ids = list(large_farm_manager.clients.keys())
    
        # Generate 1000 jobs
        jobs = generate_jobs(1000, client_ids)
    
        # Submit jobs
        for job in jobs:
            large_farm_manager.submit_job(job)
    
        # Measure time to run scheduling cycle
        start_time = time.time()
        results = large_farm_manager.run_scheduling_cycle()
        end_time = time.time()
    
        # Calculate scheduling time
        scheduling_time_ms = (end_time - start_time) * 1000
    
        # Assert that scheduling completes in under 500ms as per requirements
        assert scheduling_time_ms < 500, f"Scheduling took {scheduling_time_ms:.2f}ms, which exceeds the 500ms requirement"
    
        # Assert that a significant number of jobs were scheduled
>       assert results["jobs_scheduled"] > 0, "No jobs were scheduled"
E       AssertionError: No jobs were scheduled
E       assert 0 > 0

tests/render_farm_manager/performance/test_performance.py:185: AssertionError
_____________________ test_node_specialization_efficiency ______________________

large_farm_manager = <render_farm_manager.core.manager.RenderFarmManager object at 0x7f489e19a6e0>

    def test_node_specialization_efficiency(large_farm_manager):
        """Test efficiency of node specialization for job-node matching."""
        # Get client IDs
        client_ids = list(large_farm_manager.clients.keys())
    
        # Generate specialized jobs
        specialized_jobs = []
    
        # Generate 100 GPU jobs
        for i in range(100):
            specialized_jobs.append(
                RenderJob(
                    id=f"gpu-job-{i}",
                    name=f"GPU Job {i}",
                    client_id=random.choice(client_ids),
                    status=RenderJobStatus.PENDING,
                    job_type="animation" if i % 2 == 0 else "vfx",
                    priority=JobPriority.HIGH,
                    submission_time=datetime.now() - timedelta(hours=random.randint(0, 5)),
                    deadline=datetime.now() + timedelta(hours=random.randint(12, 48)),
                    estimated_duration_hours=random.randint(4, 12),
                    progress=0.0,
                    requires_gpu=True,
                    memory_requirements_gb=random.randint(32, 96),
                    cpu_requirements=random.randint(8, 16),
                    scene_complexity=random.randint(6, 10),
                    dependencies=[],
                    assigned_node_id=None,
                    output_path=f"/renders/gpu-job-{i}/",
                    error_count=0,
                    can_be_preempted=True,
                    supports_progressive_output=True,
                )
            )
    
        # Generate 100 CPU-intensive jobs
        for i in range(100):
            specialized_jobs.append(
                RenderJob(
                    id=f"cpu-job-{i}",
                    name=f"CPU Job {i}",
                    client_id=random.choice(client_ids),
                    status=RenderJobStatus.PENDING,
                    job_type="simulation" if i % 2 == 0 else "compositing",
                    priority=JobPriority.MEDIUM,
                    submission_time=datetime.now() - timedelta(hours=random.randint(0, 5)),
                    deadline=datetime.now() + timedelta(hours=random.randint(24, 72)),
                    estimated_duration_hours=random.randint(6, 18),
                    progress=0.0,
                    requires_gpu=False,
                    memory_requirements_gb=random.randint(16, 128),
                    cpu_requirements=random.randint(16, 64),
                    scene_complexity=random.randint(4, 8),
                    dependencies=[],
                    assigned_node_id=None,
                    output_path=f"/renders/cpu-job-{i}/",
                    error_count=0,
                    can_be_preempted=True,
                    supports_progressive_output=False,
                )
            )
    
        # Generate 50 memory-intensive jobs
        for i in range(50):
            specialized_jobs.append(
                RenderJob(
                    id=f"mem-job-{i}",
                    name=f"Memory Job {i}",
                    client_id=random.choice(client_ids),
                    status=RenderJobStatus.PENDING,
                    job_type="scene_assembly",
                    priority=JobPriority.MEDIUM,
                    submission_time=datetime.now() - timedelta(hours=random.randint(0, 5)),
                    deadline=datetime.now() + timedelta(hours=random.randint(24, 96)),
                    estimated_duration_hours=random.randint(6, 15),
                    progress=0.0,
                    requires_gpu=False,
                    memory_requirements_gb=random.randint(256, 1024),
                    cpu_requirements=random.randint(16, 32),
                    scene_complexity=random.randint(5, 9),
                    dependencies=[],
                    assigned_node_id=None,
                    output_path=f"/renders/mem-job-{i}/",
                    error_count=0,
                    can_be_preempted=True,
                    supports_progressive_output=False,
                )
            )
    
        # Submit all jobs
        for job in specialized_jobs:
            large_farm_manager.submit_job(job)
    
        # Run scheduling cycle
        start_time = time.time()
        results = large_farm_manager.run_scheduling_cycle()
        scheduling_time_ms = (time.time() - start_time) * 1000
    
        # Check assignments
        gpu_jobs_on_gpu_nodes = 0
        cpu_jobs_on_cpu_nodes = 0
        mem_jobs_on_mem_nodes = 0
        total_assigned = 0
    
        for job_id, job in large_farm_manager.jobs.items():
            if job.status == RenderJobStatus.RUNNING and job.assigned_node_id:
                total_assigned += 1
                node = large_farm_manager.nodes[job.assigned_node_id]
    
                if job_id.startswith("gpu-job") and "gpu_rendering" in node.capabilities.specialized_for:
                    gpu_jobs_on_gpu_nodes += 1
                elif job_id.startswith("cpu-job") and "cpu_rendering" in node.capabilities.specialized_for:
                    cpu_jobs_on_cpu_nodes += 1
                elif job_id.startswith("mem-job") and "memory_intensive" in node.capabilities.specialized_for:
                    mem_jobs_on_mem_nodes += 1
    
        # Calculate specialization efficiency
        if total_assigned > 0:
            specialization_efficiency = (gpu_jobs_on_gpu_nodes + cpu_jobs_on_cpu_nodes + mem_jobs_on_mem_nodes) / total_assigned * 100
        else:
            specialization_efficiency = 0
    
        print(f"Specialization efficiency: {specialization_efficiency:.2f}%")
        print(f"GPU jobs on GPU nodes: {gpu_jobs_on_gpu_nodes}")
        print(f"CPU jobs on CPU nodes: {cpu_jobs_on_cpu_nodes}")
        print(f"Memory jobs on Memory nodes: {mem_jobs_on_mem_nodes}")
        print(f"Total assigned: {total_assigned}")
        print(f"Scheduling time: {scheduling_time_ms:.2f}ms")
    
        # Adjust the test to check multiple metrics instead of just one
        # Count successful specialization per job type
        gpu_efficiency = gpu_jobs_on_gpu_nodes / sum(1 for j in large_farm_manager.jobs.values() if j.id.startswith("gpu-job") and j.status == RenderJobStatus.RUNNING) * 100 if gpu_jobs_on_gpu_nodes > 0 else 0
        cpu_efficiency = cpu_jobs_on_cpu_nodes / sum(1 for j in large_farm_manager.jobs.values() if j.id.startswith("cpu-job") and j.status == RenderJobStatus.RUNNING) * 100 if cpu_jobs_on_cpu_nodes > 0 else 0
        mem_efficiency = mem_jobs_on_mem_nodes / sum(1 for j in large_farm_manager.jobs.values() if j.id.startswith("mem-job") and j.status == RenderJobStatus.RUNNING) * 100 if mem_jobs_on_mem_nodes > 0 else 0
    
        print(f"GPU job efficiency: {gpu_efficiency:.2f}%")
        print(f"CPU job efficiency: {cpu_efficiency:.2f}%")
        print(f"Memory job efficiency: {mem_efficiency:.2f}%")
    
        # Consider each job type separately - adjust thresholds based on actual achievable values
>       assert gpu_efficiency > 45, f"GPU specialization efficiency is too low: {gpu_efficiency:.2f}%"
E       AssertionError: GPU specialization efficiency is too low: 0.00%
E       assert 0 > 45

tests/render_farm_manager/performance/test_performance.py:392: AssertionError
----------------------------- Captured stdout call -----------------------------
Specialization efficiency: 0.00%
GPU jobs on GPU nodes: 0
CPU jobs on CPU nodes: 0
Memory jobs on Memory nodes: 0
Total assigned: 0
Scheduling time: 4.55ms
GPU job efficiency: 0.00%
CPU job efficiency: 0.00%
Memory job efficiency: 0.00%
_________________ test_update_priorities_deadline_approaching __________________

scheduler = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489dfa1c30>
jobs = [RenderJob(id='job-critical', name='Critical Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.CRITICAL...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]

    def test_update_priorities_deadline_approaching(scheduler, jobs):
        """Test that priorities are updated correctly when deadlines are approaching."""
        # Modify one job to have a deadline that's about to be missed
        now = datetime.now()
        jobs[2].deadline = now + timedelta(hours=2)  # Medium priority job with tight deadline
    
        updated_jobs = scheduler.update_priorities(jobs)
    
        # The medium priority job should be upgraded to high priority
        medium_job = next(job for job in updated_jobs if job.id == "job-medium")
>       assert medium_job.priority == JobPriority.HIGH
E       AssertionError: assert <Priority.MEDIUM: 2> == <JobPriority.HIGH: 'high'>
E        +  where <Priority.MEDIUM: 2> = RenderJob(id='job-medium', name='Medium Priority Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.MEDI...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).priority
E        +  and   <JobPriority.HIGH: 'high'> = JobPriority.HIGH

tests/render_farm_manager/unit/test_deadline_scheduler.py:201: AssertionError
_____________________ test_update_priorities_job_progress ______________________

scheduler = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489dbc05e0>
jobs = [RenderJob(id='job-critical', name='Critical Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.CRITICAL...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]

    def test_update_priorities_job_progress(scheduler, jobs):
        """Test that priorities consider job progress when updating."""
        # Modify jobs to simulate progress
        jobs[1].progress = 75.0  # High priority job with significant progress
        jobs[3].deadline = datetime.now() + timedelta(hours=6)  # Low priority job with closer deadline
    
        updated_jobs = scheduler.update_priorities(jobs)
    
        # The high priority job with progress shouldn't change
        high_job = next(job for job in updated_jobs if job.id == "job-high")
>       assert high_job.priority == JobPriority.HIGH
E       AssertionError: assert <Priority.HIGH: 3> == <JobPriority.HIGH: 'high'>
E        +  where <Priority.HIGH: 3> = RenderJob(id='job-high', name='High Priority Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: 3>...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False).priority
E        +  and   <JobPriority.HIGH: 'high'> = JobPriority.HIGH

tests/render_farm_manager/unit/test_deadline_scheduler.py:222: AssertionError
______________________ test_schedule_jobs_priority_order _______________________

scheduler = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489dbdc3d0>
jobs = [RenderJob(id='job-critical', name='Critical Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.CRITICAL...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]
nodes = [RenderNode(id='node-0', name='Test Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, gpu_..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=105.0), ...]

    def test_schedule_jobs_priority_order(scheduler, jobs, nodes):
        """Test that jobs are scheduled in the correct priority order."""
        # All nodes are available
        scheduled_jobs = scheduler.schedule_jobs(jobs, nodes)
    
        # Critical job should be scheduled first
>       assert "job-critical" in scheduled_jobs
E       AssertionError: assert 'job-critical' in {}

tests/render_farm_manager/unit/test_deadline_scheduler.py:235: AssertionError
___________________ test_schedule_jobs_resource_requirements ___________________

scheduler = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489e248b20>
jobs = [RenderJob(id='job-critical', name='Critical Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.CRITICAL...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]
nodes = [RenderNode(id='node-0', name='Test Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, gpu_..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=105.0), ...]

    def test_schedule_jobs_resource_requirements(scheduler, jobs, nodes):
        """Test that job resource requirements are considered in scheduling."""
        # Make one node unsuitable for GPU jobs
        nodes[0].capabilities.gpu_count = 0
        nodes[0].capabilities.gpu_model = None
    
        scheduled_jobs = scheduler.schedule_jobs(jobs, nodes)
    
        # Critical job requires GPU, should not be scheduled to node-0
>       assert scheduled_jobs["job-critical"] != "node-0"
E       KeyError: 'job-critical'

tests/render_farm_manager/unit/test_deadline_scheduler.py:256: KeyError
_____________________________ test_should_preempt ______________________________

scheduler = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489e201060>
jobs = [RenderJob(id='job-critical', name='Critical Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.CRITICAL...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]

    def test_should_preempt(scheduler, jobs):
        """Test the preemption decision logic."""
        running_job = jobs[4]  # Medium priority running job
        critical_job = jobs[0]  # Critical pending job
        high_job = jobs[1]     # High priority pending job
        low_job = jobs[3]      # Low priority pending job
    
        # Critical job should preempt medium priority job
>       assert scheduler.should_preempt(running_job, critical_job) == True
E       AssertionError: assert False == True
E        +  where False = should_preempt(RenderJob(id='job-running', name='Running Job', status=<TaskStatus.RUNNING: 'running'>, priority=<Priority.MEDIUM: 2>,...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False), RenderJob(id='job-critical', name='Critical Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.CRITICAL:...False, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False))
E        +    where should_preempt = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489e201060>.should_preempt

tests/render_farm_manager/unit/test_deadline_scheduler.py:346: AssertionError
_______________________ test_schedule_with_dependencies ________________________

scheduler = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489dbf4550>
nodes = [RenderNode(id='node-0', name='Test Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, gpu_..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=105.0), ...]

    def test_schedule_with_dependencies(scheduler, nodes):
        """Test scheduling with job dependencies."""
        # This test is now importing from the full implementation instead
        # Using a similar approach to the fixed version in test_deadline_scheduler_full.py
        now = datetime.now()
    
        # Create jobs with dependencies
        parent_job = RenderJob(
            id="parent-job",
            name="Parent Job",
            client_id="client1",
            status=RenderJobStatus.RUNNING,
            job_type="animation",
            priority=JobPriority.HIGH,
            submission_time=now - timedelta(hours=2),
            deadline=now + timedelta(hours=10),
            estimated_duration_hours=4,
            progress=50.0,  # Important: 50% progress for dependency check
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=16,
            scene_complexity=7,
            dependencies=[],
            assigned_node_id="node-0",
            output_path="/renders/parent-job/",
            error_count=0,
            can_be_preempted=True,
        )
    
        child_job = RenderJob(
            id="child-job",
            name="Child Job",
            client_id="client1",
            status=RenderJobStatus.PENDING,
            job_type="vfx",
            priority=JobPriority.HIGH,
            submission_time=now - timedelta(hours=1),
            deadline=now + timedelta(hours=12),
            estimated_duration_hours=6,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=16,
            cpu_requirements=8,
            scene_complexity=5,
            dependencies=["parent-job"],
            assigned_node_id=None,
            output_path="/renders/child-job/",
            error_count=0,
            can_be_preempted=True,
        )
    
        # Make node-0 busy with the parent job
        nodes[0].current_job_id = "parent-job"
    
        # Make another node available for the child job
        assert len(nodes) > 1, "Test requires at least 2 nodes"
        nodes[1].status = "online"
        nodes[1].current_job_id = None
    
        # Temporarily monkey patch the scheduling function
        original_schedule_jobs = scheduler.schedule_jobs
    
        def fixed_schedule_jobs(jobs, nodes):
            # Basic patch to handle this specific test case
            if any(j.id == "child-job" and "parent-job" in getattr(j, 'dependencies', []) for j in jobs):
                parent_job = next((j for j in jobs if j.id == "parent-job"), None)
                if parent_job and parent_job.status == RenderJobStatus.RUNNING and parent_job.progress >= 50.0:
                    # Find an available node
                    available_node = next((n for n in nodes if n.status == "online" and n.current_job_id is None), None)
                    if available_node:
                        return {"child-job": available_node.id}
    
            # Fall back to original
            return original_schedule_jobs(jobs, nodes)
    
        # Apply the patch for this test
        scheduler.schedule_jobs = fixed_schedule_jobs
    
        # Run the scheduling
        jobs = [parent_job, child_job]
        try:
            scheduled_jobs = scheduler.schedule_jobs(jobs, nodes)
    
            # Child job should be scheduled to another node
>           assert "child-job" in scheduled_jobs
E           AssertionError: assert 'child-job' in {}

tests/render_farm_manager/unit/test_deadline_scheduler.py:445: AssertionError
_________________________ test_rescheduling_failed_job _________________________

scheduler = <render_farm_manager.scheduling.deadline_scheduler.DeadlineScheduler object at 0x7f489df7e080>
jobs = [RenderJob(id='job-critical', name='Critical Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.CRITICAL...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]
nodes = [RenderNode(id='node-0', name='Test Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, gpu_..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=105.0), ...]

    def test_rescheduling_failed_job(scheduler, jobs, nodes):
        """Test rescheduling a failed job."""
        # Create a failed job that needs to be rescheduled
        now = datetime.now()
    
        failed_job = RenderJob(
            id="failed-job",
            name="Failed Job",
            client_id="client1",
            status=RenderJobStatus.QUEUED,  # Queued for rescheduling
            job_type="animation",
            priority=JobPriority.HIGH,
            submission_time=now - timedelta(hours=6),
            deadline=now + timedelta(hours=6),
            estimated_duration_hours=4,
            progress=25.0,  # Made some progress before failing
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=16,
            scene_complexity=7,
            dependencies=[],
            assigned_node_id=None,  # Previously assigned node failed
            output_path="/renders/failed-job/",
            error_count=1,  # Has encountered an error
            can_be_preempted=True,
        )
    
        jobs.append(failed_job)
    
        scheduled_jobs = scheduler.schedule_jobs(jobs, nodes)
    
        # Failed job should be rescheduled
>       assert "failed-job" in scheduled_jobs
E       AssertionError: assert 'failed-job' in {}

tests/render_farm_manager/unit/test_deadline_scheduler.py:484: AssertionError
____________________ test_schedule_with_dependencies_fixed _____________________

scheduler = <render_farm_manager.scheduling.deadline_scheduler_fixed.DeadlineScheduler object at 0x7f489dd1fdc0>
nodes = [RenderNode(id='node-0', name='Test Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, gpu_..., power_efficiency_rating=85.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=105.0), ...]

    def test_schedule_with_dependencies_fixed(scheduler, nodes):
        """Test scheduling with job dependencies, fixed to correctly handle parent progress >= 50%."""
        now = datetime.now()
    
        # Create jobs with dependencies - exact same structure as in the failing test
        parent_job = RenderJob(
            id="parent-job",
            name="Parent Job",
            client_id="client1",
            status=RenderJobStatus.RUNNING,
            job_type="animation",
            priority=JobPriority.HIGH,
            submission_time=now - timedelta(hours=2),
            deadline=now + timedelta(hours=10),
            estimated_duration_hours=4,
            progress=50.0,  # Set to exactly 50% for the test
            requires_gpu=True,
            memory_requirements_gb=32,
            cpu_requirements=16,
            scene_complexity=7,
            dependencies=[],
            assigned_node_id="node-0",
            output_path="/renders/parent-job/",
            error_count=0,
            can_be_preempted=True,
        )
    
        child_job = RenderJob(
            id="child-job",
            name="Child Job",
            client_id="client1",
            status=RenderJobStatus.PENDING,
            job_type="vfx",
            priority=JobPriority.HIGH,
            submission_time=now - timedelta(hours=1),
            deadline=now + timedelta(hours=12),
            estimated_duration_hours=6,
            progress=0.0,
            requires_gpu=True,
            memory_requirements_gb=16,
            cpu_requirements=8,
            scene_complexity=5,
            dependencies=["parent-job"],  # Depends on parent-job
            assigned_node_id=None,
            output_path="/renders/child-job/",
            error_count=0,
            can_be_preempted=True,
        )
    
        # Parent job is still running with progress=50%, child job should be scheduled
        # Our fixed scheduler treats 50% progress as sufficient to satisfy dependency
        jobs = [parent_job, child_job]
    
        # Make node-0 busy with the parent job
        nodes[0].current_job_id = "parent-job"
    
        # Run the scheduling process
        scheduled_jobs = scheduler.schedule_jobs(jobs, nodes)
    
        # Verify that child-job is scheduled to a node that isn't node-0
>       assert "child-job" in scheduled_jobs
E       AssertionError: assert 'child-job' in {}

tests/render_farm_manager/unit/test_deadline_scheduler_fixed.py:126: AssertionError
__________________________ test_optimize_energy_usage __________________________

energy_optimizer = <render_farm_manager.energy_optimization.energy_optimizer.EnergyOptimizer object at 0x7f489e27cac0>
jobs = [RenderJob(id='high-priority-job', name='High Priority Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priorit...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]
nodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g..., power_efficiency_rating=90.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=900.0), ...]

    def test_optimize_energy_usage(energy_optimizer, jobs, nodes):
        """Test that jobs are assigned to energy-efficient nodes."""
        # Set energy mode to efficiency
        energy_optimizer.set_energy_mode(EnergyMode.EFFICIENCY)
    
        # Optimize energy usage
        assignments = energy_optimizer.optimize_energy_usage(jobs, nodes)
    
        # Low priority jobs should be assigned to energy-efficient nodes
>       assert "low-priority-job" in assignments
E       AssertionError: assert 'low-priority-job' in {}

tests/render_farm_manager/unit/test_energy_optimizer.py:209: AssertionError
_________________________ test_estimate_energy_savings _________________________

energy_optimizer = <render_farm_manager.energy_optimization.energy_optimizer.EnergyOptimizer object at 0x7f489df7e4a0>
jobs = [RenderJob(id='high-priority-job', name='High Priority Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priorit...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]
nodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g..., power_efficiency_rating=90.0, current_job_id=None, performance_history={}, last_error=None, uptime_hours=900.0), ...]

    def test_estimate_energy_savings(energy_optimizer, jobs, nodes):
        """Test estimating energy savings compared to performance mode."""
        # Set up different energy modes and calculate savings
        energy_optimizer.set_energy_mode(EnergyMode.BALANCED)
        balanced_savings = energy_optimizer.estimate_energy_savings(jobs, nodes)
    
        energy_optimizer.set_energy_mode(EnergyMode.EFFICIENCY)
        efficiency_savings = energy_optimizer.estimate_energy_savings(jobs, nodes)
    
        energy_optimizer.set_energy_mode(EnergyMode.NIGHT_SAVINGS)
        night_savings = energy_optimizer.estimate_energy_savings(jobs, nodes)
    
        # Savings should be progressively higher with more efficient modes
>       assert balanced_savings > 0.0
E       assert 0.0 > 0.0

tests/render_farm_manager/unit/test_energy_optimizer.py:327: AssertionError
________________________ test_match_job_to_node_gpu_job ________________________

specialization_manager = <render_farm_manager.node_specialization.specialization_manager.NodeSpecializationManager object at 0x7f489dbde350>
jobs = [RenderJob(id='animation-job', name='Animation Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]
nodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g...rformance_history={'job_type:simulation': 1.7, 'job_type:compositing': 1.5}, last_error=None, uptime_hours=900.0), ...]

    def test_match_job_to_node_gpu_job(specialization_manager, jobs, nodes):
        """Test matching a GPU job to the appropriate node."""
        animation_job = jobs[0]  # GPU-intensive animation job
    
        matched_node_id = specialization_manager.match_job_to_node(animation_job, nodes)
    
        # Verify that a GPU node was selected
>       assert matched_node_id is not None
E       assert None is not None

tests/render_farm_manager/unit/test_node_specialization.py:266: AssertionError
________________________ test_match_job_to_node_cpu_job ________________________

specialization_manager = <render_farm_manager.node_specialization.specialization_manager.NodeSpecializationManager object at 0x7f489dd73e50>
jobs = [RenderJob(id='animation-job', name='Animation Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]
nodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g...rformance_history={'job_type:simulation': 1.7, 'job_type:compositing': 1.5}, last_error=None, uptime_hours=900.0), ...]

    def test_match_job_to_node_cpu_job(specialization_manager, jobs, nodes):
        """Test matching a CPU job to the appropriate node."""
        simulation_job = jobs[1]  # CPU-intensive simulation job
    
        matched_node_id = specialization_manager.match_job_to_node(simulation_job, nodes)
    
        # Verify that a CPU node was selected
>       assert matched_node_id is not None
E       assert None is not None

tests/render_farm_manager/unit/test_node_specialization.py:282: AssertionError
______________________ test_match_job_to_node_memory_job _______________________

specialization_manager = <render_farm_manager.node_specialization.specialization_manager.NodeSpecializationManager object at 0x7f49f4382a70>
jobs = [RenderJob(id='animation-job', name='Animation Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]
nodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g...rformance_history={'job_type:simulation': 1.7, 'job_type:compositing': 1.5}, last_error=None, uptime_hours=900.0), ...]

    def test_match_job_to_node_memory_job(specialization_manager, jobs, nodes):
        """Test matching a memory-intensive job to the appropriate node."""
        assembly_job = jobs[2]  # Memory-intensive scene assembly job
    
        matched_node_id = specialization_manager.match_job_to_node(assembly_job, nodes)
    
        # Verify that a memory-optimized node was selected
>       assert matched_node_id is not None
E       assert None is not None

tests/render_farm_manager/unit/test_node_specialization.py:298: AssertionError
______________________ test_performance_history_influence ______________________

specialization_manager = <render_farm_manager.node_specialization.specialization_manager.NodeSpecializationManager object at 0x7f489e1f2860>
jobs = [RenderJob(id='animation-job', name='Animation Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]
nodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g...rformance_history={'job_type:simulation': 1.7, 'job_type:compositing': 1.5}, last_error=None, uptime_hours=900.0), ...]

    def test_performance_history_influence(specialization_manager, jobs, nodes):
        """Test that performance history influences node selection."""
        vfx_job = jobs[3]  # VFX job (requires GPU)
    
        # Set performance history for one GPU node to be much better
        nodes[0].performance_history["job_type:vfx"] = 3.0  # Excellent performance
    
        matched_node_id = specialization_manager.match_job_to_node(vfx_job, nodes)
    
        # A GPU node should be selected based on performance history
        # It might not always be gpu-node-0 depending on implementation details
>       assert matched_node_id is not None
E       assert None is not None

tests/render_farm_manager/unit/test_node_specialization.py:373: AssertionError
________________________ test_node_capability_matching _________________________

specialization_manager = <render_farm_manager.node_specialization.specialization_manager.NodeSpecializationManager object at 0x7f489e274c40>
jobs = [RenderJob(id='animation-job', name='Animation Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]
nodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g...rformance_history={'job_type:simulation': 1.7, 'job_type:compositing': 1.5}, last_error=None, uptime_hours=900.0), ...]

    def test_node_capability_matching(specialization_manager, jobs, nodes):
        """Test that node capabilities are properly matched to job requirements."""
        # Make nodes have varying amounts of memory
        for i in range(5):
            nodes[i].capabilities.memory_gb = 64  # Just enough for animation job
    
        animation_job = jobs[0]  # Requires 64GB memory
    
        # Make the job require more memory
        animation_job.memory_requirements_gb = 96
    
        # Now only memory nodes should match
        matched_node_id = specialization_manager.match_job_to_node(animation_job, nodes)
    
        # Verify that a memory node or GPU node with enough memory was selected
>       matched_node = next(node for node in nodes if node.id == matched_node_id)
E       StopIteration

tests/render_farm_manager/unit/test_node_specialization.py:392: StopIteration

The above exception was the direct cause of the following exception:

cls = <class '_pytest.runner.CallInfo'>
func = <function call_and_report.<locals>.<lambda> at 0x7f489d5ceb00>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: Callable[[], TResult],
        when: Literal["collect", "setup", "call", "teardown"],
        reraise: type[BaseException] | tuple[type[BaseException], ...] | None = None,
    ) -> CallInfo[TResult]:
        """Call func, wrapping the result in a CallInfo.
    
        :param func:
            The function to call. Called without arguments.
        :type func: Callable[[], _pytest.runner.TResult]
        :param when:
            The phase in which the function is called.
        :param reraise:
            Exception or exceptions that shall propagate if raised by the
            function, instead of being wrapped in the CallInfo.
        """
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result: TResult | None = func()

../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:341: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/runner.py:242: in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_hooks.py:513: in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pluggy/_manager.py:120: in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:92: in pytest_runtest_call
    yield from thread_exception_runtest_hook()
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/threadexception.py:68: in thread_exception_runtest_hook
    yield
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:95: in pytest_runtest_call
    yield from unraisable_exception_runtest_hook()
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/unraisableexception.py:70: in unraisable_exception_runtest_hook
    yield
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:846: in pytest_runtest_call
    yield from self._runtest_for(item, "call")
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/logging.py:829: in _runtest_for
    yield
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io....xtIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>
item = <Function test_node_capability_matching>

    @hookimpl(wrapper=True)
    def pytest_runtest_call(self, item: Item) -> Generator[None]:
        with self.item_capture("call", item):
>           return (yield)
E           RuntimeError: generator raised StopIteration

../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/_pytest/capture.py:898: RuntimeError
______________________ test_specialized_vs_general_nodes _______________________

specialization_manager = <render_farm_manager.node_specialization.specialization_manager.NodeSpecializationManager object at 0x7f489e2dc7f0>
jobs = [RenderJob(id='animation-job', name='Animation Job', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.HIGH: ...alse, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]
nodes = [RenderNode(id='gpu-node-0', name='GPU Node 0', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, g...rformance_history={'job_type:simulation': 1.7, 'job_type:compositing': 1.5}, last_error=None, uptime_hours=900.0), ...]

    def test_specialized_vs_general_nodes(specialization_manager, jobs, nodes):
        """Test specialized nodes are preferred over general-purpose nodes."""
        compositing_job = jobs[4]  # Compositing job
    
        # Add a general-purpose node
        general_node = RenderNode(
            id="general-node",
            name="General Purpose Node",
            status="online",
            capabilities=NodeCapabilities(
                cpu_cores=32,
                memory_gb=128,
                gpu_model="NVIDIA RTX A4000",
                gpu_count=1,
                gpu_memory_gb=16,
                gpu_compute_capability=8.6,
                storage_gb=2000,
                specialized_for=[],  # No specialization
            ),
            power_efficiency_rating=70,
            current_job_id=None,
            performance_history={},
            last_error=None,
            uptime_hours=500,
        )
    
        modified_nodes = nodes + [general_node]
    
        # Compositing job should prefer CPU node over general node
        matched_node_id = specialization_manager.match_job_to_node(compositing_job, modified_nodes)
    
>       assert matched_node_id.startswith("cpu-node")
E       AttributeError: 'NoneType' object has no attribute 'startswith'

tests/render_farm_manager/unit/test_node_specialization.py:441: AttributeError
_________________________ test_process_pending_outputs _________________________

progressive_renderer = <render_farm_manager.progressive_result.progressive_renderer.ProgressiveRenderer object at 0x7f489dbea740>
jobs = [RenderJob(id='long-job', name='Long Running Job', status=<TaskStatus.RUNNING: 'running'>, priority=<Priority.HIGH: 3>...True, supports_checkpoint=False, last_checkpoint_time=None, last_progressive_output_time=None, energy_intensive=False)]

    def test_process_pending_outputs(progressive_renderer, jobs):
        """Test processing pending progressive outputs."""
        # Set up some scheduled outputs for the running jobs
        now = datetime.now()
    
        # Add scheduled outputs in the past for the long job
        progressive_renderer.scheduled_outputs["long-job"] = [
            now - timedelta(minutes=30),
            now - timedelta(minutes=15),
            now + timedelta(minutes=15),
        ]
    
        # Add scheduled outputs in the past for the nearly complete job
        progressive_renderer.scheduled_outputs["almost-done-job"] = [
            now - timedelta(minutes=45),
        ]
    
        # Process pending outputs
        processed_outputs = progressive_renderer.process_pending_outputs(jobs)
    
        # Should have processed outputs for the long job and almost done job
>       assert "long-job" in processed_outputs
E       AssertionError: assert 'long-job' in {}

tests/render_farm_manager/unit/test_progressive_renderer.py:314: AssertionError
________________ TestDependencyTracker.test_update_stage_status ________________

self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestDependencyTracker object at 0x7f49519277f0>

    def test_update_stage_status(self):
        """Test updating a stage's status and propagating changes."""
        # Create dependency graph
        self.tracker.create_dependency_graph(self.simulation)
    
        # Update stage-1 to completed
>       result = self.tracker.update_stage_status(
            self.simulation,
            "stage-1",
            SimulationStageStatus.COMPLETED,
        )

tests/scientific_computing/dependency_tracking/test_dependency_tracker.py:357: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
concurrent_task_scheduler/dependency_tracking/tracker.py:289: in update_stage_status
    stage.end_time = now
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:995: in __setattr__
    elif (setattr_handler := self._setattr_handler(name, value)) is not None:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = SimulationStage(id='stage-1', name='Stage 1', status=<SimulationStageStatus.COMPLETED: 'completed'>, priority=<Priorit...kpoint_frequency=datetime.timedelta(seconds=3600), last_checkpoint_time=None, checkpoint_path=None, error_message=None)
name = 'end_time', value = datetime.datetime(2025, 6, 16, 4, 16, 45, 434589)

    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:
        """Get a handler for setting an attribute on the model instance.
    
        Returns:
            A handler for setting an attribute on the model instance. Used for memoization of the handler.
            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`
            Returns `None` when memoization is not safe, then the attribute is set directly.
        """
        cls = self.__class__
        if name in cls.__class_vars__:
            raise AttributeError(
                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '
                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'
            )
        elif not _fields.is_valid_field_name(name):
            if (attribute := cls.__private_attributes__.get(name)) is not None:
                if hasattr(attribute, '__set__'):
                    return lambda model, _name, val: attribute.__set__(model, val)
                else:
                    return _SIMPLE_SETATTR_HANDLERS['private']
            else:
                _object_setattr(self, name, value)
                return None  # Can not return memoized handler with possibly freeform attr names
    
        attr = getattr(cls, name, None)
        # NOTE: We currently special case properties and `cached_property`, but we might need
        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors
        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value
        # to the instance's `__dict__`, but other non-data descriptors might do things differently.
        if isinstance(attr, cached_property):
            return _SIMPLE_SETATTR_HANDLERS['cached_property']
    
        _check_frozen(cls, name, value)
    
        # We allow properties to be set only on non frozen models for now (to match dataclasses).
        # This can be changed if it ever gets requested.
        if isinstance(attr, property):
            return lambda model, _name, val: attr.__set__(model, val)
        elif cls.model_config.get('validate_assignment'):
            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']
        elif name not in cls.__pydantic_fields__:
            if cls.model_config.get('extra') != 'allow':
                # TODO - matching error
>               raise ValueError(f'"{cls.__name__}" object has no field "{name}"')
E               ValueError: "SimulationStage" object has no field "end_time"

../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:1042: ValueError
__________________ TestDependencyTracker.test_is_stage_ready ___________________

self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestDependencyTracker object at 0x7f4951927b50>

    def test_is_stage_ready(self):
        """Test checking if a stage is ready to execute."""
        # Create dependency graph
        self.tracker.create_dependency_graph(self.simulation)
    
        # Stage-1 should be ready
>       assert self.tracker.is_stage_ready(self.simulation, "stage-1")
E       AssertionError: assert False
E        +  where False = is_stage_ready(Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage-1': SimulationStage(id='stage-1',...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1)), 'stage-1')
E        +    where is_stage_ready = <concurrent_task_scheduler.dependency_tracking.tracker.DependencyTracker object at 0x7f489dbebac0>.is_stage_ready
E        +      where <concurrent_task_scheduler.dependency_tracking.tracker.DependencyTracker object at 0x7f489dbebac0> = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestDependencyTracker object at 0x7f4951927b50>.tracker
E        +    and   Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage-1': SimulationStage(id='stage-1',...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1)) = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestDependencyTracker object at 0x7f4951927b50>.simulation

tests/scientific_computing/dependency_tracking/test_dependency_tracker.py:399: AssertionError
______________ TestDependencyTracker.test_add_dynamic_dependency _______________

self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestDependencyTracker object at 0x7f4951970190>

    def test_add_dynamic_dependency(self):
        """Test adding a dynamically discovered dependency at runtime."""
        # Create dependency graph
        self.tracker.create_dependency_graph(self.simulation)
    
        # Add a dynamic dependency from stage-1 to stage-3
>       result = self.tracker.add_dynamic_dependency(
            self.simulation,
            "stage-1",
            "stage-3",
            DependencyType.DATA,
        )

tests/scientific_computing/dependency_tracking/test_dependency_tracker.py:471: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <concurrent_task_scheduler.dependency_tracking.tracker.DependencyTracker object at 0x7f489e216380>
simulation = Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage-1': SimulationStage(id='stage-1',...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))
from_stage_id = 'stage-1', to_stage_id = 'stage-3'
dependency_type = <DependencyType.DATA: 'data'>, condition = None

    def add_dynamic_dependency(
        self,
        simulation: Simulation,
        from_stage_id: str,
        to_stage_id: str,
        dependency_type: DependencyType = DependencyType.DATA,
        condition: Optional[str] = None,
    ) -> Result[bool]:
        """Add a dynamically discovered dependency at runtime."""
        # Validate stages
        if from_stage_id not in simulation.stages:
            return Result.err(f"Stage {from_stage_id} not found in simulation {simulation.id}")
    
        if to_stage_id not in simulation.stages:
            return Result.err(f"Stage {to_stage_id} not found in simulation {simulation.id}")
    
        # Add to simulation's stage dependencies
        to_stage = simulation.stages[to_stage_id]
        if from_stage_id not in to_stage.dependencies:
>           to_stage.dependencies.add(from_stage_id)
E           AttributeError: 'list' object has no attribute 'add'

concurrent_task_scheduler/dependency_tracking/tracker.py:491: AttributeError
________________ TestDependencyTracker.test_validate_simulation ________________

self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestDependencyTracker object at 0x7f49519704f0>

    def test_validate_simulation(self):
        """Test validating the simulation dependency structure."""
        # The test simulation should be valid
        errors = self.tracker.validate_simulation(self.simulation)
        assert not errors
    
        # Add a dependency to a non-existent stage
>       self.simulation.stages["stage-2"].dependencies.add("non-existent-stage")
E       AttributeError: 'list' object has no attribute 'add'

tests/scientific_computing/dependency_tracking/test_dependency_tracker.py:514: AttributeError
___________________ TestWorkflowManager.test_create_instance ___________________

self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestWorkflowManager object at 0x7f4951970ac0>

    def test_create_instance(self):
        """Test creating a workflow instance from a template."""
        # Create a simple sequential template
        template = WorkflowTemplate.create_sequential(
            name="Test Sequential Workflow",
            stage_names=["Stage 1", "Stage 2", "Stage 3"],
        )
    
        # Register the template
        self.manager.register_template(template)
    
        # Create an instance
>       result = self.manager.create_instance(template.id, self.simulation)

tests/scientific_computing/dependency_tracking/test_dependency_tracker.py:568: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <concurrent_task_scheduler.dependency_tracking.workflow.WorkflowManager object at 0x7f489e022c20>
template_id = 'wf_template_20250616041645_e936338217241904'
simulation = Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage_20250616041645_4386962402cbcfcb':...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))
parameters = None

    def create_instance(
        self,
        template_id: str,
        simulation: Simulation,
        parameters: Optional[Dict[str, Any]] = None,
    ) -> Result[WorkflowInstance]:
        """Create a workflow instance from a template."""
        if template_id not in self.templates:
            return Result.err(f"Template {template_id} not found")
    
        template = self.templates[template_id]
    
        # Create instance
        instance_id = generate_id("wf_instance")
        instance = WorkflowInstance(
            instance_id=instance_id,
            template=template,
            simulation=simulation,
        )
    
        if parameters:
            instance.parameters = parameters.copy()
    
        # Map stages
        for template_stage_id, template_stage in template.stages.items():
            # Check if stage already exists in simulation
            existing_stage = None
            for stage_id, stage in simulation.stages.items():
                if stage.name == template_stage.name:
                    existing_stage = stage_id
                    break
    
            if existing_stage:
                # Use existing stage
                instance.map_stage(template_stage_id, existing_stage)
            else:
                # Create new stage
                stage_id = generate_id("stage")
    
                # Convert from timedelta to hours for estimated_duration
                estimated_duration_hours = template_stage.estimated_duration.total_seconds() / 3600
    
                # Create stage in simulation
                stage = SimulationStage(
                    id=stage_id,
                    name=template_stage.name,
                    description=template_stage.description,
                    estimated_duration=timedelta(hours=estimated_duration_hours),
                    resource_requirements=[],  # Would need conversion from template format
                    status=SimulationStageStatus.PENDING,
                )
    
                simulation.stages[stage_id] = stage
                instance.map_stage(template_stage_id, stage_id)
    
        # Set up dependencies based on template transitions
        for transition in template.transitions:
            from_sim_stage = instance.get_simulation_stage_id(transition.from_stage_id)
            to_sim_stage = instance.get_simulation_stage_id(transition.to_stage_id)
    
            if from_sim_stage and to_sim_stage:
                # Add dependency in simulation
>               simulation.stages[to_sim_stage].dependencies.add(from_sim_stage)
E               AttributeError: 'list' object has no attribute 'add'

concurrent_task_scheduler/dependency_tracking/workflow.py:665: AttributeError
_______________ TestWorkflowManager.test_create_linear_workflow ________________

self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestWorkflowManager object at 0x7f4951970c70>

    def test_create_linear_workflow(self):
        """Test creating a simple linear workflow."""
        # Create a linear workflow
>       result = self.manager.create_linear_workflow(
            self.simulation,
            ["Stage A", "Stage B", "Stage C"],
        )

tests/scientific_computing/dependency_tracking/test_dependency_tracker.py:599: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
concurrent_task_scheduler/dependency_tracking/workflow.py:857: in create_linear_workflow
    instance_result = self.create_instance(template.id, simulation)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <concurrent_task_scheduler.dependency_tracking.workflow.WorkflowManager object at 0x7f489e2a8700>
template_id = 'wf_template_20250616041645_f8177b9e962497d6'
simulation = Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage_20250616041645_c4a49a80307ed389':...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))
parameters = None

    def create_instance(
        self,
        template_id: str,
        simulation: Simulation,
        parameters: Optional[Dict[str, Any]] = None,
    ) -> Result[WorkflowInstance]:
        """Create a workflow instance from a template."""
        if template_id not in self.templates:
            return Result.err(f"Template {template_id} not found")
    
        template = self.templates[template_id]
    
        # Create instance
        instance_id = generate_id("wf_instance")
        instance = WorkflowInstance(
            instance_id=instance_id,
            template=template,
            simulation=simulation,
        )
    
        if parameters:
            instance.parameters = parameters.copy()
    
        # Map stages
        for template_stage_id, template_stage in template.stages.items():
            # Check if stage already exists in simulation
            existing_stage = None
            for stage_id, stage in simulation.stages.items():
                if stage.name == template_stage.name:
                    existing_stage = stage_id
                    break
    
            if existing_stage:
                # Use existing stage
                instance.map_stage(template_stage_id, existing_stage)
            else:
                # Create new stage
                stage_id = generate_id("stage")
    
                # Convert from timedelta to hours for estimated_duration
                estimated_duration_hours = template_stage.estimated_duration.total_seconds() / 3600
    
                # Create stage in simulation
                stage = SimulationStage(
                    id=stage_id,
                    name=template_stage.name,
                    description=template_stage.description,
                    estimated_duration=timedelta(hours=estimated_duration_hours),
                    resource_requirements=[],  # Would need conversion from template format
                    status=SimulationStageStatus.PENDING,
                )
    
                simulation.stages[stage_id] = stage
                instance.map_stage(template_stage_id, stage_id)
    
        # Set up dependencies based on template transitions
        for transition in template.transitions:
            from_sim_stage = instance.get_simulation_stage_id(transition.from_stage_id)
            to_sim_stage = instance.get_simulation_stage_id(transition.to_stage_id)
    
            if from_sim_stage and to_sim_stage:
                # Add dependency in simulation
>               simulation.stages[to_sim_stage].dependencies.add(from_sim_stage)
E               AttributeError: 'list' object has no attribute 'add'

concurrent_task_scheduler/dependency_tracking/workflow.py:665: AttributeError
______________ TestWorkflowManager.test_create_parallel_workflow _______________

self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestWorkflowManager object at 0x7f4951970e20>

    def test_create_parallel_workflow(self):
        """Test creating a simple parallel workflow."""
        # Create a parallel workflow
>       result = self.manager.create_parallel_workflow(
            self.simulation,
            ["Stage X", "Stage Y", "Stage Z"],
        )

tests/scientific_computing/dependency_tracking/test_dependency_tracker.py:640: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
concurrent_task_scheduler/dependency_tracking/workflow.py:925: in create_parallel_workflow
    return self.create_instance(template.id, simulation)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <concurrent_task_scheduler.dependency_tracking.workflow.WorkflowManager object at 0x7f489dd1de10>
template_id = 'wf_template_20250616041645_98cf629cb748e58d'
simulation = Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage_20250616041645_383008864c7e1c59':...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))
parameters = None

    def create_instance(
        self,
        template_id: str,
        simulation: Simulation,
        parameters: Optional[Dict[str, Any]] = None,
    ) -> Result[WorkflowInstance]:
        """Create a workflow instance from a template."""
        if template_id not in self.templates:
            return Result.err(f"Template {template_id} not found")
    
        template = self.templates[template_id]
    
        # Create instance
        instance_id = generate_id("wf_instance")
        instance = WorkflowInstance(
            instance_id=instance_id,
            template=template,
            simulation=simulation,
        )
    
        if parameters:
            instance.parameters = parameters.copy()
    
        # Map stages
        for template_stage_id, template_stage in template.stages.items():
            # Check if stage already exists in simulation
            existing_stage = None
            for stage_id, stage in simulation.stages.items():
                if stage.name == template_stage.name:
                    existing_stage = stage_id
                    break
    
            if existing_stage:
                # Use existing stage
                instance.map_stage(template_stage_id, existing_stage)
            else:
                # Create new stage
                stage_id = generate_id("stage")
    
                # Convert from timedelta to hours for estimated_duration
                estimated_duration_hours = template_stage.estimated_duration.total_seconds() / 3600
    
                # Create stage in simulation
                stage = SimulationStage(
                    id=stage_id,
                    name=template_stage.name,
                    description=template_stage.description,
                    estimated_duration=timedelta(hours=estimated_duration_hours),
                    resource_requirements=[],  # Would need conversion from template format
                    status=SimulationStageStatus.PENDING,
                )
    
                simulation.stages[stage_id] = stage
                instance.map_stage(template_stage_id, stage_id)
    
        # Set up dependencies based on template transitions
        for transition in template.transitions:
            from_sim_stage = instance.get_simulation_stage_id(transition.from_stage_id)
            to_sim_stage = instance.get_simulation_stage_id(transition.to_stage_id)
    
            if from_sim_stage and to_sim_stage:
                # Add dependency in simulation
>               simulation.stages[to_sim_stage].dependencies.add(from_sim_stage)
E               AttributeError: 'list' object has no attribute 'add'

concurrent_task_scheduler/dependency_tracking/workflow.py:665: AttributeError
___________________ TestWorkflowManager.test_get_next_stages ___________________

self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestWorkflowManager object at 0x7f4951927ee0>

    def test_get_next_stages(self):
        """Test getting the next stages to execute in a workflow instance."""
        # Create a linear workflow
>       result = self.manager.create_linear_workflow(
            self.simulation,
            ["Stage 1", "Stage 2", "Stage 3"],
        )

tests/scientific_computing/dependency_tracking/test_dependency_tracker.py:677: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
concurrent_task_scheduler/dependency_tracking/workflow.py:857: in create_linear_workflow
    instance_result = self.create_instance(template.id, simulation)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <concurrent_task_scheduler.dependency_tracking.workflow.WorkflowManager object at 0x7f49f43e7670>
template_id = 'wf_template_20250616041645_6f9ee36f5e1407d5'
simulation = Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage_20250616041645_cc9b998e9d4c9622':...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))
parameters = None

    def create_instance(
        self,
        template_id: str,
        simulation: Simulation,
        parameters: Optional[Dict[str, Any]] = None,
    ) -> Result[WorkflowInstance]:
        """Create a workflow instance from a template."""
        if template_id not in self.templates:
            return Result.err(f"Template {template_id} not found")
    
        template = self.templates[template_id]
    
        # Create instance
        instance_id = generate_id("wf_instance")
        instance = WorkflowInstance(
            instance_id=instance_id,
            template=template,
            simulation=simulation,
        )
    
        if parameters:
            instance.parameters = parameters.copy()
    
        # Map stages
        for template_stage_id, template_stage in template.stages.items():
            # Check if stage already exists in simulation
            existing_stage = None
            for stage_id, stage in simulation.stages.items():
                if stage.name == template_stage.name:
                    existing_stage = stage_id
                    break
    
            if existing_stage:
                # Use existing stage
                instance.map_stage(template_stage_id, existing_stage)
            else:
                # Create new stage
                stage_id = generate_id("stage")
    
                # Convert from timedelta to hours for estimated_duration
                estimated_duration_hours = template_stage.estimated_duration.total_seconds() / 3600
    
                # Create stage in simulation
                stage = SimulationStage(
                    id=stage_id,
                    name=template_stage.name,
                    description=template_stage.description,
                    estimated_duration=timedelta(hours=estimated_duration_hours),
                    resource_requirements=[],  # Would need conversion from template format
                    status=SimulationStageStatus.PENDING,
                )
    
                simulation.stages[stage_id] = stage
                instance.map_stage(template_stage_id, stage_id)
    
        # Set up dependencies based on template transitions
        for transition in template.transitions:
            from_sim_stage = instance.get_simulation_stage_id(transition.from_stage_id)
            to_sim_stage = instance.get_simulation_stage_id(transition.to_stage_id)
    
            if from_sim_stage and to_sim_stage:
                # Add dependency in simulation
>               simulation.stages[to_sim_stage].dependencies.add(from_sim_stage)
E               AttributeError: 'list' object has no attribute 'add'

concurrent_task_scheduler/dependency_tracking/workflow.py:665: AttributeError
___________________ TestWorkflowManager.test_update_instance ___________________

self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestWorkflowManager object at 0x7f4951927460>

    def test_update_instance(self):
        """Test updating a stage status in a workflow instance."""
        # Create a linear workflow
>       result = self.manager.create_linear_workflow(
            self.simulation,
            ["Stage 1", "Stage 2", "Stage 3"],
        )

tests/scientific_computing/dependency_tracking/test_dependency_tracker.py:730: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
concurrent_task_scheduler/dependency_tracking/workflow.py:857: in create_linear_workflow
    instance_result = self.create_instance(template.id, simulation)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <concurrent_task_scheduler.dependency_tracking.workflow.WorkflowManager object at 0x7f489dc26bc0>
template_id = 'wf_template_20250616041645_67288c114aa573e6'
simulation = Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage_20250616041645_3c3986dafad943be':...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))
parameters = None

    def create_instance(
        self,
        template_id: str,
        simulation: Simulation,
        parameters: Optional[Dict[str, Any]] = None,
    ) -> Result[WorkflowInstance]:
        """Create a workflow instance from a template."""
        if template_id not in self.templates:
            return Result.err(f"Template {template_id} not found")
    
        template = self.templates[template_id]
    
        # Create instance
        instance_id = generate_id("wf_instance")
        instance = WorkflowInstance(
            instance_id=instance_id,
            template=template,
            simulation=simulation,
        )
    
        if parameters:
            instance.parameters = parameters.copy()
    
        # Map stages
        for template_stage_id, template_stage in template.stages.items():
            # Check if stage already exists in simulation
            existing_stage = None
            for stage_id, stage in simulation.stages.items():
                if stage.name == template_stage.name:
                    existing_stage = stage_id
                    break
    
            if existing_stage:
                # Use existing stage
                instance.map_stage(template_stage_id, existing_stage)
            else:
                # Create new stage
                stage_id = generate_id("stage")
    
                # Convert from timedelta to hours for estimated_duration
                estimated_duration_hours = template_stage.estimated_duration.total_seconds() / 3600
    
                # Create stage in simulation
                stage = SimulationStage(
                    id=stage_id,
                    name=template_stage.name,
                    description=template_stage.description,
                    estimated_duration=timedelta(hours=estimated_duration_hours),
                    resource_requirements=[],  # Would need conversion from template format
                    status=SimulationStageStatus.PENDING,
                )
    
                simulation.stages[stage_id] = stage
                instance.map_stage(template_stage_id, stage_id)
    
        # Set up dependencies based on template transitions
        for transition in template.transitions:
            from_sim_stage = instance.get_simulation_stage_id(transition.from_stage_id)
            to_sim_stage = instance.get_simulation_stage_id(transition.to_stage_id)
    
            if from_sim_stage and to_sim_stage:
                # Add dependency in simulation
>               simulation.stages[to_sim_stage].dependencies.add(from_sim_stage)
E               AttributeError: 'list' object has no attribute 'add'

concurrent_task_scheduler/dependency_tracking/workflow.py:665: AttributeError
_________________ TestWorkflowManager.test_get_workflow_status _________________

self = <tests.scientific_computing.dependency_tracking.test_dependency_tracker.TestWorkflowManager object at 0x7f4951926740>

    def test_get_workflow_status(self):
        """Test getting the current status of a workflow instance."""
        # Create a linear workflow
>       result = self.manager.create_linear_workflow(
            self.simulation,
            ["Stage 1", "Stage 2", "Stage 3"],
        )

tests/scientific_computing/dependency_tracking/test_dependency_tracker.py:785: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
concurrent_task_scheduler/dependency_tracking/workflow.py:857: in create_linear_workflow
    instance_result = self.create_instance(template.id, simulation)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <concurrent_task_scheduler.dependency_tracking.workflow.WorkflowManager object at 0x7f489e139f60>
template_id = 'wf_template_20250616041645_17fe7b6643d4951a'
simulation = Simulation(id='sim-test-1', name='Test Simulation', description=None, stages={'stage_20250616041645_4d96bd7321fc5ea3':...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))
parameters = None

    def create_instance(
        self,
        template_id: str,
        simulation: Simulation,
        parameters: Optional[Dict[str, Any]] = None,
    ) -> Result[WorkflowInstance]:
        """Create a workflow instance from a template."""
        if template_id not in self.templates:
            return Result.err(f"Template {template_id} not found")
    
        template = self.templates[template_id]
    
        # Create instance
        instance_id = generate_id("wf_instance")
        instance = WorkflowInstance(
            instance_id=instance_id,
            template=template,
            simulation=simulation,
        )
    
        if parameters:
            instance.parameters = parameters.copy()
    
        # Map stages
        for template_stage_id, template_stage in template.stages.items():
            # Check if stage already exists in simulation
            existing_stage = None
            for stage_id, stage in simulation.stages.items():
                if stage.name == template_stage.name:
                    existing_stage = stage_id
                    break
    
            if existing_stage:
                # Use existing stage
                instance.map_stage(template_stage_id, existing_stage)
            else:
                # Create new stage
                stage_id = generate_id("stage")
    
                # Convert from timedelta to hours for estimated_duration
                estimated_duration_hours = template_stage.estimated_duration.total_seconds() / 3600
    
                # Create stage in simulation
                stage = SimulationStage(
                    id=stage_id,
                    name=template_stage.name,
                    description=template_stage.description,
                    estimated_duration=timedelta(hours=estimated_duration_hours),
                    resource_requirements=[],  # Would need conversion from template format
                    status=SimulationStageStatus.PENDING,
                )
    
                simulation.stages[stage_id] = stage
                instance.map_stage(template_stage_id, stage_id)
    
        # Set up dependencies based on template transitions
        for transition in template.transitions:
            from_sim_stage = instance.get_simulation_stage_id(transition.from_stage_id)
            to_sim_stage = instance.get_simulation_stage_id(transition.to_stage_id)
    
            if from_sim_stage and to_sim_stage:
                # Add dependency in simulation
>               simulation.stages[to_sim_stage].dependencies.add(from_sim_stage)
E               AttributeError: 'list' object has no attribute 'add'

concurrent_task_scheduler/dependency_tracking/workflow.py:665: AttributeError
____________________________ test_check_node_health ____________________________

failure_detector = <concurrent_task_scheduler.failure_resilience.failure_detector.FailureDetector object at 0x7f489dc300d0>
sample_compute_node = ComputeNode(id='node_001', name='Compute Node 1', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0,...igned_simulations=[], last_failure_time=None, maintenance_window=None, location='data_center_1', reliability_score=1.0)

    def test_check_node_health(failure_detector, sample_compute_node):
        """Test checking the health of a node."""
        # First record a heartbeat so it's healthy
        failure_detector.record_heartbeat(sample_compute_node.id)
    
        # Check health
        health_check = failure_detector.check_node_health(sample_compute_node)
    
        assert isinstance(health_check, NodeHealthCheck)
        assert health_check.node_id == sample_compute_node.id
        assert health_check.status == sample_compute_node.status
        assert isinstance(health_check.metrics, dict)
        assert "cpu_load" in health_check.metrics
        assert "memory_usage" in health_check.metrics
        assert "disk_usage" in health_check.metrics
>       assert health_check.is_healthy()
E       assert False
E        +  where False = is_healthy()
E        +    where is_healthy = <concurrent_task_scheduler.failure_resilience.failure_detector.NodeHealthCheck object at 0x7f489dc30640>.is_healthy

tests/scientific_computing/failure_resilience/test_failure_detector.py:146: AssertionError
___________________________ test_reliability_metrics ___________________________

failure_detector = <concurrent_task_scheduler.failure_resilience.failure_detector.FailureDetector object at 0x7f489dd71f30>
sample_compute_node = ComputeNode(id='node_001', name='Compute Node 1', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0,...igned_simulations=[], last_failure_time=None, maintenance_window=None, location='data_center_1', reliability_score=1.0)

    def test_reliability_metrics(failure_detector, sample_compute_node):
        """Test reliability metrics calculation."""
        # Record some health checks
        node_id = sample_compute_node.id
    
        # Record healthy checks
        for _ in range(5):
            failure_detector.record_heartbeat(node_id)
            health_check = failure_detector.check_node_health(sample_compute_node)
>           assert health_check.is_healthy()
E           assert False
E            +  where False = is_healthy()
E            +    where is_healthy = <concurrent_task_scheduler.failure_resilience.failure_detector.NodeHealthCheck object at 0x7f489dd73310>.is_healthy

tests/scientific_computing/failure_resilience/test_failure_detector.py:368: AssertionError
__________________ TestFailureDetector.test_check_node_health __________________

self = <tests.scientific_computing.failure_resilience.test_failure_resilience.TestFailureDetector object at 0x7f49517d7520>
failure_detector = <concurrent_task_scheduler.failure_resilience.failure_detector.FailureDetector object at 0x7f489e49af20>
mock_node = ComputeNode(id='node-123', name='compute-123', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, gp...igned_simulations=[], last_failure_time=None, maintenance_window=None, location='data_center_1', reliability_score=1.0)

    def test_check_node_health(self, failure_detector, mock_node):
        """Test checking node health."""
        # Record a heartbeat first
        failure_detector.record_heartbeat(mock_node.id)
    
        # Check health
        health_check = failure_detector.check_node_health(mock_node)
    
        assert health_check.node_id == mock_node.id
>       assert health_check.status == NodeStatus.ONLINE
E       AssertionError: assert <NodeStatus.ONLINE: 'online'> == <NodeStatus.ONLINE: 'online'>
E        +  where <NodeStatus.ONLINE: 'online'> = <concurrent_task_scheduler.failure_resilience.failure_detector.NodeHealthCheck object at 0x7f489e499090>.status
E        +  and   <NodeStatus.ONLINE: 'online'> = NodeStatus.ONLINE

tests/scientific_computing/failure_resilience/test_failure_resilience.py:125: AssertionError
________________ TestFailureDetector.test_detect_node_failures _________________

self = <tests.scientific_computing.failure_resilience.test_failure_resilience.TestFailureDetector object at 0x7f49517b6bc0>
failure_detector = <concurrent_task_scheduler.failure_resilience.failure_detector.FailureDetector object at 0x7f489dfa2f20>
mock_node = ComputeNode(id='node-123', name='compute-123', status=<NodeStatus.ONLINE: 'online'>, cpu_cores=32, memory_gb=128.0, gp...igned_simulations=[], last_failure_time=None, maintenance_window=None, location='data_center_1', reliability_score=1.0)

    def test_detect_node_failures(self, failure_detector, mock_node):
        """Test detecting node failures."""
        # Create mock nodes
        nodes = {
            "node-123": mock_node,
            "problem-node": ComputeNode(
                id="problem-node",
                name="problem",
                status=NodeStatus.ONLINE,
                # Set up with critical disk usage to trigger failure
                current_load={"cpu": 0.5, "memory": 0.5, "storage": 0.99},
                cpu_cores=32,
                memory_gb=128,
                storage_gb=1000,
                node_type=NodeType.COMPUTE,
                gpu_count=0,
                network_bandwidth_gbps=10,
                location="data_center_1"
            ),
        }
    
        # Record heartbeats
        for node_id in nodes:
            failure_detector.record_heartbeat(node_id)
    
        # Detect failures
        failures = failure_detector.detect_node_failures(nodes)
    
        # Should detect one failure (high disk usage)
>       assert len(failures) == 1
E       assert 0 == 1
E        +  where 0 = len([])

tests/scientific_computing/failure_resilience/test_failure_resilience.py:251: AssertionError
_______________ TestLongRunningJobManager.test_submit_simulation _______________

self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951880df0>

    def test_submit_simulation(self):
        """Test submitting a simulation to the job manager."""
        # Reset the simulation to DEFINED status
        self.simulation.status = SimulationStatus.DEFINED
        for stage in self.simulation.stages.values():
            stage.status = SimulationStageStatus.PENDING
>           stage.start_time = None

tests/scientific_computing/job_management/test_long_running_job_manager.py:182: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:995: in __setattr__
    elif (setattr_handler := self._setattr_handler(name, value)) is not None:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = SimulationStage(id='stage-1', name='Preprocessing', status=<SimulationStageStatus.PENDING: 'pending'>, priority=<Prior...kpoint_frequency=datetime.timedelta(seconds=3600), last_checkpoint_time=None, checkpoint_path=None, error_message=None)
name = 'start_time', value = None

    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:
        """Get a handler for setting an attribute on the model instance.
    
        Returns:
            A handler for setting an attribute on the model instance. Used for memoization of the handler.
            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`
            Returns `None` when memoization is not safe, then the attribute is set directly.
        """
        cls = self.__class__
        if name in cls.__class_vars__:
            raise AttributeError(
                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '
                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'
            )
        elif not _fields.is_valid_field_name(name):
            if (attribute := cls.__private_attributes__.get(name)) is not None:
                if hasattr(attribute, '__set__'):
                    return lambda model, _name, val: attribute.__set__(model, val)
                else:
                    return _SIMPLE_SETATTR_HANDLERS['private']
            else:
                _object_setattr(self, name, value)
                return None  # Can not return memoized handler with possibly freeform attr names
    
        attr = getattr(cls, name, None)
        # NOTE: We currently special case properties and `cached_property`, but we might need
        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors
        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value
        # to the instance's `__dict__`, but other non-data descriptors might do things differently.
        if isinstance(attr, cached_property):
            return _SIMPLE_SETATTR_HANDLERS['cached_property']
    
        _check_frozen(cls, name, value)
    
        # We allow properties to be set only on non frozen models for now (to match dataclasses).
        # This can be changed if it ever gets requested.
        if isinstance(attr, property):
            return lambda model, _name, val: attr.__set__(model, val)
        elif cls.model_config.get('validate_assignment'):
            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']
        elif name not in cls.__pydantic_fields__:
            if cls.model_config.get('extra') != 'allow':
                # TODO - matching error
>               raise ValueError(f'"{cls.__name__}" object has no field "{name}"')
E               ValueError: "SimulationStage" object has no field "start_time"

../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/main.py:1042: ValueError
_______________ TestLongRunningJobManager.test_processing_queue ________________

self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881150>

    def test_processing_queue(self):
        """Test processing the queue of simulations."""
        # Submit a simulation
        result = self.job_manager.submit_simulation(self.simulation)
        assert result.success
    
        # Force process queue (normally called internally)
        self.job_manager._process_queue()
    
        # Verify simulation moved from queued to running
>       assert self.simulation.id not in self.job_manager.queued_simulations
E       AssertionError: assert 'sim-test-1' not in {'sim-test-1': Simulation(id='sim-test-1', name='Climate Model Test', description='Test simulation of climate model', ...ulation'], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=6))}
E        +  where 'sim-test-1' = Simulation(id='sim-test-1', name='Climate Model Test', description='Test simulation of climate model', stages={'stage-...mulation'], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=6)).id
E        +    where Simulation(id='sim-test-1', name='Climate Model Test', description='Test simulation of climate model', stages={'stage-...mulation'], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=6)) = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881150>.simulation
E        +  and   {'sim-test-1': Simulation(id='sim-test-1', name='Climate Model Test', description='Test simulation of climate model', ...ulation'], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=6))} = <concurrent_task_scheduler.job_management.scheduler.LongRunningJobManager object at 0x7f489e215a80>.queued_simulations
E        +    where <concurrent_task_scheduler.job_management.scheduler.LongRunningJobManager object at 0x7f489e215a80> = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881150>.job_manager

tests/scientific_computing/job_management/test_long_running_job_manager.py:226: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
__________ TestLongRunningJobManager.test_pause_and_resume_simulation __________

self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951880ac0>

    def test_pause_and_resume_simulation(self):
        """Test pausing and resuming a simulation."""
        # Submit and process
        self.job_manager.submit_simulation(self.simulation)
        self.job_manager._process_queue()
    
        # Pause the simulation
        result = self.job_manager.pause_simulation(self.simulation.id)
>       assert result.success
E       AssertionError: assert False
E        +  where False = Result(success=False, value=None, error='Simulation sim-test-1 not found or not running').success

tests/scientific_computing/job_management/test_long_running_job_manager.py:244: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
_________ TestLongRunningJobManager.test_simulation_stage_transitions __________

self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951880280>

    def test_simulation_stage_transitions(self):
        """Test simulation stage transitions based on dependencies."""
        # Submit and process
        self.job_manager.submit_simulation(self.simulation)
        self.job_manager._process_queue()
    
        # Initially, only the first stage should be running
>       assert self.simulation.stages["stage-1"].status == SimulationStageStatus.RUNNING
E       AssertionError: assert <TaskStatus.PENDING: 'pending'> == <SimulationStageStatus.RUNNING: 'running'>
E        +  where <TaskStatus.PENDING: 'pending'> = SimulationStage(id='stage-1', name='Preprocessing', status=<TaskStatus.PENDING: 'pending'>, priority=<Priority.MEDIUM:...kpoint_frequency=datetime.timedelta(seconds=3600), last_checkpoint_time=None, checkpoint_path=None, error_message=None).status
E        +  and   <SimulationStageStatus.RUNNING: 'running'> = SimulationStageStatus.RUNNING

tests/scientific_computing/job_management/test_long_running_job_manager.py:264: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
______________ TestLongRunningJobManager.test_handle_node_failure ______________

self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881240>

    def test_handle_node_failure(self):
        """Test handling of node failures."""
        # Submit and process
        self.job_manager.submit_simulation(self.simulation)
        self.job_manager._process_queue()
    
        # Mark a node as offline
        affected_node = self.nodes[0]
        affected_node.assigned_simulations = [self.simulation.id]
    
        # Update node status
        self.job_manager.update_node_status(affected_node.id, NodeStatus.OFFLINE)
    
        # Verify simulation is paused
>       assert self.simulation.status == SimulationStatus.PAUSED
E       AssertionError: assert <SimulationSt...: 'scheduled'> == <SimulationSt...SED: 'paused'>
E         
E         - paused
E         + scheduled

tests/scientific_computing/job_management/test_long_running_job_manager.py:300: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
______________ TestLongRunningJobManager.test_checkpoint_creation ______________

self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f49518813f0>

    def test_checkpoint_creation(self):
        """Test creation of checkpoints for simulations."""
        # Submit and process
        self.job_manager.submit_simulation(self.simulation)
        self.job_manager._process_queue()
    
        # Mock the checkpoint manager
        mock_checkpoint_manager = MagicMock()
        self.job_manager.checkpoint_manager = mock_checkpoint_manager
    
        # Pause the simulation (which should trigger checkpointing)
        self.job_manager.pause_simulation(self.simulation.id)
    
        # Verify checkpoint manager was called
>       mock_checkpoint_manager.create_checkpoint.assert_called_once()

tests/scientific_computing/job_management/test_long_running_job_manager.py:316: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='mock.create_checkpoint' id='139949867276112'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'create_checkpoint' to have been called once. Called 0 times.

../../../../.pyenv/versions/3.10.11/lib/python3.10/unittest/mock.py:908: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
_____________ TestLongRunningJobManager.test_maintenance_handling ______________

self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f49518815a0>

    def test_maintenance_handling(self):
        """Test handling of maintenance windows."""
        # Submit and process
        self.job_manager.submit_simulation(self.simulation)
        self.job_manager._process_queue()
    
        # Create a maintenance window using the scheduler's method
        now = datetime.now()
        window = self.scheduler.add_maintenance_window(
            start_time=now + timedelta(hours=1),
            end_time=now + timedelta(hours=3),
            description="Test maintenance",
            affected_nodes=[node.id for node in self.nodes[:5]],  # First 5 nodes
            severity="major",
        )
    
        # Explicitly assign the simulation to the affected nodes
        for node in self.nodes[:5]:
            if not hasattr(node, 'assigned_simulations'):
                node.assigned_simulations = []
            node.assigned_simulations.append(self.simulation.id)
            # Add a reservation that conflicts with maintenance
            result = self.scheduler.reserve_resources(
                simulation=self.simulation,
                start_time=now + timedelta(minutes=30),
                duration=timedelta(hours=4)  # Overlaps with maintenance
            )
            if result.success:
                reservation = result.value
                self.scheduler.activate_reservation(reservation.id)
            break  # Only need one node to be affected
    
        # Let the job manager handle the maintenance
        self.job_manager.handle_maintenance_window(window)
    
        # Check if simulation was paused
>       assert self.simulation.status == SimulationStatus.PAUSED
E       AssertionError: assert <SimulationSt...: 'scheduled'> == <SimulationSt...SED: 'paused'>
E         
E         - paused
E         + scheduled

tests/scientific_computing/job_management/test_long_running_job_manager.py:354: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:246 Reservation for simulation sim-test-1 overlaps with maintenance window maint_20250616041643_42881187492809b2
______________ TestLongRunningJobManager.test_preemption_policies ______________

self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881750>

    def test_preemption_policies(self):
        """Test preemption policies for simulations of different priorities."""
        # Create simulations with different priorities
        low_priority_sim = self._create_test_simulation()
        low_priority_sim.id = "sim-low"
        low_priority_sim.priority = SimulationPriority.LOW
    
        high_priority_sim = self._create_test_simulation()
        high_priority_sim.id = "sim-high"
        high_priority_sim.priority = SimulationPriority.HIGH
    
        critical_sim = self._create_test_simulation()
        critical_sim.id = "sim-critical"
        critical_sim.priority = SimulationPriority.CRITICAL
    
        # Submit all simulations
        self.job_manager.max_concurrent_simulations = 10  # Increase the limit
        self.job_manager.submit_simulation(low_priority_sim)
        self.job_manager.submit_simulation(high_priority_sim)
        self.job_manager._process_queue()
    
        # Force the system into a resource shortage
        for node in self.nodes:
            node.current_load = {
                ResourceType.CPU: node.cpu_cores * 0.9,  # 90% CPU usage
                ResourceType.MEMORY: node.memory_gb * 0.9,  # 90% memory usage
            }
    
        # Create a special method for the test to ensure preemption happens
        def preempt_low_priority_for_test(self):
            # Pause all low priority simulations in the system
            for sim_id, sim in list(self.running_simulations.items()):
                if sim.priority == SimulationPriority.LOW:
                    self.pause_simulation(sim_id)
    
        # Monkey patch the method to the job manager for this test
        self.job_manager.preempt_low_priority_for_test = types.MethodType(preempt_low_priority_for_test, self.job_manager)
    
        # Try to submit a critical simulation
        self.job_manager.submit_simulation(critical_sim)
    
        # Force preempt low priority simulations for test
        self.job_manager.preempt_low_priority_for_test()
    
        # Now process the queue
        self.job_manager._process_queue()
    
        # Verify low priority simulation was preempted
>       assert low_priority_sim.status == SimulationStatus.PAUSED
E       AssertionError: assert <SimulationSt...: 'scheduled'> == <SimulationSt...SED: 'paused'>
E         
E         - paused
E         + scheduled

tests/scientific_computing/job_management/test_long_running_job_manager.py:404: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
_____________ TestLongRunningJobManager.test_long_term_reservation _____________

self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881900>

    def test_long_term_reservation(self):
        """Test long-term resource reservation for extended simulations."""
        # Create a long-running simulation
        long_sim = self._create_test_simulation()
        long_sim.id = "sim-long"
        long_sim.estimated_total_duration = timedelta(days=30)  # 30-day simulation
    
        # Submit the simulation
        self.job_manager.submit_simulation(long_sim)
        self.job_manager._process_queue()
    
        # Check if a long-term reservation was created
        reservations = self.scheduler.get_reservations_for_simulation(long_sim.id)
>       assert len(reservations) > 0
E       assert 0 > 0
E        +  where 0 = len([])

tests/scientific_computing/job_management/test_long_running_job_manager.py:425: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
_________ TestLongRunningJobManager.test_node_allocation_optimization __________

self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881ab0>

    def test_node_allocation_optimization(self):
        """Test optimization of node allocation for efficiency."""
        # Create simulations with different resource requirements
        sim1 = self._create_test_simulation()
        sim1.id = "sim-cpu-intensive"
        for stage in sim1.stages.values():
            for req in stage.resource_requirements:
                if req.resource_type == ResourceType.CPU:
                    req.amount *= 2  # Double CPU requirements
    
        sim2 = self._create_test_simulation()
        sim2.id = "sim-gpu-intensive"
        for stage in sim2.stages.values():
            for req in stage.resource_requirements:
                if req.resource_type == ResourceType.GPU:
                    req.amount *= 2  # Double GPU requirements
    
        # Submit both simulations
        self.job_manager.max_concurrent_simulations = 10
        self.job_manager.submit_simulation(sim1)
        self.job_manager.submit_simulation(sim2)
        self.job_manager._process_queue()
    
        # Both should be running
>       assert sim1.status == SimulationStatus.RUNNING
E       AssertionError: assert <SimulationSt...: 'scheduled'> == <SimulationSt...NG: 'running'>
E         
E         - running
E         + scheduled

tests/scientific_computing/job_management/test_long_running_job_manager.py:456: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
__________ TestLongRunningJobManager.test_dynamic_priority_adjustment __________

self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881c60>

    def test_dynamic_priority_adjustment(self):
        """Test dynamic priority adjustment based on system conditions."""
        # Submit a simulation
        self.job_manager.submit_simulation(self.simulation)
        self.job_manager._process_queue()
    
        # Record current priority
        original_priority = self.simulation.priority
    
        # Simulate system overload
        for node in self.nodes:
            node.current_load = {
                ResourceType.CPU: node.cpu_cores * 0.95,  # 95% CPU usage
                ResourceType.MEMORY: node.memory_gb * 0.95,  # 95% memory usage
            }
    
        # Trigger system condition check
        self.job_manager._adjust_priorities_based_on_system_load()
    
        # Verify priority was lowered (higher value means lower priority)
        priority_order = {
            SimulationPriority.CRITICAL: 0,
            SimulationPriority.HIGH: 1,
            SimulationPriority.MEDIUM: 2,
            SimulationPriority.LOW: 3,
            SimulationPriority.BACKGROUND: 4
        }
    
        # Numerical comparison - higher number = lower priority
>       assert priority_order[self.simulation.priority] > priority_order[original_priority]
E       assert 2 > 2

tests/scientific_computing/job_management/test_long_running_job_manager.py:530: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
_____________ TestLongRunningJobManager.test_recovery_from_failure _____________

self = <tests.scientific_computing.job_management.test_long_running_job_manager.TestLongRunningJobManager object at 0x7f4951881fc0>

    def test_recovery_from_failure(self):
        """Test recovery of a simulation after node failure."""
        # Submit and process
        self.job_manager.submit_simulation(self.simulation)
        self.job_manager._process_queue()
    
        # Mock the checkpoint manager
        mock_checkpoint_manager = MagicMock()
        self.job_manager.checkpoint_manager = mock_checkpoint_manager
    
        # Set up a checkpoint
        mock_checkpoint_manager.get_latest_checkpoint.return_value = "checkpoint-1"
        mock_checkpoint_manager.load_checkpoint.return_value = True
    
        # Mark a node as offline
        affected_node = self.nodes[0]
        affected_node.assigned_simulations = [self.simulation.id]
        self.job_manager.update_node_status(affected_node.id, NodeStatus.OFFLINE)
    
        # Simulate recovery effort
        result = self.job_manager.recover_simulation(self.simulation.id)
    
        # Verify successful recovery
>       assert result.success
E       AssertionError: assert False
E        +  where False = Result(success=False, value=None, error='Simulation sim-test-1 not found').success

tests/scientific_computing/job_management/test_long_running_job_manager.py:572: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
WARNING  concurrent_task_scheduler.job_management.scheduler:scheduler.py:707 No available nodes to process queue
=============================== warnings summary ===============================
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_config.py:323
../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_config.py:323
  /home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)

../../../../.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:293: 11 warnings
  /home/justinchiu_cohere_com/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:293: PydanticDeprecatedSince20: `json_encoders` is deprecated. See https://docs.pydantic.dev/2.11/concepts/serialization/#custom-serializers for alternatives. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    warnings.warn(

tests/scientific_computing/resource_forecasting/test_data_collector.py: 5 warnings
tests/scientific_computing/resource_forecasting/test_forecaster.py: 21 warnings
tests/scientific_computing/resource_forecasting/test_optimizer.py: 5 warnings
tests/scientific_computing/resource_forecasting/test_reporter.py: 25 warnings
  /home/justinchiu_cohere_com/minicode/large_repos/concurrent_task_scheduler/unified/concurrent_task_scheduler/resource_forecasting/data_collector.py:466: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.
    aggregated = df.resample(freq).agg(agg_dict)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
--------------------------------- JSON report ----------------------------------
report saved to: report.json
=========================== short test summary info ============================
FAILED tests/render_farm_manager/integration/test_error_recovery_fixed.py::test_error_recovery_checkpoint_simple
FAILED tests/render_farm_manager/integration/test_error_recovery_fixed_complete.py::test_error_count_threshold_handling
FAILED tests/render_farm_manager/integration/test_fault_tolerance.py::test_fault_tolerance_multiple_node_failures
FAILED tests/render_farm_manager/integration/test_fault_tolerance_fixed.py::test_fault_tolerance_multiple_node_failures
FAILED tests/render_farm_manager/integration/test_job_dependencies_fixed.py::test_circular_dependency_detection
FAILED tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py::test_dependent_job_priority_inheritance
FAILED tests/render_farm_manager/integration/test_job_dependencies_fixed_direct.py::test_circular_dependency_detection
FAILED tests/render_farm_manager/integration/test_job_dependencies_fixed_direct_complete.py::test_simple_dependency
FAILED tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py::test_job_dependency_scheduling
FAILED tests/render_farm_manager/integration/test_job_dependencies_fixed_full.py::test_dependent_job_priority_inheritance
FAILED tests/render_farm_manager/integration/test_job_dependencies_scheduling_patch.py::test_job_dependency_scheduling_patched
FAILED tests/render_farm_manager/integration/test_job_dependencies_simple.py::test_simple_dependency
FAILED tests/render_farm_manager/integration/test_job_dependencies_simple_fixed.py::test_simple_dependency
FAILED tests/render_farm_manager/integration/test_job_dependencies_simple_patched.py::test_simple_dependency
FAILED tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch.py::test_simple_dependency_with_monkey_patch
FAILED tests/render_farm_manager/integration/test_job_dependencies_with_monkey_patch_all.py::test_job_dependency_scheduling
FAILED tests/render_farm_manager/integration/test_render_farm_manager.py::test_scheduling_cycle
FAILED tests/render_farm_manager/integration/test_render_farm_manager.py::test_job_progress_update
FAILED tests/render_farm_manager/integration/test_render_farm_manager.py::test_node_failure
FAILED tests/render_farm_manager/integration/test_render_farm_manager.py::test_cancel_job
FAILED tests/render_farm_manager/integration/test_render_farm_manager.py::test_energy_optimization
FAILED tests/render_farm_manager/integration/test_render_farm_manager.py::test_full_end_to_end_workflow
FAILED tests/render_farm_manager/performance/test_performance.py::test_scheduling_performance
FAILED tests/render_farm_manager/performance/test_performance.py::test_node_specialization_efficiency
FAILED tests/render_farm_manager/unit/test_deadline_scheduler.py::test_update_priorities_deadline_approaching
FAILED tests/render_farm_manager/unit/test_deadline_scheduler.py::test_update_priorities_job_progress
FAILED tests/render_farm_manager/unit/test_deadline_scheduler.py::test_schedule_jobs_priority_order
FAILED tests/render_farm_manager/unit/test_deadline_scheduler.py::test_schedule_jobs_resource_requirements
FAILED tests/render_farm_manager/unit/test_deadline_scheduler.py::test_should_preempt
FAILED tests/render_farm_manager/unit/test_deadline_scheduler.py::test_schedule_with_dependencies
FAILED tests/render_farm_manager/unit/test_deadline_scheduler.py::test_rescheduling_failed_job
FAILED tests/render_farm_manager/unit/test_deadline_scheduler_fixed.py::test_schedule_with_dependencies_fixed
FAILED tests/render_farm_manager/unit/test_energy_optimizer.py::test_optimize_energy_usage
FAILED tests/render_farm_manager/unit/test_energy_optimizer.py::test_estimate_energy_savings
FAILED tests/render_farm_manager/unit/test_node_specialization.py::test_match_job_to_node_gpu_job
FAILED tests/render_farm_manager/unit/test_node_specialization.py::test_match_job_to_node_cpu_job
FAILED tests/render_farm_manager/unit/test_node_specialization.py::test_match_job_to_node_memory_job
FAILED tests/render_farm_manager/unit/test_node_specialization.py::test_performance_history_influence
FAILED tests/render_farm_manager/unit/test_node_specialization.py::test_node_capability_matching
FAILED tests/render_farm_manager/unit/test_node_specialization.py::test_specialized_vs_general_nodes
FAILED tests/render_farm_manager/unit/test_progressive_renderer.py::test_process_pending_outputs
FAILED tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_update_stage_status
FAILED tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_is_stage_ready
FAILED tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_add_dynamic_dependency
FAILED tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_validate_simulation
FAILED tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_create_instance
FAILED tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_create_linear_workflow
FAILED tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_create_parallel_workflow
FAILED tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_get_next_stages
FAILED tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_update_instance
FAILED tests/scientific_computing/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_get_workflow_status
FAILED tests/scientific_computing/failure_resilience/test_failure_detector.py::test_check_node_health
FAILED tests/scientific_computing/failure_resilience/test_failure_detector.py::test_reliability_metrics
FAILED tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_check_node_health
FAILED tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_detect_node_failures
FAILED tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_submit_simulation
FAILED tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_processing_queue
FAILED tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_pause_and_resume_simulation
FAILED tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_simulation_stage_transitions
FAILED tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_handle_node_failure
FAILED tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_checkpoint_creation
FAILED tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_maintenance_handling
FAILED tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_preemption_policies
FAILED tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_long_term_reservation
FAILED tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_node_allocation_optimization
FAILED tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_dynamic_priority_adjustment
FAILED tests/scientific_computing/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_recovery_from_failure
ERROR tests/render_farm_manager/unit/test_resource_partitioner.py::test_calculate_resource_usage
ERROR tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_check_simulation_health
ERROR tests/scientific_computing/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_detect_simulation_failures
= 67 failed, 283 passed, 4 xfailed, 1 xpassed, 69 warnings, 3 errors in 24.33s =
